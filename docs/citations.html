<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Patent Citations | The WIPO Patent Analytics Handbook</title>
  <meta name="description" content="Chapter 6 Patent Citations | The WIPO Patent Analytics Handbook." />
  <meta name="generator" content="bookdown 0.28.1 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Patent Citations | The WIPO Patent Analytics Handbook" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Chapter 6 Patent Citations | The WIPO Patent Analytics Handbook." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Patent Citations | The WIPO Patent Analytics Handbook" />
  
  <meta name="twitter:description" content="Chapter 6 Patent Citations | The WIPO Patent Analytics Handbook." />
  

<meta name="author" content="Paul Oldham" />


<meta name="date" content="2022-01-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="classification.html"/>
<link rel="next" href="future.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<script src="libs/d3-4.13.0/d3.min.js"></script>
<script src="libs/sankey-1/sankey.js"></script>
<script src="libs/sankeyNetwork-binding-0.4/sankeyNetwork.js"></script>
<link href="libs/leaflet-1.3.1/leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-1.3.1/leaflet.js"></script>
<link href="libs/leafletfix-1.0.0/leafletfix.css" rel="stylesheet" />
<script src="libs/proj4-2.6.2/proj4.min.js"></script>
<script src="libs/Proj4Leaflet-1.0.1/proj4leaflet.js"></script>
<link href="libs/rstudio_leaflet-1.3.1/rstudio_leaflet.css" rel="stylesheet" />
<script src="libs/leaflet-binding-2.1.1/leaflet.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<script src="libs/sund2b-binding-2.1.6/sund2b.js"></script>
<link href="libs/d2b-1.0.9/d2b_custom.css" rel="stylesheet" />
<script src="libs/d2b-1.0.9/d2b.min.js"></script>
<link href="libs/collapsibleTree-0.1.6/collapsibleTree.css" rel="stylesheet" />
<script src="libs/collapsibleTree-binding-0.1.7/collapsibleTree.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The WIPO Patent Analytics Handbook</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About the Author</a></li>
<li class="chapter" data-level="" data-path="acknowlegements.html"><a href="acknowlegements.html"><i class="fa fa-check"></i>Acknowlegements</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="how-to-use-the-handbook.html"><a href="how-to-use-the-handbook.html"><i class="fa fa-check"></i>How to use the Handbook</a></li>
<li class="chapter" data-level="" data-path="note-to-readers.html"><a href="note-to-readers.html"><i class="fa fa-check"></i>Note to Readers</a></li>
<li class="chapter" data-level="" data-path="datasets.html"><a href="datasets.html"><i class="fa fa-check"></i>Datasets</a></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="literature.html"><a href="literature.html"><i class="fa fa-check"></i><b>2</b> Scientific Literature</a>
<ul>
<li class="chapter" data-level="2.1" data-path="literature.html"><a href="literature.html#accessing-the-scientific-literature"><i class="fa fa-check"></i><b>2.1</b> Accessing the Scientific Literature</a></li>
<li class="chapter" data-level="2.2" data-path="literature.html"><a href="literature.html#searching-literature-databases"><i class="fa fa-check"></i><b>2.2</b> Searching Literature Databases</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="literature.html"><a href="literature.html#stemming"><i class="fa fa-check"></i><b>2.2.1</b> Stemming</a></li>
<li class="chapter" data-level="2.2.2" data-path="literature.html"><a href="literature.html#using-search-operators"><i class="fa fa-check"></i><b>2.2.2</b> Using Search Operators</a></li>
<li class="chapter" data-level="2.2.3" data-path="literature.html"><a href="literature.html#proximity-operators"><i class="fa fa-check"></i><b>2.2.3</b> Proximity Operators</a></li>
<li class="chapter" data-level="2.2.4" data-path="literature.html"><a href="literature.html#regular-expressions"><i class="fa fa-check"></i><b>2.2.4</b> Regular Expressions</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="literature.html"><a href="literature.html#precision-vs.-recall"><i class="fa fa-check"></i><b>2.3</b> Precision vs. Recall</a></li>
<li class="chapter" data-level="2.4" data-path="literature.html"><a href="literature.html#processing-scientific-literature"><i class="fa fa-check"></i><b>2.4</b> Processing Scientific Literature</a></li>
<li class="chapter" data-level="2.5" data-path="literature.html"><a href="literature.html#visualizing-the-scientific-literature"><i class="fa fa-check"></i><b>2.5</b> Visualizing the Scientific Literature</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="literature.html"><a href="literature.html#dashboards"><i class="fa fa-check"></i><b>2.5.1</b> Dashboards</a></li>
<li class="chapter" data-level="2.5.2" data-path="literature.html"><a href="literature.html#network-visualisation"><i class="fa fa-check"></i><b>2.5.2</b> Network Visualisation</a></li>
<li class="chapter" data-level="2.5.3" data-path="literature.html"><a href="literature.html#other-forms-of-visualisation"><i class="fa fa-check"></i><b>2.5.3</b> Other forms of visualisation</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="literature.html"><a href="literature.html#linking-the-scientific-literature-with-patent-analysis"><i class="fa fa-check"></i><b>2.6</b> Linking the Scientific Literature with Patent Analysis</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="literature.html"><a href="literature.html#mapping-authors-to-inventors"><i class="fa fa-check"></i><b>2.6.1</b> Mapping Authors to Inventors</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="literature.html"><a href="literature.html#linking-citations-with-patent-literature"><i class="fa fa-check"></i><b>2.7</b> Linking Citations with Patent Literature</a></li>
<li class="chapter" data-level="2.8" data-path="literature.html"><a href="literature.html#conclusion"><i class="fa fa-check"></i><b>2.8</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="geocoding.html"><a href="geocoding.html"><i class="fa fa-check"></i><b>3</b> Geocoding</a>
<ul>
<li class="chapter" data-level="3.0.1" data-path="geocoding.html"><a href="geocoding.html#getting-started"><i class="fa fa-check"></i><b>3.0.1</b> Getting Started</a></li>
<li class="chapter" data-level="3.0.2" data-path="geocoding.html"><a href="geocoding.html#getting-set-up-with-the-google-maps-api"><i class="fa fa-check"></i><b>3.0.2</b> Getting set up with the Google Maps API</a></li>
<li class="chapter" data-level="3.0.3" data-path="geocoding.html"><a href="geocoding.html#using-the-api"><i class="fa fa-check"></i><b>3.0.3</b> Using the API</a></li>
<li class="chapter" data-level="3.0.4" data-path="geocoding.html"><a href="geocoding.html#the-source-data"><i class="fa fa-check"></i><b>3.0.4</b> The Source Data</a></li>
<li class="chapter" data-level="3.1" data-path="geocoding.html"><a href="geocoding.html#lookup-the-records"><i class="fa fa-check"></i><b>3.1</b> Lookup the Records</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="geocoding.html"><a href="geocoding.html#using-placement"><i class="fa fa-check"></i><b>3.1.1</b> Using placement</a></li>
<li class="chapter" data-level="3.1.2" data-path="geocoding.html"><a href="geocoding.html#using-ggmap"><i class="fa fa-check"></i><b>3.1.2</b> Using ggmap</a></li>
<li class="chapter" data-level="3.1.3" data-path="geocoding.html"><a href="geocoding.html#using-googleway"><i class="fa fa-check"></i><b>3.1.3</b> Using Googleway</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="geocoding.html"><a href="geocoding.html#reviewing-initial-results"><i class="fa fa-check"></i><b>3.2</b> Reviewing Initial Results</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="geocoding.html"><a href="geocoding.html#tackling-abbreviations"><i class="fa fa-check"></i><b>3.2.1</b> Tackling Abbreviations</a></li>
<li class="chapter" data-level="3.2.2" data-path="geocoding.html"><a href="geocoding.html#lookup-edited-names"><i class="fa fa-check"></i><b>3.2.2</b> Lookup edited names</a></li>
<li class="chapter" data-level="3.2.3" data-path="geocoding.html"><a href="geocoding.html#bringing-the-data-together"><i class="fa fa-check"></i><b>3.2.3</b> Bringing the data together</a></li>
<li class="chapter" data-level="3.2.4" data-path="geocoding.html"><a href="geocoding.html#assessing-the-quality-of-geocoding"><i class="fa fa-check"></i><b>3.2.4</b> Assessing the Quality of Geocoding</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="geocoding.html"><a href="geocoding.html#preprocess-the-data-and-rerun-the-query"><i class="fa fa-check"></i><b>3.3</b> Preprocess the Data and Rerun the Query</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="geocoding.html"><a href="geocoding.html#duplicated-affiliation-names"><i class="fa fa-check"></i><b>3.3.1</b> Duplicated Affiliation Names</a></li>
<li class="chapter" data-level="3.3.2" data-path="geocoding.html"><a href="geocoding.html#quickly-mapping-the-data"><i class="fa fa-check"></i><b>3.3.2</b> Quickly Mapping the Data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="geocoding.html"><a href="geocoding.html#round-up"><i class="fa fa-check"></i><b>3.4</b> Round Up</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="patents.html"><a href="patents.html"><i class="fa fa-check"></i><b>4</b> Counting Patent Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="patents.html"><a href="patents.html#the-structure-of-patent-numbers"><i class="fa fa-check"></i><b>4.1</b> The structure of patent numbers</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="patents.html"><a href="patents.html#the-country-code"><i class="fa fa-check"></i><b>4.1.1</b> The country code</a></li>
<li class="chapter" data-level="4.1.2" data-path="patents.html"><a href="patents.html#the-numeric-identifier"><i class="fa fa-check"></i><b>4.1.2</b> The numeric identifier</a></li>
<li class="chapter" data-level="4.1.3" data-path="patents.html"><a href="patents.html#kind-codes"><i class="fa fa-check"></i><b>4.1.3</b> Kind Codes</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="patents.html"><a href="patents.html#preparing-to-count-patent-data"><i class="fa fa-check"></i><b>4.2</b> Preparing to Count Patent Data</a></li>
<li class="chapter" data-level="4.3" data-path="patents.html"><a href="patents.html#counting-priority-or-first-filings"><i class="fa fa-check"></i><b>4.3</b> Counting Priority or First Filings</a></li>
<li class="chapter" data-level="4.4" data-path="patents.html"><a href="patents.html#counting-priority-applications"><i class="fa fa-check"></i><b>4.4</b> Counting Priority Applications</a></li>
<li class="chapter" data-level="4.5" data-path="patents.html"><a href="patents.html#counting-applications"><i class="fa fa-check"></i><b>4.5</b> Counting Applications</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="patents.html"><a href="patents.html#mapping-publications-family-members"><i class="fa fa-check"></i><b>4.5.1</b> Mapping Publications (Family Members)</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="patents.html"><a href="patents.html#trends-by-country-using-publication-data"><i class="fa fa-check"></i><b>4.6</b> Trends by Country using Publication Data</a></li>
<li class="chapter" data-level="4.7" data-path="patents.html"><a href="patents.html#families"><i class="fa fa-check"></i><b>4.7</b> Patent Families</a></li>
<li class="chapter" data-level="4.8" data-path="patents.html"><a href="patents.html#modelling-data"><i class="fa fa-check"></i><b>4.8</b> Modelling Data</a>
<ul>
<li class="chapter" data-level="4.8.1" data-path="patents.html"><a href="patents.html#the-data"><i class="fa fa-check"></i><b>4.8.1</b> The Data</a></li>
<li class="chapter" data-level="4.8.2" data-path="patents.html"><a href="patents.html#loess-smoothing"><i class="fa fa-check"></i><b>4.8.2</b> Loess Smoothing</a></li>
</ul></li>
<li class="chapter" data-level="4.9" data-path="patents.html"><a href="patents.html#forecasting"><i class="fa fa-check"></i><b>4.9</b> Forecasting</a>
<ul>
<li class="chapter" data-level="4.9.1" data-path="patents.html"><a href="patents.html#day-of-days"><i class="fa fa-check"></i><b>4.9.1</b> Day of Days</a></li>
<li class="chapter" data-level="4.9.2" data-path="patents.html"><a href="patents.html#the-data-1"><i class="fa fa-check"></i><b>4.9.2</b> The Data</a></li>
</ul></li>
<li class="chapter" data-level="4.10" data-path="patents.html"><a href="patents.html#conclusion-1"><i class="fa fa-check"></i><b>4.10</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="classification.html"><a href="classification.html"><i class="fa fa-check"></i><b>5</b> Patent Classfication</a>
<ul>
<li class="chapter" data-level="5.0.1" data-path="classification.html"><a href="classification.html#exploring-the-international-patent-classification"><i class="fa fa-check"></i><b>5.0.1</b> Exploring the International Patent Classification</a></li>
<li class="chapter" data-level="5.0.2" data-path="classification.html"><a href="classification.html#the-us-ipc-table"><i class="fa fa-check"></i><b>5.0.2</b> The US IPC Table</a></li>
<li class="chapter" data-level="5.0.3" data-path="classification.html"><a href="classification.html#assessing-relationships-between-technology-areas"><i class="fa fa-check"></i><b>5.0.3</b> Assessing Relationships Between Technology Areas</a></li>
<li class="chapter" data-level="5.0.4" data-path="classification.html"><a href="classification.html#visualising-relationships-with-chord-diagrams"><i class="fa fa-check"></i><b>5.0.4</b> Visualising Relationships with Chord Diagrams</a></li>
<li class="chapter" data-level="5.0.5" data-path="classification.html"><a href="classification.html#the-short-ipc"><i class="fa fa-check"></i><b>5.0.5</b> The Short IPC</a></li>
<li class="chapter" data-level="5.0.6" data-path="classification.html"><a href="classification.html#the-structure-of-patent-activity"><i class="fa fa-check"></i><b>5.0.6</b> The Structure of Patent Activity</a></li>
<li class="chapter" data-level="5.0.7" data-path="classification.html"><a href="classification.html#conclusion-2"><i class="fa fa-check"></i><b>5.0.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="citations.html"><a href="citations.html"><i class="fa fa-check"></i><b>6</b> Patent Citations</a>
<ul>
<li class="chapter" data-level="6.0.1" data-path="citations.html"><a href="citations.html#non-patent-literature"><i class="fa fa-check"></i><b>6.0.1</b> Non Patent Literature</a></li>
<li class="chapter" data-level="6.0.2" data-path="citations.html"><a href="citations.html#literature-and-patent-citation-data-with-the-lens"><i class="fa fa-check"></i><b>6.0.2</b> Literature and Patent Citation Data with the Lens</a></li>
<li class="chapter" data-level="6.0.3" data-path="citations.html"><a href="citations.html#patent-citations"><i class="fa fa-check"></i><b>6.0.3</b> Patent Citations</a></li>
<li class="chapter" data-level="6.0.4" data-path="citations.html"><a href="citations.html#navigating-patent-networks"><i class="fa fa-check"></i><b>6.0.4</b> Navigating Patent Networks</a></li>
<li class="chapter" data-level="6.0.5" data-path="citations.html"><a href="citations.html#forward-citations"><i class="fa fa-check"></i><b>6.0.5</b> Forward Citations</a></li>
<li class="chapter" data-level="6.0.6" data-path="citations.html"><a href="citations.html#counting-citations-by-patent-families"><i class="fa fa-check"></i><b>6.0.6</b> Counting Citations by Patent Families</a></li>
<li class="chapter" data-level="6.0.7" data-path="citations.html"><a href="citations.html#word-vectors-with-fastext"><i class="fa fa-check"></i><b>6.0.7</b> Word Vectors with fastext</a></li>
<li class="chapter" data-level="6.0.8" data-path="citations.html"><a href="citations.html#training-word-vectors-for-drones"><i class="fa fa-check"></i><b>6.0.8</b> Training Word Vectors for Drones</a></li>
<li class="chapter" data-level="6.0.9" data-path="citations.html"><a href="citations.html#using-word-vectors"><i class="fa fa-check"></i><b>6.0.9</b> Using Word Vectors</a></li>
<li class="chapter" data-level="6.0.10" data-path="citations.html"><a href="citations.html#exploring-analogies"><i class="fa fa-check"></i><b>6.0.10</b> Exploring Analogies</a></li>
<li class="chapter" data-level="6.0.11" data-path="citations.html"><a href="citations.html#patent-specific-word-embeddings"><i class="fa fa-check"></i><b>6.0.11</b> Patent Specific Word Embeddings</a></li>
<li class="chapter" data-level="6.0.12" data-path="citations.html"><a href="citations.html#machine-learning-in-classification"><i class="fa fa-check"></i><b>6.0.12</b> Machine learning in Classification</a></li>
<li class="chapter" data-level="6.1" data-path="citations.html"><a href="citations.html#machine-learning-for-text-classification"><i class="fa fa-check"></i><b>6.1</b> Machine Learning for Text Classification</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="citations.html"><a href="citations.html#step-1-binary-text-classification"><i class="fa fa-check"></i><b>6.1.1</b> Step 1: Binary Text Classification</a></li>
<li class="chapter" data-level="6.1.2" data-path="citations.html"><a href="citations.html#step-2-multilabel-text-classification"><i class="fa fa-check"></i><b>6.1.2</b> Step 2: Multilabel Text Classification</a></li>
<li class="chapter" data-level="6.1.3" data-path="citations.html"><a href="citations.html#step-3-named-entity-recognition"><i class="fa fa-check"></i><b>6.1.3</b> Step 3: Named Entity Recognition</a></li>
<li class="chapter" data-level="6.1.4" data-path="citations.html"><a href="citations.html#from-vector-based-models-to-transformers"><i class="fa fa-check"></i><b>6.1.4</b> From Vector Based Models to Transformers</a></li>
<li class="chapter" data-level="6.1.5" data-path="citations.html"><a href="citations.html#conclusion-3"><i class="fa fa-check"></i><b>6.1.5</b> Conclusion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="future.html"><a href="future.html"><i class="fa fa-check"></i><b>7</b> The Future</a></li>
<li class="divider"></li>
<li><a href="https://github.com/wipo-analytics/handbook" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The WIPO Patent Analytics Handbook</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="citations" class="section level1 hasAnchor" number="6">
<h1><span class="header-section-number">Chapter 6</span> Patent Citations<a href="citations.html#citations" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>
<!--- last spell checked 2022-08-18--->
This chapter focuses on the use of patent citations in patent analytics. Patent citations take two main forms:</p>
<ol style="list-style-type: lower-alpha">
<li>Citations of the scientific literature and other material such as news articles and websites, known as the Non-Patent Literature or NPL</li>
<li>Citations of patent documents.</li>
</ol>
<p>The patent citation system is similar to the familiar academic citation system. However, patent citations differ from academic citations because they <em>limit the scope</em> of what an applicant can claim to be new or novel or as involving an inventive step. As the American economist Suzanne Scotchmer reminds us when approaching the patent system applicants are ‘standing on the shoulders of giants’ <span class="citation">(<a href="#ref-Scotchmer_1991" role="doc-biblioref">Scotchmer 1991</a>)</span>. Put simply new applicants are confronted by the combined weight of the scientific literature, other material and existing patent applications that make up the <em>prior art</em>. The prior art limits the scope of what may be claimed by new applicants and is recorded in patent citations.</p>
<p>Citations within the patent system have become an important focus for research in fields such as econometrics, scientometrics and innovation studies. Citations of the non-patent literature are important focus of research because they help to reveal the closeness of the relationship between scientific research and innovative activity reflected in the patent system. Citations of patent documents are a focus of research because the number of citations that a patent document or patent family attracts is an indicator of social and economic value <span class="citation">(<a href="#ref-Jaffe_2017" role="doc-biblioref">A. B. Jaffe and Rassenfosse 2017</a>)</span>. In addition, analysis of patent citations can lead to the identification of similar patent documents in a technology field, technology spillovers and technology trajectories.</p>
<p>A rich literature has emerged around patent citation analysis and we will highlight some of the key sources in the course of this chapter. Recent work by <span class="citation">A. B. Jaffe and Rassenfosse (<a href="#ref-Jaffe_2017" role="doc-biblioref">2017</a>)</span> provides an accessible and detailed overview of social scientific research involving patent citations. For those seeking to ground themselves in key literature the work by Adam Jaffe and Manuel Trajtenberg <span class="citation">(<a href="#ref-Jaffe_1998" role="doc-biblioref">A. Jaffe and Trajtenberg 1998</a>, <a href="#ref-Jaffe_2002" role="doc-biblioref">2002</a>)</span>, along with work by Bronwyn Hall <span class="citation">(<a href="#ref-Hall_2001" role="doc-biblioref">B. Hall, Jaffe, and Trajtenberg 2001</a>; <a href="#ref-Hall_2005" role="doc-biblioref">B. H. Hall, Jaffe, and Trajtenberg 2005</a>; <a href="#ref-Hall_2012" role="doc-biblioref">B. H. Hall and Harhoff 2012</a>)</span>, Colin Webb and Helene Dernis at the OECD <span class="citation">(<a href="#ref-Webb_2005" role="doc-biblioref">Webb et al. 2005</a>)</span> and Dietmar Harhoff <span class="citation">(<a href="#ref-Harhoff_1999" role="doc-biblioref">Harhoff et al. 1999</a>; <a href="#ref-Harhoff_2003" role="doc-biblioref">Harhoff, Scherer, and Vopel 2003</a>)</span> are essential reading. For those interested in exploring the wider literature on patent citations a <a href="https://www.lens.org/lens/search/scholar/list?collectionId=203349">Lens public collection</a> is available to assist readers with getting started. The <a href="https://www.lens.org/lens/search/scholar/list?collectionId=203349">collection</a> is dynamic and will automatically update to the latest literature on patent citations.</p>
<p>In this chapter we will begin with the non-patent literature and move step by step through the issues that need to be considered when working with patent citation data. We will use examples from synthetic biology and CRISPR genome editing technology to illustrate approaches to the non-patent literature and patent citations and finish with recent research on identifying technology paths with citation data.</p>
<div id="non-patent-literature" class="section level3 hasAnchor" number="6.0.1">
<h3><span class="header-section-number">6.0.1</span> Non Patent Literature<a href="citations.html#non-patent-literature" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Non-patent literature mainly takes the form of scientific publications such as journals, books, or chapters but also extends to other types of materials such as manuals, news reports, drawings and websites. As we will see below, viewed from a data science perspective the non-patent literature is that it generally takes the form of messy free text that requires extensive cleaning. However, from approximately 1980 onwards NPL has become an increasingly important focus for research.</p>
<p>For those seeking to track the emergence of research on NPL citations we will simply provide some useful way points. An important starting point of research on non patent literature is work by <span class="citation">Carpenter, Cooper, and Narin (<a href="#ref-Carpenter_1980" role="doc-biblioref">1980</a>)</span> on the links between basic academic research and patent activity using US patent citation data. This was followed in the mid-1990s by more detailed studies with larger scale data by Narin and collaborators <span class="citation">(<a href="#ref-Narin_1995" role="doc-biblioref">F. Narin, Hamilton, and Olivastro 1995</a>; <a href="#ref-Narin_1997" role="doc-biblioref">Francis Narin, Hamilton, and Olivastro 1997</a>)</span>. This was accompanied by the growing proliferation of technology specific studies using citations such as work by <span class="citation">Meyer (<a href="#ref-Meyer_2000" role="doc-biblioref">2000</a>)</span> on the relationship between nanoscale technologies and the cited literature. Citation based studies have tended to be heavily focused, for reasons of accessibility, on US patent data. However, in the mid-2000s work by <span class="citation">Callaert, Looy, et al. (<a href="#ref-Callaert_2006" role="doc-biblioref">2006</a>)</span> examined 10,000 citations from the US and the European Patent Offices. This was followed by work to address the significant problems that exist with noise in NPL data using machine learning models <span class="citation">(<a href="#ref-Callaert_2011" role="doc-biblioref">Callaert, Grouwels, and Looy 2011</a>)</span>. The growing availability of NPL citation data is reflected in the growth of national and sectoral studies such as work by <span class="citation">Fukuzawa and Ida (<a href="#ref-Fukuzawa_2015" role="doc-biblioref">2015</a>)</span> exploring the links between scientific articles and patents for leading researchers in Japan. Recent work by <span class="citation">Ding et al. (<a href="#ref-Ding_2017" role="doc-biblioref">2017</a>)</span> has focused on the characteristics of scientific articles that facilitate knowledge flows between science and technology while <span class="citation">Chen (<a href="#ref-Chen_2017" role="doc-biblioref">2017</a>)</span> has explored the textual similarities between scientific articles and the contents of patent applications. Research by <span class="citation">Rizzo et al. (<a href="#ref-Rizzo_2018" role="doc-biblioref">2018</a>)</span> has focused on the closeness of publicly funded research and radical inventions in UK filings at the European Patent Office.</p>
<p>As this very brief set of way markers suggests, a significant body of work on diverse topics has emerged around the non-patent literature. While much of the original research focused on US patent data the creation of the EPO World Patent Statistical Database (PATSTAT) has made the wider non-patent literature available in a single table and served as a spur for research <span class="citation">(<a href="#ref-Webb_2005" role="doc-biblioref">Webb et al. 2005</a>; <a href="#ref-Callaert_2011" role="doc-biblioref">Callaert, Grouwels, and Looy 2011</a>; <a href="#ref-Karvonen_2013" role="doc-biblioref">Karvonen and Kässi 2013</a>)</span>. The recent PatentsView service from the USPTO now makes NPL citations available as a single table containing over 6 million raw references that can be freely downloaded. As such, data on non-patent literature citations is becoming more and more accessible for research.</p>
<p>We will consider the case of PatentsView in more detail below. However, one of the most important recent development are efforts by patent databases, led by the <a href="https://www.lens.org/">open access Lens database</a> to electronically link literature databases and patent databases together.</p>
</div>
<div id="literature-and-patent-citation-data-with-the-lens" class="section level3 hasAnchor" number="6.0.2">
<h3><span class="header-section-number">6.0.2</span> Literature and Patent Citation Data with the Lens<a href="citations.html#literature-and-patent-citation-data-with-the-lens" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As discussed in the chapter on the scientific literature, free electronic access to sources of the scientific literature is increasingly available through services such as Crossref, PubMed and Microsoft Academic Graph. The Lens has now developed a <code>Scholar Search</code> service that includes approximately 291.4 million scholarly works from PubMed, Crossref and Microsoft Academic. These records are then linked to citations in patent over 110 million patent documents covered by the Lens <code>Patent Search</code>. The importance of this approach is that it allows the user to navigate the non-patent literature linked to a particular record or to extract the cited literature from a record and create a collection to download and analyse. As we will also see, the recent <code>PATCITE</code> service also allows users to retrieve data on citations at scale.</p>
<p>To illustrate the possibilities opened up by combining the scientific and patent literature we will use the example of a patent search for synthetic biology and then move into exploration of the controversial subject of genome editing.</p>
<div id="retrieving-cited-literature-from-a-patent-search" class="section level4 hasAnchor" number="6.0.2.1">
<h4><span class="header-section-number">6.0.2.1</span> Retrieving Cited Literature from a Patent Search<a href="citations.html#retrieving-cited-literature-from-a-patent-search" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We will begin with a simple patent search of the titles and abstracts for terms relating to synthetic biology in the Lens.</p>
<blockquote>
<p>title: (“synthetic biology” OR (“synthetic genome” OR (“synthetic genomes” OR “synthetic genomics”))) OR abstract: (“synthetic biology” OR (“synthetic genome” OR (“synthetic genomes” OR “synthetic genomics”)))</p>
</blockquote>
<p>You can try this exact query and example live by following this <a href="https://www.lens.org/lens/scholar/search/results?patentQueryId=fv9ylnbkosQklD7F6hnaE96bpl76I3qQAJNHCqMtXc4J9Gw8nAyrsCKM9eCrn5ih&amp;preview=true">link</a>. To make the most of the Lens we recommend that you register as a user (registration is free) as this will allow you to create collections for export.</p>
<p>At the time of writing this search generated 287 patent results and 776 cited works that can be viewed in the Cited Works panel as illustrated below:</p>
<p><img src="images/citations/lenscitedworks.png" width="1607" /></p>
<p>Registered users can create a public or private collection that can be shared with others and can also download the citation data up to a maximum of 50,000 records.</p>
<p>As an alternative to creating a collection an export button is provided in the Cited Works tool bar with options to download in CSV, RIS, Bibtex and JSON formats. The CSV (comma separated values) option is particularly suited to text mining or visualisation in tables while the popular Bibtex format will be useful for creating bibliographies for researchers writing in markdown. Table <a href="citations.html#tab:importcitedworks">6.1</a> displays a small selection of the results.</p>
<table>
<caption>
<span id="tab:importcitedworks">Table 6.1: </span>Sample Literature Fields from the Lens for Synthetic Biology
</caption>
<thead>
<tr>
<th style="text-align:left;">
Title
</th>
<th style="text-align:right;">
Publication Year
</th>
<th style="text-align:left;">
DOI
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Pathway optimization and key enzyme evolution of N-acetylneuraminate biosynthesis using an in vivo aptazyme-based biosensor
</td>
<td style="text-align:right;">
2017
</td>
<td style="text-align:left;">
10.1016/j.ymben.2017.08.001
</td>
</tr>
<tr>
<td style="text-align:left;">
Transcriptome-based identification of the optimal reference CHO genes for normalisation of qPCR data
</td>
<td style="text-align:right;">
2017
</td>
<td style="text-align:left;">
10.1002/biot.201700259
</td>
</tr>
<tr>
<td style="text-align:left;">
The N-Acetylmuramic Acid 6-Phosphate Phosphatase MupP Completes the Pseudomonas Peptidoglycan Recycling Pathway Leading to Intrinsic Fosfomycin Resistance
</td>
<td style="text-align:right;">
2017
</td>
<td style="text-align:left;">
10.1128/mbio.00092-17
</td>
</tr>
<tr>
<td style="text-align:left;">
Efficient whole-cell biocatalyst for Neu5Ac production by manipulating synthetic, degradation and transmembrane pathways
</td>
<td style="text-align:right;">
2016
</td>
<td style="text-align:left;">
10.1007/s10529-016-2215-z
</td>
</tr>
<tr>
<td style="text-align:left;">
Droplet immobilization within a polymeric organogel improves lipid bilayer durability and portability
</td>
<td style="text-align:right;">
2016
</td>
<td style="text-align:left;">
10.1039/c6lc00391e
</td>
</tr>
</tbody>
</table>
<p>A wide range of other fields are also available with the downloaded data such as authors, keywords, abstracts (where available), MeSH terms (Medical Subject Headings), Chemicals, source urls and the number of patent documents that reference an article among others. This is therefore a very rich set of data for further exploration.</p>
<p>Taking our small sample data for synthetic biology we can identify the articles that are the top cited in the patent dataset as displayed in Table <a href="citations.html#tab:topranking">6.2</a>. Note that the <code>Referenced by Patent Count</code> column refers to the total known count of patent documents citing the article. This measure will therefore generally favour older and foundational literature. This is revealed by the dominance of citations to the Basic Local Alignment Search Tool (BLAST) algorithm that is very widely used in biology.</p>
<table>
<caption>
<span id="tab:topranking">Table 6.2: </span>Sample Literature Fields from the Lens for Synthetic Biology
</caption>
<thead>
<tr>
<th style="text-align:left;">
Title
</th>
<th style="text-align:right;">
Publication Year
</th>
<th style="text-align:right;">
Referenced by Patent Count
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
Basic Local Alignment Search Tool
</td>
<td style="text-align:right;">
1990
</td>
<td style="text-align:right;">
8553
</td>
</tr>
<tr>
<td style="text-align:left;">
Continuous cultures of fused cells secreting antibody of predefined specificity
</td>
<td style="text-align:right;">
1975
</td>
<td style="text-align:right;">
7772
</td>
</tr>
<tr>
<td style="text-align:left;">
A general method applicable to the search for similarities in the amino acid sequence of two proteins
</td>
<td style="text-align:right;">
1970
</td>
<td style="text-align:right;">
6370
</td>
</tr>
<tr>
<td style="text-align:left;">
Gapped BLAST and PSI-BLAST: a new generation of protein database search programs
</td>
<td style="text-align:right;">
1997
</td>
<td style="text-align:right;">
5978
</td>
</tr>
<tr>
<td style="text-align:left;">
Amino acid substitution matrices from protein blocks
</td>
<td style="text-align:right;">
1992
</td>
<td style="text-align:right;">
2347
</td>
</tr>
<tr>
<td style="text-align:left;">
Melamine Deaminase and Atrazine Chlorohydrolase: 98 Percent Identical but Functionally Different
</td>
<td style="text-align:right;">
2001
</td>
<td style="text-align:right;">
1692
</td>
</tr>
<tr>
<td style="text-align:left;">
Comparison of biosequences
</td>
<td style="text-align:right;">
1981
</td>
<td style="text-align:right;">
1627
</td>
</tr>
<tr>
<td style="text-align:left;">
Rapid and efficient site-specific mutagenesis without phenotypic selection
</td>
<td style="text-align:right;">
1985
</td>
<td style="text-align:right;">
1568
</td>
</tr>
<tr>
<td style="text-align:left;">
The tac promoter: a functional hybrid derived from the trp and lac promoters
</td>
<td style="text-align:right;">
1983
</td>
<td style="text-align:right;">
1161
</td>
</tr>
<tr>
<td style="text-align:left;">
One-step inactivation of chromosomal genes in Escherichia coli K-12 using PCR products
</td>
<td style="text-align:right;">
2000
</td>
<td style="text-align:right;">
836
</td>
</tr>
</tbody>
</table>
<p>The availability of this type of data opens up a wide range of research opportunities and the maximum export of 50,000 records per query is likely to prove ample for most research purposes. These opportunities include.</p>
<ol style="list-style-type: lower-alpha">
<li>Refining search strategies by text mining the titles and abstracts and keywords of cited literature;</li>
<li>Exploring networks of patent documents citing a key piece of literature;</li>
<li>Examining available data on the funding of scientific research cited in patent documents as part of innovation studies;</li>
<li>Assessing issues around the closeness of scientific research to inventions.</li>
</ol>
<p>One illustration of patent exploration using our sample dataset might involve genome editing with CRISPR/CAS9. Patent activity in this field has attracted widespread attention following litigation between the University of California and the Harvard-MIT Broad Institute and the patent landscape linked to the CRISPR dispute is discussed in detail by <span class="citation">Egelie et al. (<a href="#ref-Egelie_2016" role="doc-biblioref">2016</a>)</span> (see also <span class="citation">Ledford (<a href="#ref-Ledford_2016" role="doc-biblioref">2016</a>)</span>, <span class="citation">Ledford (<a href="#ref-Ledford_2017" role="doc-biblioref">2017</a>)</span>, <span class="citation">Ledford (<a href="#ref-Ledford_2018" role="doc-biblioref">2018</a>)</span>). While not linked directly to patent activity, genome editing in humans using CRISPR/CAS9 has also recently attracted international media attention <span class="citation">(<a href="#ref-Cyranoski_2018" role="doc-biblioref">Cyranoski and Ledford 2018</a>)</span>.</p>
<p>Two key researchers in this field are Jennifer Doudna at Berkeley and Feng Zhang at the Harvard-MIT Broad Institute. Both appear in the NPL citations for our simple patent query for synthetic biology. By splitting the data so that each author name appears on its own row we can identify our authors of interest in the dataset as show in Table <a href="citations.html#tab:crispr">6.3</a></p>
<table>
<caption>
<span id="tab:crispr">Table 6.3: </span>CRISPR Authors Cited in Patent Literature
</caption>
<thead>
<tr>
<th style="text-align:left;">
Lens ID
</th>
<th style="text-align:left;">
Author/s
</th>
<th style="text-align:right;">
Publication Year
</th>
<th style="text-align:left;">
Title
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
<a href="https://www.lens.org/018-893-049-787-145" class="uri">https://www.lens.org/018-893-049-787-145</a>
</td>
<td style="text-align:left;">
Jennifer A. Doudna
</td>
<td style="text-align:right;">
2016
</td>
<td style="text-align:left;">
New CRISPR-Cas systems from uncultivated microbes.
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="https://www.lens.org/049-383-529-000-174" class="uri">https://www.lens.org/049-383-529-000-174</a>
</td>
<td style="text-align:left;">
Jennifer A. Doudna
</td>
<td style="text-align:right;">
2014
</td>
<td style="text-align:left;">
Enhanced homology-directed human genome engineering by controlled timing of CRISPR/Cas9 delivery.
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="https://www.lens.org/009-802-337-761-385" class="uri">https://www.lens.org/009-802-337-761-385</a>
</td>
<td style="text-align:left;">
Feng Zhang
</td>
<td style="text-align:right;">
2015
</td>
<td style="text-align:left;">
Cpf1 is a single RNA-guided endonuclease of a class 2 CRISPR-Cas system.
</td>
</tr>
<tr>
<td style="text-align:left;">
<a href="https://www.lens.org/150-951-500-543-136" class="uri">https://www.lens.org/150-951-500-543-136</a>
</td>
<td style="text-align:left;">
Feng Zhang
</td>
<td style="text-align:right;">
2013
</td>
<td style="text-align:left;">
Double Nicking by RNA-Guided CRISPR Cas9 for Enhanced Genome Editing Specificity
</td>
</tr>
</tbody>
</table>
<p>In Table <a href="citations.html#tab:crispr">6.3</a> we see four publications by the key researchers cited in the sample dataset. Clicking on one of the links provides access to details on the records and also to patent documents that cite the literature at the Lens. In Figure <a href="citations.html#fig:citedworks">6.1</a> we have used the record in the third row of Table <a href="citations.html#tab:crispr">6.3</a> above. In Figure <a href="citations.html#fig:citedworks">6.1</a> we can see the publication record and also the 170 patent documents that cite this paper.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:citedworks"></span>
<img src="images/citations/examplecited.png" alt="Patent Documents Citing A Key CRISPR Research Article" width="682" />
<p class="caption">
Figure 6.1: Patent Documents Citing A Key CRISPR Research Article
</p>
</div>
<p>This suggests that one opportunity for using the cited literature identified from a raw search is to begin to build a portfolio of documents that cite key researchers in a field such as CRISPR. In the case of the four articles identified above we simply used the hyperlink to access the data, selected Patent Citations and then View full patent data. By viewing the full patent data for each link we were able to add the citing patents to a new CRISPR collection in a few minutes (you must be logged in to create a collection). At the time of writing a collection of 304 patent documents in 256 families linked to these CRISPR articles . This collection is is publicly accessible at <a href="https://www.lens.org/lens/collection/167967">https://www.lens.org/lens/collection/167967</a>.</p>
<p>As this makes clear, linking the scientific and patent literature together makes it very easy to construct an exploratory patent portfolio in minutes. In the past this might have taken weeks or months. However, it is now also possible to work on a larger scale.</p>
</div>
<div id="retrieving-citations-at-scale-with-patcite" class="section level4 hasAnchor" number="6.0.2.2">
<h4><span class="header-section-number">6.0.2.2</span> Retrieving Citations at Scale with PATCITE<a href="citations.html#retrieving-citations-at-scale-with-patcite" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The <a href="https://www.lens.org/lens/patcite">PATCITE tool</a> in the Lens is a recent introduction that allows a user to paste in a set of article or patent identifiers to retrieve citation data. The advantage of PATCITE is that it is possible to do this in bulk at the level of thousands of identifiers, such as the widespread Document Object Identifiers (dois) for the scientific literature. This will normally be more convenient when working with data from other datasets.</p>
<p>To briefly illustrate PATCITE we will use the four documents identified from the CRISPR publications identified above.</p>
<blockquote>
<p>“10.1038/nature21059
10.7554/elife.04766
10.1016/j.cell.2015.09.038
10.1016/j.cell.2013.08.021”</p>
</blockquote>
<p>We paste these numbers into the option to <code>Explore the cited scholarly work found in patent literature</code>. This will then produce a screen showing the four articles and a Citing Patents list. At the time of writing this list contained 304 Members. Figure <a href="citations.html#fig:patcite">6.2</a> displays the summary of results and demonstrates that it is possible to retrieve a patent portfolio based on the use of document identifiers (dois) that can then be exported (see Export results in the bottom left of Figure <a href="citations.html#fig:patcite">6.2</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:patcite"></span>
<img src="images/citations/patcite.png" alt="PATCITE Results for Four Key CRISPR Articles" width="1570" />
<p class="caption">
Figure 6.2: PATCITE Results for Four Key CRISPR Articles
</p>
</div>
<p>PATCITE also features analysis tools such as rankings of applicants citing the documents and the visualisation of networks of citations. The visualisation of networks is likely to be of particular interest as it allows for the exploration of other literature cited in a patent document. By way of illustration, Figure <a href="citations.html#fig:patcitenetwork">6.3</a> shows all patent citations linked to a literature record.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:patcitenetwork"></span>
<img src="images/citations/patcite_network.png" alt="Literature Citation Network for A CRISPR related Patent Filing" width="714" />
<p class="caption">
Figure 6.3: Literature Citation Network for A CRISPR related Patent Filing
</p>
</div>
<p>PATCITE includes options to export both the cited literature (where using patent numbers as the starting point) or citing patent documents. This is particularly useful when working at a larger scale.</p>
<p>In recent work on synthetic biology, <span class="citation">Oldham and Hall (<a href="#ref-Oldham483826" role="doc-biblioref">2018</a>)</span> mapped authors of scientific articles on synthetic biology identified in Web of Science into the global patent system by matching author names to inventor names. The literature dataset on which the research was based consisted of 4,463 publications containing 3,970 dois. These dois can be accessed <a href="https://osf.io/2xa5y/">here</a> if you would like to reproduce this test.</p>
<p>Figure <a href="citations.html#fig:patciteciting">6.4</a> reveals that PATCITE identified 893 of the 3,970 scientific article identifiers in patent documents. Figure <a href="citations.html#fig:patciteciting">6.4</a> displays the scientific records with the highest number of citing patent documents from a set of 2,349 patent families and 3,323 patent documents.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:patciteciting"></span>
<img src="images/citations/patcite_citinglit.png" alt="Patent Citations for Synthetic Biology Baseline Literature Dataset" width="1541" />
<p class="caption">
Figure 6.4: Patent Citations for Synthetic Biology Baseline Literature Dataset
</p>
</div>
<p>Each of the datasets can be downloaded in Excel format for further analysis. As this example makes clear PATCITE addresses issues of scale in exploring the relationship between the scientific literature and the patent literature. As discussed above in the case of CRISPR this opens up the possibility of creating collections of patent data based on links with the scientific literature either as a starting point for a search strategy, mapping the impacts of research, or exploring the closeness of relationships between the scientific literature and the patent literature in innovation studies.</p>
<p>In the case of new and emerging areas of science and technology, such as synthetic biology PATCITE also opens up the possibility of overcoming some of the limitations of key word based searches. In the case of synthetic biology it can be argued that it is emerging <em>within</em> the wider field of genetic engineering and biotechnology and uses much the same language. This makes it difficult to develop a keyword strategy that adequately captures the field without capturing unrelated activity. At the same time, an additional challenge with keyword strategies is that analysts through the selection of particular terms inevitably impose their own definitions on emerging fields. For example, in the case of synthetic biology should we assume that any reference to a synthetic gene, or to protein engineering or systems biology in a patent document should be treated as synthetic biology?</p>
<p>PATCITE offers the possibility of beginning these explorations directly from the scientific literature and following through into the patent literature and merits serious consideration by researchers seeking to map emerging areas of science and technology in the patent system. Specifically, literature based patent searching could provide the basis for landscape construction and also be used as part of a strategy for validating the outcomes of key word based queries.</p>
<p>One logical question for researchers seeking to match the scientific literature into patent data is the issue of data capture. That is, it is not immediately clear whether the 3,077 dois that were not identified in PATCITE were not identified because they are absent from patent data or because of limitations in capture. The answer to this question may be a combination of the two. In practice, the ability of an analyst to interrogate data capture at the database level, such as the accurate identification of dois, is likely to be limited. However, we can gain an insight into these issues using the raw citation data from the US PatentsView service.</p>
</div>
<div id="the-us-patentsview-non-patent-literature-table" class="section level4 hasAnchor" number="6.0.2.3">
<h4><span class="header-section-number">6.0.2.3</span> The US PatentsView Non-Patent Literature Table<a href="citations.html#the-us-patentsview-non-patent-literature-table" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For most patent analysts the Lens literature and PATCITE service is the logical starting point for research, for the straight forward reason that it is so easy to use and can generate a targeted patent collection for exploration within a few minutes. However, this may not suit all purposes, particularly where larger scale data is required. It is also a very good idea to have an understanding of what the raw NPL looks like in understanding the strengths and limitations of different databases.</p>
<p>In the case of offline patent databases such as PATSTAT a table is available containing the non-patent literature for subscribers (also accessible through the online version of PATSTAT). However, the USPTO, through the PatentsView service makes a non-patent literature table available for download (presently as a 2.7 GB tab separated zipped file).<a href="#fn23" class="footnote-ref" id="fnref23"><sup>23</sup></a></p>
<p>Engaging with the raw non-patent literature data reveals that it is a free form text field. Table <a href="citations.html#tab:patentsview">6.4</a> shows a sample of entries from the over 6 million entries in the 2018 USPTO PatentsView non-patent literature table.</p>
<table>
<caption>
<span id="tab:patentsview">Table 6.4: </span>A sample of Non Patent Literature
</caption>
<thead>
<tr>
<th style="text-align:left;">
patent_id
</th>
<th style="text-align:left;">
text
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
9339622
</td>
<td style="text-align:left;">
English Translation of Chinese Examination Report; Application No. 2007800266164; 5 pages.
</td>
</tr>
<tr>
<td style="text-align:left;">
5013322
</td>
<td style="text-align:left;">
Surgery News-An Advertising supplement, Aug. 1, 1985, vol. 3, No. 15, Clayman Ovoid Model No. 8743 and Kratz/Johnson 7 mm Lightweight Model No. 8663, (2 pages).
</td>
</tr>
<tr>
<td style="text-align:left;">
8773357
</td>
<td style="text-align:left;">
U.S. Office Action dated Dec. 23, 2011 in U.S. Appl. No. 12/571,157.
</td>
</tr>
<tr>
<td style="text-align:left;">
7307640
</td>
<td style="text-align:left;">
Duke, “Dreamcast Technical Specs”, Sega Dreamcast Review, Sega, Feb. 1999, www.game-revolution.com.
</td>
</tr>
<tr>
<td style="text-align:left;">
8543711
</td>
<td style="text-align:left;">
Ranjan, S. and Rolia, J., Fu, H., and Knightly, E., “QoS-Driven Server Migration for Internet Data Centers,” In Proc. of IWQoS 2002, Miami, FL, May 2002.
</td>
</tr>
<tr>
<td style="text-align:left;">
5849555
</td>
<td style="text-align:left;">
" J. Hughes et al., ""How Does Pseudomonas Fluorescens, the Producing Organism of the Antibiotic Pseudomonic Acid A, Avoid Suicide?"", FEBS Letters, 122(2) pp. 322-324 (1980). "
</td>
</tr>
<tr>
<td style="text-align:left;">
8811330
</td>
<td style="text-align:left;">
Kaitz et al., “Changing the status of Subchannelization in OFDM mode,” IEEE 802.16 Broadband Wireless Access Working Group, IEEE C802.16d-03/80, IEEE, New York, New York (Nov. 13, 2003).
</td>
</tr>
<tr>
<td style="text-align:left;">
9500933
</td>
<td style="text-align:left;">
Jain et al., “Efficient Nonlinear Frequency Conversion with Maximal Atomic Coherence”, The American Physical Society, Physical Review Letters, vol. 77, No. 21, Nov. 18, 1996, pp. 4326-4329, 4 pages.
</td>
</tr>
<tr>
<td style="text-align:left;">
6653062
</td>
<td style="text-align:left;">
English Translation of Migulina.
</td>
</tr>
<tr>
<td style="text-align:left;">
8137555
</td>
<td style="text-align:left;">
Communication Relating to the Results of the Partial International Search for corresponding International Patent Application No. PCT/US2011/031412 mailed Aug. 9, 2011.
</td>
</tr>
<tr>
<td style="text-align:left;">
6855523
</td>
<td style="text-align:left;">
Biebricher, et al. (1986) Nature 321: 89-91.
</td>
</tr>
<tr>
<td style="text-align:left;">
8960456
</td>
<td style="text-align:left;">
Office Action issued Oct. 4, 2013 in U.S. Appl. No. 13/268,712 by Didehvar.
</td>
</tr>
<tr>
<td style="text-align:left;">
6576467
</td>
<td style="text-align:left;">
Feinberg et al., &amp;#8220;A Technique for Radiolabeling DNA Restruction Endonuclease Fragments to High Specific Activity,&amp;#8221;
</td>
</tr>
<tr>
<td style="text-align:left;">
8735564
</td>
<td style="text-align:left;">
Li et al; Detection of Human Papillomavirus Genotypes With Liquid Bead Microarray in Cervical Lesions of Northern Chinese Patients; Cancer Genetics and Cytogenetics, Elsevier Science Publishing, New York, NY, US; vol. 182; No. 1; Mar. 6, 2008; pp. 12-17; Abstract.
</td>
</tr>
<tr>
<td style="text-align:left;">
9440232
</td>
<td style="text-align:left;">
Fungi (Wikipedia.com accessed Jun. 3, 2013).
</td>
</tr>
<tr>
<td style="text-align:left;">
9169348
</td>
<td style="text-align:left;">
USPTO Office Action dated Sep. 9, 2008 for copending U.S. Appl. No. 11/391/571.
</td>
</tr>
<tr>
<td style="text-align:left;">
6940750
</td>
<td style="text-align:left;">
Jian-Gang Zhu et al. “Ultrahigh Density Vertical Magnetoresistive Random Access Memory (Invited),” Journal of Applied Physics, vol. 87, No. 9, May 1, 2000, pp. 6668-6673.
</td>
</tr>
<tr>
<td style="text-align:left;">
8343171
</td>
<td style="text-align:left;">
U.S. Appl. No. 60/990,062, filed Nov. 26, 2007.
</td>
</tr>
<tr>
<td style="text-align:left;">
8046478
</td>
<td style="text-align:left;">
MCL Paper Abstracts; Ahanger et al.; “A Language to Support Automatic Composition of Newscasts”; Journal of Compuoter Information Technology ; vol. 6, No. 3; 1998.
</td>
</tr>
<tr>
<td style="text-align:left;">
9763641
</td>
<td style="text-align:left;">
Ophir et al., “Elastography: Ultrasonic Estimation and Imaging of the Elastic Properties of Tissues,” Proc Instn Mech Engrs 213(Part H) (1999) 203-233.
</td>
</tr>
</tbody>
</table>
<p>As we can see from this small sample of over 6 million entries in the USPTO NPL data, the individual entry fields can reasonably be described as a messy text field. Among the issues that we encounter are partial references, spelling mistakes such as “Journal of Computer Information Technology”, abbreviations such as “Proc Instn Mech Engrs” and considerable variation in the presence of dois that will all need to be addressed to successfully extract the literature references.</p>
<p>To extract meaningful information from this table we would need to think about identifying patterns. For example, we might look for document identifiers (dois) for the scientific literature and note that most begin with <code>https:://doi.org</code>. We would then discover that the references to dois within the table are limited and might switch to using titles to cross match with other databases such as Crossref or PubMed. In short, when seeking to work with the NPL data, experimentation with regular expression based pattern matching and development of a strategy would rapidly become necessary to achieve meaningful results.</p>
<p>To illustrate this we will use the example of web addresses in this table. While our aim is not complete accuracy in the extraction of web addresses, we can illustrate the growing relevance of web sites as sources of prior art in the US patent data.</p>
<p>If we were looking for web addresses we could use an approach that detects the presence of <code>http</code> in a reference as a distinctive string. In practice this would filter this large table down to 304,590 entries containing this term. We would then do further work to identify the domains etc. using a regular expression pattern such as <code>www\\..*?\\.com</code>. We could also look at modifying the regular expression pattern to capture alternative URL endings such as <code>.org, .net</code> etc. This is certainly doable but could rapidly become quite complicated.</p>
<p>An alternative approach would be to recognise that others have worked on this kind of problem before with similar types of text data. We can therefore look at using existing solutions for this particular task rather than repeating work on writing regular expression patterns. In the case of the R programming language a solution to this problem is provided by Tyler Rinkr’s recent <code>qdapregex</code> package in R that complements his larger qualitative data analysis package <code>qdap</code>. We would install and load this package as follows.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="citations.html#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;qdapregex&quot;</span>)</span>
<span id="cb90-2"><a href="citations.html#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(qdapRegex)</span></code></pre></div>
<p><code>qdapregex</code> contains a function to extract urls from texts called <code>ex_url()</code> without needing to work on regular expressions. Here we create a new web object containing the text and extract the urls with <code>ex_url()</code> (for extract url). What we would like to do is to identify the top domain names (such as <code>google.com</code>) appearing in the references table.</p>
<p>In reality the way in which URLs are expressed in the references is quite messy and requires quite a lot of tidying up. We would probably need to do some more work to tidy up and validate the data for truly accurate results, but the code below takes us most of the way for the purposes of illustration.</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="citations.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb91-2"><a href="citations.html#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(qdapRegex)</span>
<span id="cb91-3"><a href="citations.html#cb91-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(stringi)</span>
<span id="cb91-4"><a href="citations.html#cb91-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-5"><a href="citations.html#cb91-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Our aim is to extract urls and then reduce to the domain</span></span>
<span id="cb91-6"><a href="citations.html#cb91-6" aria-hidden="true" tabindex="-1"></a><span class="co"># ex_url returns a list object</span></span>
<span id="cb91-7"><a href="citations.html#cb91-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-8"><a href="citations.html#cb91-8" aria-hidden="true" tabindex="-1"></a>web <span class="ot">&lt;-</span> npl<span class="sc">$</span>text <span class="sc">%&gt;%</span> </span>
<span id="cb91-9"><a href="citations.html#cb91-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ex_url</span>()</span>
<span id="cb91-10"><a href="citations.html#cb91-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-11"><a href="citations.html#cb91-11" aria-hidden="true" tabindex="-1"></a><span class="co"># process the list and return a data frame</span></span>
<span id="cb91-12"><a href="citations.html#cb91-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-13"><a href="citations.html#cb91-13" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> web <span class="sc">%&gt;%</span></span>
<span id="cb91-14"><a href="citations.html#cb91-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">map</span>(., <span class="st">`</span><span class="at">[[</span><span class="st">`</span>, <span class="dv">1</span>) <span class="sc">%&gt;%</span> <span class="co"># extract the first element from the list of results</span></span>
<span id="cb91-15"><a href="citations.html#cb91-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">discard</span>(., is.na) <span class="sc">%&gt;%</span> <span class="co"># drop NA for Not Available</span></span>
<span id="cb91-16"><a href="citations.html#cb91-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">tibble</span>(<span class="at">url =</span> .) <span class="sc">%&gt;%</span> <span class="co"># convert to tibble</span></span>
<span id="cb91-17"><a href="citations.html#cb91-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">unnest</span>() <span class="sc">%&gt;%</span> <span class="co"># unnest list column</span></span>
<span id="cb91-18"><a href="citations.html#cb91-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">url =</span> <span class="fu">str_replace_all</span>(.<span class="sc">$</span>url, <span class="st">&quot;http://|https:|http|http:|:|//&quot;</span>, <span class="st">&quot;&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb91-19"><a href="citations.html#cb91-19" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">url =</span> <span class="fu">str_replace_all</span>(.<span class="sc">$</span>url, <span class="st">&quot;www.|&gt;&quot;</span>, <span class="st">&quot;&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb91-20"><a href="citations.html#cb91-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">domain =</span> <span class="fu">sub</span>(<span class="st">&quot;/.*&quot;</span>, <span class="st">&quot;&quot;</span>, url)) <span class="sc">%&gt;%</span> </span>
<span id="cb91-21"><a href="citations.html#cb91-21" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">domain =</span> <span class="fu">str_trim</span>(domain, <span class="at">side =</span> <span class="st">&quot;both&quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb91-22"><a href="citations.html#cb91-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">domain =</span> stringi<span class="sc">::</span><span class="fu">stri_reverse</span>(domain)) <span class="sc">%&gt;%</span> <span class="co"># reverse string </span></span>
<span id="cb91-23"><a href="citations.html#cb91-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">domain =</span> <span class="fu">str_replace</span>(domain, <span class="st">&quot;^[.]|^,|^;&quot;</span>, <span class="st">&quot;&quot;</span>)) <span class="sc">%&gt;%</span>  <span class="co"># remove junk</span></span>
<span id="cb91-24"><a href="citations.html#cb91-24" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">domain =</span> stringi<span class="sc">::</span><span class="fu">stri_reverse</span>(domain)) <span class="co"># reverse back</span></span>
<span id="cb91-25"><a href="citations.html#cb91-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb91-26"><a href="citations.html#cb91-26" aria-hidden="true" tabindex="-1"></a><span class="co"># count up domains and filter out blank results</span></span>
<span id="cb91-27"><a href="citations.html#cb91-27" aria-hidden="true" tabindex="-1"></a>domain <span class="ot">&lt;-</span> url <span class="sc">%&gt;%</span> </span>
<span id="cb91-28"><a href="citations.html#cb91-28" aria-hidden="true" tabindex="-1"></a>  <span class="fu">count</span>(domain, <span class="at">sort =</span> <span class="cn">TRUE</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb91-29"><a href="citations.html#cb91-29" aria-hidden="true" tabindex="-1"></a>  <span class="fu">filter</span>(domain <span class="sc">!=</span> <span class="st">&quot;&quot;</span>)</span></code></pre></div>
<p>This code parses the results down to 214,756 domains</p>
<table>
<caption>
<span id="tab:viewurllink">Table 6.5: </span>Top Domains in PatentsView Non-Patent Literature
</caption>
<thead>
<tr>
<th style="text-align:left;">
domain
</th>
<th style="text-align:right;">
n
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
web.archive.org
</td>
<td style="text-align:right;">
12973
</td>
</tr>
<tr>
<td style="text-align:left;">
en.wikipedia.org
</td>
<td style="text-align:right;">
10563
</td>
</tr>
<tr>
<td style="text-align:left;">
gsmarena.com
</td>
<td style="text-align:right;">
3962
</td>
</tr>
<tr>
<td style="text-align:left;">
ncbi.nlm.nih.gov
</td>
<td style="text-align:right;">
3277
</td>
</tr>
<tr>
<td style="text-align:left;">
ieeexplore.ieee.org
</td>
<td style="text-align:right;">
2879
</td>
</tr>
<tr>
<td style="text-align:left;">
youtube.com
</td>
<td style="text-align:right;">
2834
</td>
</tr>
<tr>
<td style="text-align:left;">
msdn.microsoft.com
</td>
<td style="text-align:right;">
1935
</td>
</tr>
<tr>
<td style="text-align:left;">
amazon.com
</td>
<td style="text-align:right;">
1819
</td>
</tr>
<tr>
<td style="text-align:left;">
microsoft.com
</td>
<td style="text-align:right;">
1562
</td>
</tr>
<tr>
<td style="text-align:left;">
citeseerx.ist.psu.edu
</td>
<td style="text-align:right;">
1506
</td>
</tr>
<tr>
<td style="text-align:left;">
w3.org
</td>
<td style="text-align:right;">
1490
</td>
</tr>
<tr>
<td style="text-align:left;">
ietf.org
</td>
<td style="text-align:right;">
1428
</td>
</tr>
<tr>
<td style="text-align:left;">
3gpp.org
</td>
<td style="text-align:right;">
1108
</td>
</tr>
<tr>
<td style="text-align:left;">
research.microsoft.com
</td>
<td style="text-align:right;">
1093
</td>
</tr>
<tr>
<td style="text-align:left;">
clinicaltrials.gov
</td>
<td style="text-align:right;">
936
</td>
</tr>
<tr>
<td style="text-align:left;">
cisco.com
</td>
<td style="text-align:right;">
922
</td>
</tr>
<tr>
<td style="text-align:left;">
google.com
</td>
<td style="text-align:right;">
920
</td>
</tr>
<tr>
<td style="text-align:left;">
tools.ietf.org
</td>
<td style="text-align:right;">
909
</td>
</tr>
<tr>
<td style="text-align:left;">
sciencedirect.com
</td>
<td style="text-align:right;">
857
</td>
</tr>
<tr>
<td style="text-align:left;">
merriam-webster.com
</td>
<td style="text-align:right;">
850
</td>
</tr>
</tbody>
</table>
<p>While this data would require further cleaning we now have a working idea of what the top web domains are across the US patent collection. In particular we can see that applicants make particular use of the Internet Archive at <a href="http://web.archive.org/">http://web.archive.org/</a> and the English language version of Wikipedia, with the third result focusing on the Global System for Mobile Communication (GSM) website GSM Arena <a href="https://www.gsmarena.com">https://www.gsmarena.com/</a>. We can also see that in some cases such as Microsoft (or Google), specific sub-domains such as the Microsoft Developers Network (MSDN) at <a href="https://msdn.microsoft.com/en-us/">https://msdn.microsoft.com/en-us/</a> are included. If we were to do further work we would want to trim these down to the respective core domain. As this suggests, the apparently simple task of extracting and ranking web domains involves more thought than might initially be suggested. However, awareness of existing tools can radically reduce the work involved.</p>
<p>In this section we have seen that access to the non-patent literature in patent databases has improved dramatically in recent years. As a result of the integration of the scientific literature and patent literature by the Lens it is now possible to enter scientific literature of interest and retrieve a patent portfolio in a matter of minutes. Similar developments are taking place among commercial providers such as the subscription based <a href="https://www.dimensions.ai/">Dimensions</a> database that applies machine learning to scientific publications, grant information, clinical trials data and patent data. It is likely that other companies will be working on similar initiatives.</p>
<p>When coupled with other developments such as non-patent literature tables in PATSTAT and access to raw non-patent literature with PatentsView it is clear that large scale analysis of the NPL literature is now possible. The ability to work with such data at scale will typically involve programmatic skills, but it is important to bear in mind that many other fields involve finding solutions to very similar problems, such as extracting URLs from texts. Investment in research on solutions to similar problems will often radically reduce the amount of work required and allow for the detailed exploration of the non-patent literature.</p>
<p>We now turn to the use of patent citations.</p>
</div>
</div>
<div id="patent-citations" class="section level3 hasAnchor" number="6.0.3">
<h3><span class="header-section-number">6.0.3</span> Patent Citations<a href="citations.html#patent-citations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Patent citations are citations to other patent documents. They take the form of backward (cited) and forward (citing) citations. Backward citations, also referred to as back citations or cited patent documents, refer to earlier patent applications or grants that affect the scope of the claims of an application. Forward citations or citing documents refer to later filings of applications that are affected by the scope of the claims of the cited document.</p>
<p>Patent citations have two main sources <span class="citation">(<a href="#ref-Jaffe_2017" role="doc-biblioref">A. B. Jaffe and Rassenfosse 2017</a>; <a href="#ref-Hegde_2009" role="doc-biblioref">Hegde and Sampat 2009</a>)</span>:</p>
<ol style="list-style-type: decimal">
<li>inventors and their patent attorneys</li>
<li>patent examiners</li>
</ol>
<p>The different sources of patent citations have important implications. Specifically, the two different sources of citations may have very different motives for including a citation <span class="citation">(<a href="#ref-Webb_2005" role="doc-biblioref">Webb et al. 2005</a>)</span>. Thus, patent applicants and their attorneys will have an interest in disclosing references that have a limited impact on claims to novelty and inventive step. In the United States, and possibly other jurisdictions, applicants are expected to provide the prior art they are aware of as part of a duty of candour <span class="citation">(<a href="#ref-Webb_2005" role="doc-biblioref">Webb et al. 2005</a>; <a href="#ref-Cotropia_2013" role="doc-biblioref">Cotropia, Lemley, and Sampat 2013</a>)</span>. This may lead to practices such as seeking to draft around the prior art <span class="citation">(<a href="#ref-Cotropia_2013" role="doc-biblioref">Cotropia, Lemley, and Sampat 2013</a>)</span>. This perhaps explains why <span class="citation">Cotropia, Lemley, and Sampat (<a href="#ref-Cotropia_2013" role="doc-biblioref">2013</a>)</span> found that patent examiners typically ignore prior art provided by applicants.</p>
<p>In contrast, patent examiners can be expected to focus more closely on identify those that impact novelty and inventive step. Prior art searches by examiners are widely regarded as the highest quality of citations, because this involves a search by trained examiners for relevant prior art affecting an application. However, it is important to recognise that citation practices vary between patent offices <span class="citation">(<a href="#ref-Jaffe_2002" role="doc-biblioref">A. Jaffe and Trajtenberg 2002</a>; <a href="#ref-Webb_2005" role="doc-biblioref">Webb et al. 2005</a>)</span>. Thus, in the United States examiners are expected to list all relevant prior art while at the European Patent Office the examination guidelines stipulate that the European Search report include only the most <em>relevant</em> references <span class="citation">(<a href="#ref-Webb_2005" role="doc-biblioref">Webb et al. 2005</a>)</span>. The practical upshot of this is that citations from the USPTO will often be longer than those from the EPO. As such, it is important to be aware of the differences between patent offices in citation practices.</p>
<p>Bearing these issues in mind, <span class="citation">A. B. Jaffe and Rassenfosse (<a href="#ref-Jaffe_2017" role="doc-biblioref">2017</a>)</span> highlights that in broad terms patent citations provide insights in two main areas:</p>
<ol style="list-style-type: lower-alpha">
<li>the impact of inventions on other applicants and their economic and social value;</li>
<li>as proxies for knowledge flows and networks.</li>
</ol>
<p>We are now in a position to begin navigating patent citation networks.</p>
</div>
<div id="navigating-patent-networks" class="section level3 hasAnchor" number="6.0.4">
<h3><span class="header-section-number">6.0.4</span> Navigating Patent Networks<a href="citations.html#navigating-patent-networks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the discussion below we will focus on the basic issues involved in navigating patent citations using the CRISPR Cas9 genome editing cases above as our example. We will start with a fictional search of a patent database for CRISPR that identifies <a href="https://www.lens.org/lens/patent/081-370-519-314-433">EP2784162B1</a> from the Harvard-MIT Broad Institute for <code>Engineering of systems, methods and optimized guide compositions for sequence manipulation</code> by inventors Le Cong, Feng Zhang and other collaborators. We will then explore different ways of counting and navigating patent citations.</p>
<p>If you wish to explore the citations discussed below using the Lens you can access the reference document <a href="https://www.lens.org/lens/patent/081-370-519-314-433">EP2784162B1 here</a>. Note that counts of citations may vary across different databases. In the working examples below we use data from Clarivate Analytics Derwent Innovation database. Because of its greater coverage of national collections worldwide Derwent Innovation will often involve higher scores than other databases.</p>
<div id="back-citations" class="section level4 hasAnchor" number="6.0.4.1">
<h4><span class="header-section-number">6.0.4.1</span> Back citations<a href="citations.html#back-citations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Back citations are earlier patent filings (either applicants or grants) that may be listed by the applicant or an examiner during search and/or examination.</p>
<p>Table <a href="citations.html#tab:citedciting">6.6</a> shows a selection of patent documents (applications or grants) cited by EP2784162B1 and patent documents citing this document. In total for EP2784162B1 there were 40 documents in the cited table and 22 documents in the citing table.</p>
<table>
<caption>
<span id="tab:citedciting">Table 6.6: </span>EP2784162B1 Cited to Citing Relationships
</caption>
<thead>
<tr>
<th style="text-align:left;">
cited patents
</th>
<th style="text-align:left;">
publication number
</th>
<th style="text-align:left;">
citing patents
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
US4235871A
</td>
<td style="text-align:left;">
EP2784162B1
</td>
<td style="text-align:left;">
US10000772B2
</td>
</tr>
<tr>
<td style="text-align:left;">
US4774085A
</td>
<td style="text-align:left;">
EP2784162B1
</td>
<td style="text-align:left;">
US10077445B2
</td>
</tr>
<tr>
<td style="text-align:left;">
US4797368A
</td>
<td style="text-align:left;">
EP2784162B1
</td>
<td style="text-align:left;">
WO2016049163A2
</td>
</tr>
<tr>
<td style="text-align:left;">
WO2013176772A1
</td>
<td style="text-align:left;">
EP2784162B1
</td>
<td style="text-align:left;">
WO2016049251A1
</td>
</tr>
<tr>
<td style="text-align:left;">
US20130074819A1
</td>
<td style="text-align:left;">
EP2784162B1
</td>
<td style="text-align:left;">
WO2016069591A2
</td>
</tr>
<tr>
<td style="text-align:left;">
WO2014099744A1
</td>
<td style="text-align:left;">
EP2784162B1
</td>
<td style="text-align:left;">
WO2016182893A1
</td>
</tr>
</tbody>
</table>
<p>Table <a href="citations.html#tab:citedciting">6.6</a> illustrates the basic relationship between a reference document (EP2784162B1), its cited documents (e.g US4235871A) and citing documents (e.g. US10000772B2). The cited documents reference earlier patents and inform us about technologies that are informing and shaping the claims of our target document (EP2784162B1). Table <a href="citations.html#tab:citedselection">6.7</a> shows a selection of the cited documents ranked by the number of later filings that cite them (citing_count). The citing count in this case refers to the global total of citing patent documents. We can clearly see that the older documents are generally those with the highest citations. That is, while many patent documents will never attract citations, of those that do, older documents will typically attract higher citations scores than younger ones <span class="citation">(<a href="#ref-Webb_2005" role="doc-biblioref">Webb et al. 2005</a>)</span>.</p>
<table>
<caption>
<span id="tab:citedselection">Table 6.7: </span>EP2784162B1: Top Cited Patent Documents by Global Citing Count
</caption>
<thead>
<tr>
<th style="text-align:left;">
first_applicant
</th>
<th style="text-align:right;">
citing_count
</th>
<th style="text-align:right;">
cited_patent_count
</th>
<th style="text-align:left;">
publication_number
</th>
<th style="text-align:right;">
publication_year
</th>
<th style="text-align:left;">
title
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
PAPAHADJOPOULOS DEMETRIOS P
</td>
<td style="text-align:right;">
1858
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
US4235871A
</td>
<td style="text-align:right;">
1980
</td>
<td style="text-align:left;">
Method of encapsulating biologically active materials in lipid vesicles
</td>
</tr>
<tr>
<td style="text-align:left;">
Liposome Technology Inc. 
</td>
<td style="text-align:right;">
1276
</td>
<td style="text-align:right;">
4
</td>
<td style="text-align:left;">
US4837028A
</td>
<td style="text-align:right;">
1989
</td>
<td style="text-align:left;">
Liposomes with enhanced circulation time
</td>
</tr>
<tr>
<td style="text-align:left;">
Technology Unlimited Inc. 
</td>
<td style="text-align:right;">
1050
</td>
<td style="text-align:right;">
10
</td>
<td style="text-align:left;">
US4501728A
</td>
<td style="text-align:right;">
1985
</td>
<td style="text-align:left;">
Masking of liposomes from RES recognition
</td>
</tr>
<tr>
<td style="text-align:left;">
Syntex (UA.) Inc. 
</td>
<td style="text-align:right;">
960
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
US4897355A
</td>
<td style="text-align:right;">
1990
</td>
<td style="text-align:left;">
N[&amp;#969;,(&amp;#969;-1)-dialkyloxy]-and N-[&amp;#969;,(&amp;#969;-1)-dialkenyloxy]-alk-1-yl-N,N,N-tetrasubstituted ammonium lipids and uses therefor
</td>
</tr>
<tr>
<td style="text-align:left;">
Biogen Inc. 
</td>
<td style="text-align:right;">
828
</td>
<td style="text-align:right;">
13
</td>
<td style="text-align:left;">
US4873316A
</td>
<td style="text-align:right;">
1989
</td>
<td style="text-align:left;">
Isolation of exogenous recombinant proteins from the milk of transgenic mammals
</td>
</tr>
</tbody>
</table>
<p>One issue to bear in mind when working with patent citations is that there are a variety of reasons why a citation may be awarded. In some cases what may appear to be a relatively minor technical feature of a claimed invention may attract a citation from a patent document in an otherwise unrelated field. This will reflect the reality that patent claims are considered against the much wider background of the existing prior art. We will come back to this issue below. Furthermore, cited patents that have received the largest number of citations may not in fact be the closest to the new claimed invention. In this case the most important cited document is <a href="https://www.lens.org/lens/patent/076-607-252-634-172">WO2013176772A1</a> relating to RNA directed target DNA modification and modulation of transcription.<a href="#fn24" class="footnote-ref" id="fnref24"><sup>24</sup></a> Table <a href="citations.html#tab:citeddoudna">6.8</a> displays this family member filed by Jennifer Doudna and colleagues at the University of California at Berkeley with an earliest priority from the 25th of May 2012 (provisional application US201261652086P).</p>
<table>
<caption>
<span id="tab:citeddoudna">Table 6.8: </span>The Berkeley CRISPR Patent
</caption>
<thead>
<tr>
<th style="text-align:left;">
first_applicant
</th>
<th style="text-align:right;">
citing_count
</th>
<th style="text-align:right;">
cited_patent_count
</th>
<th style="text-align:left;">
publication_number
</th>
<th style="text-align:right;">
publication_year
</th>
<th style="text-align:left;">
title
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
THE REGENTS OF THE UNIVERSITY OF CALIFORNIA
</td>
<td style="text-align:right;">
307
</td>
<td style="text-align:right;">
2
</td>
<td style="text-align:left;">
WO2013176772A1
</td>
<td style="text-align:right;">
2013
</td>
<td style="text-align:left;">
METHODS AND COMPOSITIONS FOR RNA-DIRECTED TARGET DNA MODIFICATION AND FOR RNA-DIRECTED MODULATION OF TRANSCRIPTION
</td>
</tr>
</tbody>
</table>
<p>In this case we can see that a relatively young document has attracted a very significant number of citing documents relative to its age. In practice, documents that are relatively young but are attracting a significant number of citations should be an important focus of our attention. These will often either be highly original, and therefore disruptive in the technology space, and/or their claims will be broadly drawn. While older documents may attract more citations this may apply to a wider range of fields. Younger documents that are attracting citations are likely in technology terms to be closer to the reference document (EP2784162B1). This is particularly true where the cited document and the target document share the same International Patent Classification (IPC) or Cooperative Patent Classification (CPC) codes.</p>
<p>Thus, the cited document with the highest level of citations in our set <a href="https://www.lens.org/lens/patent/060-160-799-009-187">US4235871A</a> shares the C12N subclass with the reference document <a href="https://www.lens.org/lens/patent/015-360-099-978-897">EP2784162A1</a> and also shares the C12N15 group. However, it does not share a sub-group with the reference document. In contrast our Berkeley patent document <a href="https://www.lens.org/lens/patent/076-607-252-634-172">WO2013176772A1</a> shares IPC code C12N15/63 and Cooperative Patent Classification (CPC) codes C12N15/102, C12N15/113, C12N15/63, C12N15/907 and C12N9/22. The fact that the documents share a significant number of classification codes suggests technological closeness and, in this case, rivalry. The rivalrous nature of this particular case will become clearer when we consider forward citations.</p>
</div>
</div>
<div id="forward-citations" class="section level3 hasAnchor" number="6.0.5">
<h3><span class="header-section-number">6.0.5</span> Forward Citations<a href="citations.html#forward-citations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As noted above, forward citations or citing documents are later patent filings that cite a reference document (EP2784162B1). Table <a href="citations.html#tab:citingselection">6.9</a> displays a selection of the documents citing our reference document from the Broad Institute (<a href="https://www.lens.org/lens/patent/015-360-099-978-897">EP2784162B1</a>). The first point that stands out is that this data is dominated by the Broad Institute (which is a joint Harvard and MIT entity), and MIT with the bulk of forward citations arising from other filings by the Broad Institute and MIT accounting for 17, Brigham and Womens Hospital in Boston for 2, the University of California accounting for 2 and Vertex Pharmaceuticals for 1 citation.</p>
<table>
<caption>
<span id="tab:citingselection">Table 6.9: </span>A selection of Patent Documents Citing EP2784162B1
</caption>
<thead>
<tr>
<th style="text-align:left;">
first_applicant
</th>
<th style="text-align:right;">
cited_patent_count
</th>
<th style="text-align:right;">
citing_count
</th>
<th style="text-align:left;">
publication_number
</th>
<th style="text-align:right;">
publication_year
</th>
<th style="text-align:left;">
title
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
University of California
</td>
<td style="text-align:right;">
241
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
US10000772B2
</td>
<td style="text-align:right;">
2018
</td>
<td style="text-align:left;">
Methods and compositions for RNA-directed target DNA modification and for RNA-directed modulation of transcription
</td>
</tr>
<tr>
<td style="text-align:left;">
University of California
</td>
<td style="text-align:right;">
262
</td>
<td style="text-align:right;">
0
</td>
<td style="text-align:left;">
US10077445B2
</td>
<td style="text-align:right;">
2018
</td>
<td style="text-align:left;">
Methods and compositions for RNA-directed target DNA modification and for RNA-directed modulation of transcription
</td>
</tr>
<tr>
<td style="text-align:left;">
THE BROAD INSTITUTE INC
</td>
<td style="text-align:right;">
134
</td>
<td style="text-align:right;">
8
</td>
<td style="text-align:left;">
WO2016049163A2
</td>
<td style="text-align:right;">
2016
</td>
<td style="text-align:left;">
USE AND PRODUCTION OF CHD8+/- TRANSGENIC ANIMALS WITH BEHAVIORAL PHENOTYPES CHARACTERISTIC OF AUTISM SPECTRUM DISORDER
</td>
</tr>
<tr>
<td style="text-align:left;">
THE BROAD INSTITUTE INC
</td>
<td style="text-align:right;">
104
</td>
<td style="text-align:right;">
9
</td>
<td style="text-align:left;">
WO2016049251A1
</td>
<td style="text-align:right;">
2016
</td>
<td style="text-align:left;">
DELIVERY, USE AND THERAPEUTIC APPLICATIONS OF THE CRISPR-CAS SYSTEMS AND COMPOSITIONS FOR MODELING MUTATIONS IN LEUKOCYTES
</td>
</tr>
<tr>
<td style="text-align:left;">
THE BROAD INSTITUTE INC
</td>
<td style="text-align:right;">
98
</td>
<td style="text-align:right;">
6
</td>
<td style="text-align:left;">
WO2016069591A2
</td>
<td style="text-align:right;">
2016
</td>
<td style="text-align:left;">
COMPOSITIONS, METHODS AND USE OF SYNTHETIC LETHAL SCREENING
</td>
</tr>
</tbody>
</table>
<p>An alternative way of viewing patent citations is to visualise them as a tree of relationships. Figure <a href="citations.html#fig:citingnet1">6.5</a> displays the set of citing applicants and citing documents for EP2784162B1 as a tree grouped on the first applicant.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:citingnet1"></span>
<div id="htmlwidget-e1cec9891e32613b0a08" style="width:600px;height:480px;" class="collapsibleTree html-widget"></div>
<script type="application/json" data-for="htmlwidget-e1cec9891e32613b0a08">{"x":{"data":{"name":".","children":[{"name":"University of California","children":[{"name":"US10000772B2"},{"name":"US10077445B2"}]},{"name":"THE BROAD INSTITUTE INC","children":[{"name":"WO2016049163A2"},{"name":"WO2016049251A1"},{"name":"WO2016069591A2"},{"name":"WO2016182893A1"},{"name":"WO2016205745A2"},{"name":"WO2017070605A1"},{"name":"WO2017074788A1"},{"name":"WO2017075451A1"},{"name":"WO2017075465A1"},{"name":"WO2017075478A2"},{"name":"WO2017219027A1"},{"name":"WO2018035250A1"},{"name":"WO2018035364A1"},{"name":"WO2018049025A2"}]},{"name":"MIT","children":[{"name":"WO2016205728A1"},{"name":"WO2017147196A1"},{"name":"WO2017161325A1"}]},{"name":"THE BRIGHAM AND WOMEN'S HOSPITAL INC","children":[{"name":"WO2017069958A2"},{"name":"WO2018067991A1"}]},{"name":"VERTEX PHARMACEUTICALS INCORPORATED","children":[{"name":"WO2018013840A1"}]}]},"options":{"hierarchy":["first_applicant","publication_number"],"input":null,"attribute":"leafCount","linkLength":220,"fontSize":10,"tooltip":false,"collapsed":false,"zoomable":false,"margin":{"top":20,"bottom":20,"left":30,"right":95},"fill":"lightsteelblue"}},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 6.5: Citing Documents Grouped by Applicants
</p>
</div>
<p>In this case 9 of the 22 documents involve inventors from the Broad Institute (Feng Zhang and Le Cong) while the two citing documents from the University of California involve four of the inventors on the Berkeley document cited by the Broad Institute (<a href="https://www.lens.org/lens/patent/076-607-252-634-172">WO2013176772A1</a>).</p>
<p>There are actually two separate types of activity that emerge in the cited and citing documents for <a href="https://www.lens.org/lens/patent/015-360-099-978-897">EP2784162A1</a>.</p>
<ol style="list-style-type: lower-alpha">
<li>We observe competing filings by two groups of inventors, one from Berkeley and the other from Harvard-MIT Broad Institute. A filing from Berkeley that occurred before the Broad Institute filing is cited by the Broad Institute, but <em>later</em> filings from Berkeley cite the Broad Institute filing.</li>
<li>The Broad Institute filings are self-citing one of their own earlier filings in later filings.</li>
</ol>
<p>In the first case the citation data reveals competition in this space. The second case would however require further investigation of whether the citations are inserted by the applicants (and therefore less likely to affect the scope of the newer filings) or whether they are inserted by examiners (and may therefore impact on the scope of the claims). A review of the source of the citation in the first of the Broad Institute citing documents <a href="https://www.lens.org/181-916-033-206-149">WO2016049163A2</a> reveals that it was inserted by the applicant and has no recorded impact on the claims (typically marked by XYI). Note also that even where an earlier filing does have an impact on the claims it may be possible to identify ways to modify the claims to accommodate the prior art.</p>
<!--- follow up and insert dossier link for this record--->
<p>We now have a basic grounding in back and forward patent citations using our CRISPR case. What we have learned is that:</p>
<ol style="list-style-type: decimal">
<li>Back citations may be from a wide range of technology fields in the prior art;</li>
<li>Back citations may not have a direct impact on the scope of the later filing, particularly where they are inserted by the applicant rather than examiners;</li>
<li>Older patent documents generally attract more citations than newer ones;</li>
<li>Recent documents that are attracting a significant number of citations deserve close attention;</li>
<li>Attention is required to International Patent Classification/Cooperative Patent Classification codes to determine how close cited inventions are to each other;</li>
<li>Competition can be observed between applicants where they are citing each other;</li>
<li>The sources of citations matter. Citations by examiners have a greater impact than citations inserted by applicants.</li>
</ol>
<p>Each of these observations merits further exploration than can be provided in this chapter. The background literature discussed in this chapter explores a number of these topics. We now turn to the question of how to approach counts of patent citations and the implications of counting citations by patent families. Once again we will use the CRISPR example as a real world case.</p>
</div>
<div id="counting-citations-by-patent-families" class="section level3 hasAnchor" number="6.0.6">
<h3><span class="header-section-number">6.0.6</span> Counting Citations by Patent Families<a href="citations.html#counting-citations-by-patent-families" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As we have seen above, one method or exploring the landscape of patent citations is to focus on individual documents. However, as we will now see, conducting analysis on a per document basis may miss the majority of patent citations associated with the wider patent family and the key document or documents within a family. As such <em>counts limited to individual documents may radically underestimate the impact of a claimed invention within technology space</em>.</p>
<p>We can illustrate this for our reference document EP2784162B1 and its wider INPADOC patent family. At the time of writing this document forms part of a patent family with 277 members that has 369 cited patents and 717 citing patent documents <!--- do I mean documents here--->. Table <a href="citations.html#tab:EP2784162B1">6.10</a> displays the top ranking publications based on citing patents within this family.</p>
<table>
<caption>
<span id="tab:EP2784162B1">Table 6.10: </span>Broad Institute CRISPR Patent Family Ranked by Citing Patent Count
</caption>
<thead>
<tr>
<th style="text-align:left;">
publication_number
</th>
<th style="text-align:right;">
count_of_citing_patents
</th>
<th style="text-align:left;">
title_original
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
US8697359B1
</td>
<td style="text-align:right;">
274
</td>
<td style="text-align:left;">
CRISPR-Cas systems and methods for altering expression of gene products
</td>
</tr>
<tr>
<td style="text-align:left;">
WO2014093622A2
</td>
<td style="text-align:right;">
157
</td>
<td style="text-align:left;">
DELIVERY, ENGINEERING AND OPTIMIZATION OF SYSTEMS, METHODS AND COMPOSITIONS FOR SEQUENCE MANIPULATION AND THERAPEUTIC APPLICATIONS | DÉLIVRANCE, FABRICATION ET OPTIMISATION DE SYSTÈMES, DE PROCÉDÉS ET DE COMPOSITIONS POUR LA MANIPULATION DE SÉQUENCES ET APPLICATIONS THÉRAPEUTIQUES
</td>
</tr>
<tr>
<td style="text-align:left;">
WO2014093712A1
</td>
<td style="text-align:right;">
125
</td>
<td style="text-align:left;">
ENGINEERING OF SYSTEMS, METHODS AND OPTIMIZED GUIDE COMPOSITIONS FOR SEQUENCE MANIPULATION | FABRICATION DE SYSTÈMES, PROCÉDÉS ET COMPOSITIONS DE GUIDE OPTIMISÉES POUR LA MANIPULATION DE SÉQUENCES
</td>
</tr>
<tr>
<td style="text-align:left;">
WO2014093655A2
</td>
<td style="text-align:right;">
122
</td>
<td style="text-align:left;">
ENGINEERING AND OPTIMIZATION OF SYSTEMS, METHODS AND COMPOSITIONS FOR SEQUENCE MANIPULATION WITH FUNCTIONAL DOMAINS | FABRICATION ET OPTIMISATION DE SYSTÈMES, DE PROCÉDÉS ET DE COMPOSITIONS POUR LA MANIPULATION DE SÉQUENCE AVEC DES DOMAINES FONCTIONNELS
</td>
</tr>
<tr>
<td style="text-align:left;">
US20140179770A1
</td>
<td style="text-align:right;">
121
</td>
<td style="text-align:left;">
Delivery, Engineering and Optimization of Systems, Methods and Compositions for Sequence Manipulation and Therapeutic Applications
</td>
</tr>
</tbody>
</table>
<p>This data makes clear that our original reference document EP2784162B1, with 23 citing documents at the time of writing, was not reflecting the wider citation landscape for this patent family. Furthermore, as in the litigation surrounding this case discussed by <span class="citation">Egelie et al. (<a href="#ref-Egelie_2016" role="doc-biblioref">2016</a>)</span> and <span class="citation">Ledford (<a href="#ref-Ledford_2018" role="doc-biblioref">2018</a>)</span>, the most important individual document in this family is in fact US granted patent <a href="https://www.lens.org/lens/patent/014-343-086-528-757">US8697359B1</a>. Specifically, while, as we have seen, the Broad Institute patent family cites the Berkeley patent application, in reality the Broad Institute patent attorneys used the expedited examination procedure at the USPTO with the effect that the Broad Institute patent filing was granted <em>before</em> the Berkeley filing was examined.<a href="#fn25" class="footnote-ref" id="fnref25"><sup>25</sup></a></p>
<p>The seminal paper on word vectors by Mikolov et al 2013 neatly summarises the problem they seek to address as follows:</p>
<blockquote>
<p>“Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. <span class="citation">(<a href="#ref-lens.org/104-512-929-235-758" role="doc-biblioref">Mikolov et al. 2013</a>)</span></p>
</blockquote>
<p>The problem that Mikolov and co-authors identify is that the development of approaches such as automatic speech recognition is constrained by dependency on high quality manual transcripts of speech containing only millions of words while machine translation models are constrained by the fact that “…corpora for many languages contain only a few billions of words or less” <span class="citation">(<a href="#ref-lens.org/104-512-929-235-758" role="doc-biblioref">Mikolov et al. 2013</a>)</span>. Put another way, the constraint presented by approaches at the time was that the examples available for computational modelling could not accommodate the range of human uses of language or more precisely, the meanings conveyed. Mikolov et. al. successfully demonstrated that distributed representations of words using neural network based language models outperformed the existing Ngram (words and phrases) models on much larger datasets (using 1 million common tokens from the Google News corpus) <span class="citation">(<a href="#ref-lens.org/104-512-929-235-758" role="doc-biblioref">Mikolov et al. 2013</a>)</span>.</p>
<blockquote>
<p>“We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14].
Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].”</p>
</blockquote>
<p>This observation has become one of the most famous in computational linguistics and is worth elaborating on. In a word vector it was found that.</p>
<blockquote>
<p>King - Man + Woman = Queen</p>
</blockquote>
<p>In a 2016 blog post on “The amazing power of word vectors” Adrian Coyler provides the following illustration of how this works. Note that the labels do not exist in the vectors and are added purely for explanation in this hypothetical example.<a href="#fn26" class="footnote-ref" id="fnref26"><sup>26</sup></a>(<a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/" class="uri">https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/</a>)</p>
<div class="figure" style="text-align: center">
<img src="images/word2vec_coyler.png" alt="Word Vector Example (Coyler 2016)" width="328" />
<p class="caption">
(#fig:coyler_vectors)Word Vector Example (Coyler 2016)
</p>
</div>
<p>Let us imagine that each word in each individual vector has a distributed value across hundreds of dimensions across the corpus. Words like King, Queen, Princess have a high similarity in vector space with the word Royalty. In contrast, King has a strong similarity with Masculinity while Queen has a strong similarity with Femininity. Deducting Man from King and adding Woman can readily be seen to lead to Queen in the vector space. Other well known examples from the same paper lead to the calculation that “big-bigger” = “small:larger” etc.</p>
<!---In follow up work Mikolov and others at Google released the word2vec tool for the creation of --->
</div>
<div id="word-vectors-with-fastext" class="section level3 hasAnchor" number="6.0.7">
<h3><span class="header-section-number">6.0.7</span> Word Vectors with fastext<a href="citations.html#word-vectors-with-fastext" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To illustrate the use of word vectors we will use the fastText machine learning package developed by Facebook.</p>
<p>FastText is a free text classification and representation package produced by Facebook that provides downloadable multi-language models for use in machine learning. At present vector models are available for 157 languages.</p>
<p>FastText can be used from the command line or in Python (fasttext) or in R with the <a href="https://github.com/pommedeterresautee/fastrtext">fastrtext package</a>. FastText is a good way to get started with word vectors and machine learning because it is very easy to install and use. Fasttext is under active development with the latest updates posted on the <a href="https://fasttext.cc/">fasttext website</a>.</p>
<p>FastText was developed by researchers including Tomas Mikolov as an alternative to the increasingly popular deep learning models for text classification at scale. It is a lightweight tool that emphasizes speed in classification <span class="citation">(<a href="#ref-bojanowski2016enriching" role="doc-biblioref">Bojanowski et al. 2016</a>; <a href="#ref-joulin2016bag" role="doc-biblioref">Joulin, Grave, Bojanowski, and Mikolov 2016</a>; <a href="#ref-joulin2016fasttext" role="doc-biblioref">Joulin, Grave, Bojanowski, Douze, et al. 2016</a>)</span> and arguably outperforms deep learning models.</p>
<p>To get started follow <a href="https://fasttext.cc/docs/en/support.html">the fastext instructions</a> to install fasttext from the command line or in Python. We will demonstrate fasttext in Python but it is easy, if not easier, to run from the command line.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb92-1"><a href="citations.html#cb92-1" aria-hidden="true" tabindex="-1"></a>git clone https:<span class="op">//</span>github.com<span class="op">/</span>facebookresearch<span class="op">/</span>fastText.git</span>
<span id="cb92-2"><a href="citations.html#cb92-2" aria-hidden="true" tabindex="-1"></a>cd fastText</span>
<span id="cb92-3"><a href="citations.html#cb92-3" aria-hidden="true" tabindex="-1"></a>sudo pip install .</span>
<span id="cb92-4"><a href="citations.html#cb92-4" aria-hidden="true" tabindex="-1"></a><span class="co"># or</span></span>
<span id="cb92-5"><a href="citations.html#cb92-5" aria-hidden="true" tabindex="-1"></a>sudo python setup.py install</span></code></pre></div>
<p>Verify the installation</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb93-1"><a href="citations.html#cb93-1" aria-hidden="true" tabindex="-1"></a>python</span>
<span id="cb93-2"><a href="citations.html#cb93-2" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span> <span class="im">import</span> fasttext</span>
<span id="cb93-3"><a href="citations.html#cb93-3" aria-hidden="true" tabindex="-1"></a><span class="op">&gt;&gt;&gt;</span></span></code></pre></div>
<p>If this has worked correctly you should not see anything after <code>import fasttext</code>.</p>
<p>In the Terminal we now need some data to train. The fastext example uses Wikipedia pages in English that take up 15Gb. In the terminal download the smaller version as follows.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="citations.html#cb94-1" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span> mkdir data</span>
<span id="cb94-2"><a href="citations.html#cb94-2" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span> wget <span class="sc">-</span>c http<span class="sc">:</span><span class="er">//</span>mattmahoney.net<span class="sc">/</span>dc<span class="sc">/</span>enwik9.zip <span class="sc">-</span>P data</span>
<span id="cb94-3"><a href="citations.html#cb94-3" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span> unzip data<span class="sc">/</span>enwik9.zip <span class="sc">-</span>d data</span></code></pre></div>
<p>As this is an XML file it needs to be parsed. The file for parsing is bundled with fastext as wikifil.pl and you will need to get the path right for your installation. If in doubt download fasttext from the command line, make and then cd into the directory for this step. Record the path to the file that you will need in the next step in Python.</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="citations.html#cb95-1" aria-hidden="true" tabindex="-1"></a>perl wikifil.pl data<span class="sc">/</span>enwik9 <span class="sc">&gt;</span> data<span class="sc">/</span>fil9</span></code></pre></div>
<p>Check that the file has parsed on the command line.</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="citations.html#cb96-1" aria-hidden="true" tabindex="-1"></a>head <span class="sc">-</span>c <span class="dv">80</span> data<span class="sc">/</span>fil9</span></code></pre></div>
<p>Train word vectors</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb97-1"><a href="citations.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> fasttext</span>
<span id="cb97-2"><a href="citations.html#cb97-2" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> fasttext.train_unsupervised(<span class="st">&#39;/Users/colinbarnes/fastText/data/fil9&#39;</span>)</span></code></pre></div>
</div>
<div id="training-word-vectors-for-drones" class="section level3 hasAnchor" number="6.0.8">
<h3><span class="header-section-number">6.0.8</span> Training Word Vectors for Drones<a href="citations.html#training-word-vectors-for-drones" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will use a small set of patent texts on drones from the drones package for illustration. Ideally use the largest possible set. However, for better results use texts in a single language and regularise the texts so that everything is lower case. You may also improve results by removing all punctuation.</p>
<p>If we wished to do that in R by way of example we access the title, abstract and claims table (tac) in the drones training package. We would then combine the the fields, convert to lowercase and then replace all the punctuation with a space. We might tidy up by removing any double spaces created by removing the punctuation</p>
<!--- to do: provide a pre processed file in the drones package-->
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="citations.html#cb98-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb98-2"><a href="citations.html#cb98-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(drones)</span>
<span id="cb98-3"><a href="citations.html#cb98-3" aria-hidden="true" tabindex="-1"></a> drones_texts_vec <span class="ot">&lt;-</span> drones<span class="sc">::</span>tac <span class="sc">%&gt;%</span></span>
<span id="cb98-4"><a href="citations.html#cb98-4" aria-hidden="true" tabindex="-1"></a>   <span class="fu">unite</span>(text, <span class="fu">c</span>(<span class="st">&quot;title_english&quot;</span>, <span class="st">&quot;abstract_english&quot;</span>, <span class="st">&quot;first_claim&quot;</span>), <span class="at">sep =</span> <span class="st">&quot; &quot;</span>) <span class="sc">%&gt;%</span> </span>
<span id="cb98-5"><a href="citations.html#cb98-5" aria-hidden="true" tabindex="-1"></a>   <span class="fu">mutate</span>(<span class="at">text =</span> <span class="fu">str_to_lower</span>(text)) <span class="sc">%&gt;%</span></span>
<span id="cb98-6"><a href="citations.html#cb98-6" aria-hidden="true" tabindex="-1"></a>   <span class="fu">mutate</span>(<span class="at">text =</span> <span class="fu">str_replace_all</span>(text, <span class="st">&quot;[[:punct:]]&quot;</span>, <span class="st">&quot; &quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb98-7"><a href="citations.html#cb98-7" aria-hidden="true" tabindex="-1"></a>   <span class="fu">mutate</span>(<span class="at">text =</span> <span class="fu">str_replace_all</span>(text, <span class="st">&quot;  &quot;</span>, <span class="st">&quot; &quot;</span>)) <span class="sc">%&gt;%</span> </span>
<span id="cb98-8"><a href="citations.html#cb98-8" aria-hidden="true" tabindex="-1"></a>   <span class="fu">select</span>(text)</span>
<span id="cb98-9"><a href="citations.html#cb98-9" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb98-10"><a href="citations.html#cb98-10" aria-hidden="true" tabindex="-1"></a> <span class="fu">head</span>(drones_texts_vec)</span></code></pre></div>
<p>This cleaned up anonymised text is available in the data folder of this handbook and in the drones package <!--- TO DO--->. Note that patent texts can be messy and you may want to engage in further processing. Once we have the data in a cleaned cleaned up format we can pass it to fast text in the terminal.</p>
<p>There are two available models for word vectors in fast text. These are:</p>
<ol style="list-style-type: lower-alpha">
<li>skipgrams (identify a word from closely related words) <!--- check this definition---></li>
<li>cbow (predict a word from the context words around it)</li>
</ol>
<p>Next in the terminal we navigate to the fasttext folder and provide our csv or simple text file as an input and specify the output.</p>
<!--- THIS IS NOW THROWING AN ERROR AND NEEDS TO BE UPDATED--->
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="citations.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span> .<span class="sc">/</span>fasttext skipgram <span class="sc">-</span>input <span class="sc">/</span>handbook<span class="sc">/</span>data<span class="sc">/</span>fasttext<span class="sc">/</span>drones_vector_texts.csv <span class="sc">-</span>output <span class="sc">/</span>handbook<span class="sc">/</span>data<span class="sc">/</span>fasttext<span class="sc">/</span>drones_vec</span></code></pre></div>
<p>There are total of 4.4 million words (tokens) in the vocabulary with xxx <!--- fill in---> distinct words. It takes about 30 seconds for fasttext to process these words. These words boil down to 19,125 words in total.</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="citations.html#cb100-1" aria-hidden="true" tabindex="-1"></a>Read 4M words</span>
<span id="cb100-2"><a href="citations.html#cb100-2" aria-hidden="true" tabindex="-1"></a>Number of words<span class="sc">:</span>  <span class="dv">19125</span></span>
<span id="cb100-3"><a href="citations.html#cb100-3" aria-hidden="true" tabindex="-1"></a>Number of labels<span class="sc">:</span> <span class="dv">0</span></span>
<span id="cb100-4"><a href="citations.html#cb100-4" aria-hidden="true" tabindex="-1"></a>Progress<span class="sc">:</span> <span class="fl">100.0</span>% words<span class="sc">/</span>sec<span class="sc">/</span>thread<span class="sc">:</span>   <span class="dv">50624</span> lr<span class="sc">:</span>  <span class="fl">0.000000</span> avg.loss<span class="sc">:</span>  <span class="fl">1.790453</span> ETA<span class="sc">:</span>   0h 0m 0s</span></code></pre></div>
<p>The processing creates two files in our target directory. The first is <code>drones_vec.bin</code> containing the model and the second is <code>drones_vec.vec</code>. The second file is actually a simple text file that contains the weights for each individual terms in the vector. Here is a glimpse of that file for the word <code>device</code>.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="citations.html#cb101-1" aria-hidden="true" tabindex="-1"></a>device <span class="fl">0.21325</span> <span class="fl">0.11661</span> <span class="sc">-</span><span class="fl">0.060266</span> <span class="sc">-</span><span class="fl">0.17116</span> <span class="fl">0.16712</span> <span class="sc">-</span><span class="fl">0.03204</span> <span class="sc">-</span><span class="fl">0.54853</span> <span class="sc">-</span><span class="fl">0.30647</span> <span class="fl">0.023724</span> <span class="sc">-</span><span class="fl">0.047807</span> <span class="sc">-</span><span class="fl">0.068384</span> <span class="sc">-</span><span class="fl">0.22845</span> <span class="sc">-</span><span class="fl">0.08171</span> <span class="fl">0.046688</span> <span class="fl">0.26321</span> <span class="sc">-</span><span class="fl">0.51804</span> <span class="sc">-</span><span class="fl">0.02021</span> <span class="fl">0.099132</span> <span class="sc">-</span><span class="fl">0.27856</span> <span class="fl">0.33479</span> <span class="sc">-</span><span class="fl">0.027596</span> <span class="sc">-</span><span class="fl">0.27679</span> <span class="fl">0.31599</span> <span class="sc">-</span><span class="fl">0.32319</span> <span class="fl">0.048407</span> <span class="sc">-</span><span class="fl">0.067782</span> <span class="sc">-</span><span class="fl">0.086028</span> <span class="fl">0.070966</span> <span class="sc">-</span><span class="fl">0.27628</span> <span class="sc">-</span><span class="fl">0.43886</span> <span class="sc">-</span><span class="fl">0.23275</span> <span class="fl">0.15364</span> <span class="sc">-</span><span class="fl">0.037609</span> <span class="fl">0.16732</span> <span class="sc">-</span><span class="fl">0.55758</span> <span class="fl">0.24021</span> <span class="sc">-</span><span class="fl">0.21904</span> <span class="sc">-</span><span class="fl">0.00074375</span> <span class="sc">-</span><span class="fl">0.2962</span> <span class="fl">0.41962</span> <span class="fl">0.069979</span> <span class="fl">0.039564</span> <span class="fl">0.31745</span> <span class="sc">-</span><span class="fl">0.11433</span> <span class="fl">0.15294</span> <span class="sc">-</span><span class="fl">0.4063</span> <span class="fl">0.16489</span> <span class="sc">-</span><span class="fl">0.17881</span> <span class="sc">-</span><span class="fl">0.24346</span> <span class="sc">-</span><span class="fl">0.17451</span> <span class="fl">0.19218</span> <span class="sc">-</span><span class="fl">0.13081</span> <span class="sc">-</span><span class="fl">0.052599</span> <span class="fl">0.12156</span> <span class="sc">-</span><span class="fl">0.023431</span> <span class="sc">-</span><span class="fl">0.066951</span> <span class="fl">0.19624</span> <span class="fl">0.11179</span> <span class="fl">0.17482</span> <span class="fl">0.34394</span> <span class="fl">0.17303</span> <span class="sc">-</span><span class="fl">0.32398</span> <span class="fl">0.54666</span> <span class="sc">-</span><span class="fl">0.30731</span> <span class="sc">-</span><span class="fl">0.1117</span> <span class="sc">-</span><span class="fl">0.017867</span> <span class="fl">0.081936</span> <span class="sc">-</span><span class="fl">0.068579</span> <span class="sc">-</span><span class="fl">0.15465</span> <span class="fl">0.057545</span> <span class="fl">0.026571</span> <span class="sc">-</span><span class="fl">0.3714</span> <span class="fl">0.029978</span> <span class="fl">0.081706</span> <span class="fl">0.017101</span> <span class="fl">0.036847</span> <span class="sc">-</span><span class="fl">0.13174</span> <span class="fl">0.24701</span> <span class="sc">-</span><span class="fl">0.10006</span> <span class="sc">-</span><span class="fl">0.11838</span> <span class="sc">-</span><span class="fl">0.045929</span> <span class="sc">-</span><span class="fl">0.13226</span> <span class="fl">0.20067</span> <span class="fl">0.12056</span> <span class="fl">0.43343</span> <span class="fl">0.052317</span> <span class="sc">-</span><span class="fl">0.030258</span> <span class="fl">0.066875</span> <span class="sc">-</span><span class="fl">0.1951</span> <span class="fl">0.12343</span> <span class="fl">0.031962</span> <span class="sc">-</span><span class="fl">0.52444</span> <span class="fl">0.041814</span> <span class="sc">-</span><span class="fl">0.64228</span> <span class="fl">0.13036</span> <span class="fl">0.040553</span> <span class="fl">0.30254</span> <span class="sc">-</span><span class="fl">0.15474</span> <span class="sc">-</span><span class="fl">0.57587</span> <span class="fl">0.29205</span> </span></code></pre></div>
<p>Note two points here. The first is that the default vector space is 100 dimensions but popular choices go up to 300. Note also that there will be common stop words (and, the, etc.) in the model that we may want to remove. The second main point is that the file size of the .bin file is nearly 800Mb and may get much larger fast.</p>
<p>From the terminal we can print the word vectors for specific words as follows:</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="citations.html#cb102-1" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span> echo <span class="st">&quot;drone autonomous weapon&quot;</span> <span class="sc">|</span> .<span class="sc">/</span>fasttext print<span class="sc">-</span>word<span class="sc">-</span>vectors <span class="sc">/</span>Users<span class="sc">/</span>colinbarnes<span class="sc">/</span>handbook<span class="sc">/</span>data<span class="sc">/</span>fasttext<span class="sc">/</span>drones_vec.bin</span>
<span id="cb102-2"><a href="citations.html#cb102-2" aria-hidden="true" tabindex="-1"></a>drone <span class="fl">0.38326</span> <span class="fl">0.4115</span> <span class="fl">0.28873</span> <span class="sc">-</span><span class="fl">0.35648</span> <span class="sc">-</span><span class="fl">0.24769</span> <span class="sc">-</span><span class="fl">0.22507</span> <span class="fl">0.18887</span> <span class="fl">0.012016</span> <span class="dv">0</span>.<span class="dv">51823</span>...</span>
<span id="cb102-3"><a href="citations.html#cb102-3" aria-hidden="true" tabindex="-1"></a>autonomous <span class="fl">0.41683</span> <span class="fl">0.39242</span> <span class="fl">0.16987</span> <span class="sc">-</span><span class="fl">0.028905</span> <span class="fl">0.38609</span> <span class="sc">-</span><span class="fl">0.57572</span> <span class="sc">-</span><span class="fl">0.44157</span> <span class="sc">-</span><span class="dv">0</span>.<span class="dv">51236</span>...</span>
<span id="cb102-4"><a href="citations.html#cb102-4" aria-hidden="true" tabindex="-1"></a>weapon <span class="fl">0.20932</span> <span class="fl">0.59608</span> <span class="fl">0.21891</span> <span class="sc">-</span><span class="fl">0.42716</span> <span class="fl">0.19016</span> <span class="sc">-</span><span class="fl">0.76555</span> <span class="fl">0.23395</span> <span class="sc">-</span><span class="fl">0.63699</span> <span class="sc">-</span><span class="dv">0</span>.<span class="dv">12079</span>...</span></code></pre></div>
</div>
<div id="using-word-vectors" class="section level3 hasAnchor" number="6.0.9">
<h3><span class="header-section-number">6.0.9</span> Using Word Vectors<a href="citations.html#using-word-vectors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One common use of word vectors is to build a thesaurus. We can also check how our vectors are performing, and adjust the parameters if we are not getting what we expect. We do this by calculating the nearest neighbours (nn) in the vector space and then entering a query term, The higher the score the closer the neighbour is.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="citations.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span> .<span class="sc">/</span>fasttext nn <span class="sc">/</span>Users<span class="sc">/</span>colinbarnes<span class="sc">/</span>handbook<span class="sc">/</span>data<span class="sc">/</span>fasttext<span class="sc">/</span>drones_vec.bin</span>
<span id="cb103-2"><a href="citations.html#cb103-2" aria-hidden="true" tabindex="-1"></a>Query word? drone</span>
<span id="cb103-3"><a href="citations.html#cb103-3" aria-hidden="true" tabindex="-1"></a>codrone <span class="fl">0.69161</span></span>
<span id="cb103-4"><a href="citations.html#cb103-4" aria-hidden="true" tabindex="-1"></a>drones <span class="fl">0.66626</span></span>
<span id="cb103-5"><a href="citations.html#cb103-5" aria-hidden="true" tabindex="-1"></a>dron <span class="fl">0.627708</span></span>
<span id="cb103-6"><a href="citations.html#cb103-6" aria-hidden="true" tabindex="-1"></a>microdrone <span class="fl">0.603919</span></span>
<span id="cb103-7"><a href="citations.html#cb103-7" aria-hidden="true" tabindex="-1"></a>quadrone <span class="fl">0.603164</span></span>
<span id="cb103-8"><a href="citations.html#cb103-8" aria-hidden="true" tabindex="-1"></a>stabilisation <span class="fl">0.594831</span></span>
<span id="cb103-9"><a href="citations.html#cb103-9" aria-hidden="true" tabindex="-1"></a>stabilised <span class="fl">0.579854</span></span>
<span id="cb103-10"><a href="citations.html#cb103-10" aria-hidden="true" tabindex="-1"></a>naval <span class="fl">0.572352</span></span>
<span id="cb103-11"><a href="citations.html#cb103-11" aria-hidden="true" tabindex="-1"></a>piloted <span class="fl">0.571288</span></span>
<span id="cb103-12"><a href="citations.html#cb103-12" aria-hidden="true" tabindex="-1"></a>stabilise <span class="fl">0.564452</span></span></code></pre></div>
<p>Hmmm, OK but maybe we should try UAV</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="citations.html#cb104-1" aria-hidden="true" tabindex="-1"></a>Query word? uav</span>
<span id="cb104-2"><a href="citations.html#cb104-2" aria-hidden="true" tabindex="-1"></a>uavgs <span class="fl">0.768899</span></span>
<span id="cb104-3"><a href="citations.html#cb104-3" aria-hidden="true" tabindex="-1"></a>aerial <span class="fl">0.742971</span></span>
<span id="cb104-4"><a href="citations.html#cb104-4" aria-hidden="true" tabindex="-1"></a>uavs <span class="fl">0.710861</span></span>
<span id="cb104-5"><a href="citations.html#cb104-5" aria-hidden="true" tabindex="-1"></a>unmanned <span class="fl">0.692975</span></span>
<span id="cb104-6"><a href="citations.html#cb104-6" aria-hidden="true" tabindex="-1"></a>uad <span class="fl">0.684599</span></span>
<span id="cb104-7"><a href="citations.html#cb104-7" aria-hidden="true" tabindex="-1"></a>ua <span class="fl">0.667772</span></span>
<span id="cb104-8"><a href="citations.html#cb104-8" aria-hidden="true" tabindex="-1"></a>copter <span class="fl">0.666946</span></span>
<span id="cb104-9"><a href="citations.html#cb104-9" aria-hidden="true" tabindex="-1"></a>usv <span class="fl">0.652238</span></span>
<span id="cb104-10"><a href="citations.html#cb104-10" aria-hidden="true" tabindex="-1"></a>uas <span class="fl">0.644046</span></span>
<span id="cb104-11"><a href="citations.html#cb104-11" aria-hidden="true" tabindex="-1"></a>flight <span class="fl">0.643298</span></span></code></pre></div>
<p>This is printing some words that we would expect in both cases such as plurals (drones, uavs) along with types of drones but we need to investigate some high scoring terms, for example in set one we have the word <code>codrone</code> which is a specific make of drone. The word <code>dron</code> may be the word for drone in another language. In the second set we have <code>uavgs</code> which may stand for UAV Ground School along with terms such as <code>uad</code> which stands for unmanned aerial device.</p>
<p>So, this reveals that we are obtaining some meaningful results on single terms that can guide our construction of a search query. We could also look at this another way by identifying terms that may be sources of noise. Here we will use the word bee.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="citations.html#cb105-1" aria-hidden="true" tabindex="-1"></a>Query word? bee</span>
<span id="cb105-2"><a href="citations.html#cb105-2" aria-hidden="true" tabindex="-1"></a>honey <span class="fl">0.861416</span></span>
<span id="cb105-3"><a href="citations.html#cb105-3" aria-hidden="true" tabindex="-1"></a>hive <span class="fl">0.830507</span></span>
<span id="cb105-4"><a href="citations.html#cb105-4" aria-hidden="true" tabindex="-1"></a>bees <span class="fl">0.826173</span></span>
<span id="cb105-5"><a href="citations.html#cb105-5" aria-hidden="true" tabindex="-1"></a>hives <span class="fl">0.81909</span></span>
<span id="cb105-6"><a href="citations.html#cb105-6" aria-hidden="true" tabindex="-1"></a>honeybee <span class="fl">0.81183</span></span>
<span id="cb105-7"><a href="citations.html#cb105-7" aria-hidden="true" tabindex="-1"></a>queen <span class="fl">0.806328</span></span>
<span id="cb105-8"><a href="citations.html#cb105-8" aria-hidden="true" tabindex="-1"></a>honeycombs <span class="fl">0.803329</span></span>
<span id="cb105-9"><a href="citations.html#cb105-9" aria-hidden="true" tabindex="-1"></a>honeybees <span class="fl">0.784235</span></span>
<span id="cb105-10"><a href="citations.html#cb105-10" aria-hidden="true" tabindex="-1"></a>honeycomb <span class="fl">0.783923</span></span>
<span id="cb105-11"><a href="citations.html#cb105-11" aria-hidden="true" tabindex="-1"></a>beehives <span class="fl">0.783663</span></span></code></pre></div>
<p>This is yielding decent results. We could for example use this to build a term exclusion list and we might try something similar with the word music (to exclude words like musician, musical, melodic, melody) where it is clear they cannot be linked to drones. For example, there may be drones that play music… but the words musician, melodic and melody are unlikely to be associated with musical drones.</p>
<!---Another way of generating word vectors is using the Continuous Bag of Words (CBOW) methods. 

A Continuous Bag of Words approach attempts to predict the target word from the context words that surround it . <!--- Make sure clear about the distinction with this--->
<!---The word drone is another name for an unmanned aerial vehicle or UAV. --->
</div>
<div id="exploring-analogies" class="section level3 hasAnchor" number="6.0.10">
<h3><span class="header-section-number">6.0.10</span> Exploring Analogies<a href="citations.html#exploring-analogies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Word vectors are famous for being able to predict relationships of the type</p>
<p>”King” - ”Man” + ”Woman” = “Queen”</p>
<p>We can experiment with this with the drones vector we created earlier using the <code>analogies</code> function in fasttext. In the <em>terminal</em> run:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="citations.html#cb106-1" aria-hidden="true" tabindex="-1"></a><span class="sc">$</span> .<span class="sc">/</span>fasttext analogies <span class="sc">/</span>Users<span class="sc">/</span>colinbarnes<span class="sc">/</span>handbook<span class="sc">/</span>data<span class="sc">/</span>fasttext<span class="sc">/</span>drones_vec.bin</span></code></pre></div>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="citations.html#cb107-1" aria-hidden="true" tabindex="-1"></a>Query <span class="fu">triplet</span> (A <span class="sc">-</span> B <span class="sc">+</span> C)? drone autonomous bee</span>
<span id="cb107-2"><a href="citations.html#cb107-2" aria-hidden="true" tabindex="-1"></a>honey <span class="fl">0.687067</span></span>
<span id="cb107-3"><a href="citations.html#cb107-3" aria-hidden="true" tabindex="-1"></a>larvae <span class="fl">0.668081</span></span>
<span id="cb107-4"><a href="citations.html#cb107-4" aria-hidden="true" tabindex="-1"></a>larva <span class="fl">0.668004</span></span>
<span id="cb107-5"><a href="citations.html#cb107-5" aria-hidden="true" tabindex="-1"></a>brood <span class="fl">0.664371</span></span>
<span id="cb107-6"><a href="citations.html#cb107-6" aria-hidden="true" tabindex="-1"></a>honeycombs <span class="fl">0.655226</span></span>
<span id="cb107-7"><a href="citations.html#cb107-7" aria-hidden="true" tabindex="-1"></a>hive <span class="fl">0.653633</span></span>
<span id="cb107-8"><a href="citations.html#cb107-8" aria-hidden="true" tabindex="-1"></a>honeycomb <span class="fl">0.651272</span></span>
<span id="cb107-9"><a href="citations.html#cb107-9" aria-hidden="true" tabindex="-1"></a>bees <span class="fl">0.632742</span></span>
<span id="cb107-10"><a href="citations.html#cb107-10" aria-hidden="true" tabindex="-1"></a>comb <span class="fl">0.626189</span></span>
<span id="cb107-11"><a href="citations.html#cb107-11" aria-hidden="true" tabindex="-1"></a>colonies <span class="fl">0.61386</span></span></code></pre></div>
<p>what this tells us is that drone - autonomous + bee = honey or larvae, or larva or brood. We can more or less reverse this calculation.</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="citations.html#cb108-1" aria-hidden="true" tabindex="-1"></a>Query <span class="fu">triplet</span> (A <span class="sc">-</span> B <span class="sc">+</span> C)? bee honey drone</span>
<span id="cb108-2"><a href="citations.html#cb108-2" aria-hidden="true" tabindex="-1"></a>drones <span class="fl">0.651486</span></span>
<span id="cb108-3"><a href="citations.html#cb108-3" aria-hidden="true" tabindex="-1"></a>codrone <span class="fl">0.63545</span></span>
<span id="cb108-4"><a href="citations.html#cb108-4" aria-hidden="true" tabindex="-1"></a>stabilisation <span class="fl">0.592739</span></span>
<span id="cb108-5"><a href="citations.html#cb108-5" aria-hidden="true" tabindex="-1"></a>dron <span class="fl">0.582929</span></span>
<span id="cb108-6"><a href="citations.html#cb108-6" aria-hidden="true" tabindex="-1"></a>na <span class="fl">0.572516</span> <span class="co"># drop NA from the underlying set</span></span>
<span id="cb108-7"><a href="citations.html#cb108-7" aria-hidden="true" tabindex="-1"></a>microdrone <span class="fl">0.571889</span></span>
<span id="cb108-8"><a href="citations.html#cb108-8" aria-hidden="true" tabindex="-1"></a>naval <span class="fl">0.541626</span></span>
<span id="cb108-9"><a href="citations.html#cb108-9" aria-hidden="true" tabindex="-1"></a>continuation <span class="fl">0.54127</span></span>
<span id="cb108-10"><a href="citations.html#cb108-10" aria-hidden="true" tabindex="-1"></a>proposition <span class="fl">0.540825</span></span>
<span id="cb108-11"><a href="citations.html#cb108-11" aria-hidden="true" tabindex="-1"></a>déstabilisation <span class="fl">0.536781</span></span></code></pre></div>
<p>What this example illustrates is that, as we might expect, terms for bees and terms for drones as a technology occupy different parts of the vector space.</p>
<p>It is fundamentally quite difficult to conceptualise a 100 or 300 dimension vector space. However, Google has developed a <a href="http://projector.tensorflow.org/">tensorflow projector</a> and a video that discusses high dimensional space in an accessible way <a href="https://www.youtube.com/watch?v=wvsE8jm1GzE">A.I. Experiments: Visualizing High-Dimensional Space</a>. The <a href="https://distill.pub/">Distill website</a> offers good examples of the visualisation of a range of machine learning components.</p>
<p>We can view a simplified visualisation of the term drones in 200 dimension vector space (on a much larger model than we have discussed above) in Figure <a href="citations.html#fig:dronesviz">6.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dronesviz"></span>
<img src="images/drones1_embedding.png" alt="The term drone in 200 dimension vector space" width="948" />
<p class="caption">
Figure 6.6: The term drone in 200 dimension vector space
</p>
</div>
<p>Here we have selected to display 400 terms linked to the source word drones across the representation of the vector space. As with network analysis we can see that clusters of association emerge. As we zoom in to the vector space representation we start to more clearly see nearest points in this case based on Principle Components Analysis (PCA) in Figure <a href="citations.html#fig:dronespca">6.7</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dronespca"></span>
<img src="images/dronesembedding_isolated.png" alt="Zooming in to the Drones cluster" width="606" />
<p class="caption">
Figure 6.7: Zooming in to the Drones cluster
</p>
</div>
<p>The representation of terms in vector space in these images is different to those we viewed above and more clearly favours bees and music, although closer inspection reveals words such as ‘winged’, ‘terrorized’. ‘missile’ and ‘predator’ that suggest the presence of news related terms when compared with the patent data used above.</p>
<p>This visualisation highlights the power of the representation of words as vectors in vector space. It also highlights that the vector space is determined by the source data. For example, many vector models are built from downloads of the <a href="https://en.wikipedia.org/wiki/Wikipedia:Database_download#English-language_Wikipedia">content of Wikipedia</a> (available in a number of languages) or on a much larger scale from internet web pages through services such as <a href="https://commoncrawl.org/">Common Crawl</a>.</p>
<p>For patent analytics, this can present the problem that the language in vector models lacks the specificity in terms of the use of technical language found in patent documents. For that reason you may be better, as illustrated above, wherever possible it is better to use patent domain specific word embeddings.</p>
</div>
<div id="patent-specific-word-embeddings" class="section level3 hasAnchor" number="6.0.11">
<h3><span class="header-section-number">6.0.11</span> Patent Specific Word Embeddings<a href="citations.html#patent-specific-word-embeddings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The increasing accessibility of patent data, with both the US and the EP full text collections now available free of charge, has witnessed growing efforts to develop word embedding approaches to patent classification and search.</p>
<p>A very good example of this type of approach is provided by Julian Risch and Ralf Krestel at the Hasso Plattner Institute at the University of Potsdam with a focus on patent classification <span class="citation">(<a href="#ref-lens.org/078-034-591-343-369" role="doc-biblioref">Risch and Krestel 2019</a>)</span>.</p>
<p>Risch and Krestel test word embedding approaches using three different datasets:</p>
<ol style="list-style-type: lower-alpha">
<li>the WIPO-alpha dataset of 75,000 excerpts of English language PCT applications accompanied by subclass information <span class="citation">(<a href="#ref-lens.org/056-971-725-855-254" role="doc-biblioref"><em>Automated Categorization in the International Patent Classification</em> 2003</a>)</span>;</li>
<li>A dataset of 5.4 million patent documents from the USPTO between 1976-2016 called USPTO-5M containing the full text and bibliographic data.^[<a href="https://www.uspto.gov/learning-and-resources/bulk-data-products">USPTO Bulk Products</a>, now more readily available from <a href="http://www.patentsview.org/download/">PatentsView</a></li>
<li>A public dataset of with 2 million JSON formatted USPTO patent documents called USPTO-2M created by Jason Hoou containing titles, abstracts and IPC subclasses. <a href="#fn27" class="footnote-ref" id="fnref27"><sup>27</sup></a>. [NOTE THIS IS ACCESS PROTECTED! EXCLUDE AS NOT ACTIVE]</li>
</ol>
<p>These datasets are primarily intended to assist with the automatic classification of patent documents. They used fastText on 5 million patent documents to train word embeddings with 100, 200 and 300 dimensions based on lower-case words occurring 10 or more times and with a context window of 5. This involved a total of 28 billion tokens and, as they rightly point out, this is larger than the English Wikipedia corpus (16 billion) but lower than the 600 billion tokens in the Common Crawl dataset <span class="citation">(<a href="#ref-lens.org/078-034-591-343-369" role="doc-biblioref">Risch and Krestel 2019</a>)</span>. As part of an open access approach focusing on reproducibility the resulting datasets are made available free of charge<a href="#fn28" class="footnote-ref" id="fnref28"><sup>28</sup></a></p>
<p>The word embedding were then used to support the development of a deep neural network using gated recurrent units (GRU) with the aim of predicting the main subclass for the document using 300 words from the title and abstract of the documents. The same network architecture was used to test the WIPO set, the standard Wikipedia embeddings. the USPTO-2M set and the USPTO-5M (restricted to titles and abstracts). The specific details of the experiments performed with the word embeddings and GRU deep neural network are available in the article. The main finding of the research is to demonstrate that patent specific word embeddings outperform the Wikipedia based embeddings. This confirms the very crude intuition that we gained from the very small samples of data on the term drones compared with the exploration of Wikipedia based embeddings.</p>
<p>One important challenge with the use of word vectors or embeddings is size. FastText has the considerable advantage that it is designed to run on CPU. That is, as we have seen it can be run on a laptop. However, the word embeddings provided by Risch and Krestel, notably the 300 dimension dataset, may present significant memory challenges to be used in practice. The word embeddings generated by the work of Risch and Krestel demonstrate some of these issues. Thus, the 100 dimension word embeddings vectors are 6 gigabytes in size, the 200 dimensions file is 12Gb and the 300 dimensions is 18Gb. In practice, these file sizes are not as initially intimidating as they appear. Thus, the 6Gb 100 dimension vectors easily run in fasttext on laptop with 16Gb of RAM. Nevertheless, you should expect to require increased storage space to accommodate the size of datasets associated with machine learning and it is well worth investing in additional RAM for running machine learning tasks on CPU. However, the recent rise of state of the art Transformer models has introduced new demands in terms of size and a transition to GPU rather than CPU based processing. We will return to this at the end of this discussion.</p>
<p>Having introduced the basics of vector space models we now turn to a small practical example of text classification. We will then move on to Named Entity Recognition.</p>
</div>
<div id="machine-learning-in-classification" class="section level3 hasAnchor" number="6.0.12">
<h3><span class="header-section-number">6.0.12</span> Machine learning in Classification<a href="citations.html#machine-learning-in-classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The rise of machine learning has been accompanied by increasing attention to the possibility of automating the classification of patent documents. As patent filings come in to patent offices they need to be initially screened and classified in order to pass them to the relevant section of examiners. The ability to automate, or partly automate, this task could represent a significant cost saving for patent offices. At the same time, the determination of IPC or CPC classification codes that should be applied to describe the contents of a patent documents by examiner could be assisted by machine learning based approaches to produce predictions based on past experience on how similar documents were classified.</p>
<p>There is a growing body of literature on this topic and in order to understand existing progress we recommend searching for recent articles that will provide an overview of the state of the art and exploring articles that are cited in the references and articles that cite recent reviews. To assist in that process an open access collection on patent classification has been created on the Lens at [<a href="https://www.lens.org/lens/search/scholar/list?collectionId=199722(https://www.lens.org/lens/search/scholar/list?collectionId=199722" class="uri">https://www.lens.org/lens/search/scholar/list?collectionId=199722(https://www.lens.org/lens/search/scholar/list?collectionId=199722</a>)].</p>
<p>In the next section we will focus on a step by step example of text classification tasks to introduce the approaches.</p>
</div>
<div id="machine-learning-for-text-classification" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Machine Learning for Text Classification<a href="citations.html#machine-learning-for-text-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>To illustrate text classification we will use the Prodigy annotation tool from explosion.ai. explosion.ai is the company behind the very popular spaCy open source Python library for Natural Language Processing. spaCy is free. However, the Prodigy annotation tool involves an annual subscription fee. We focus here on the use of Prodigy because it allows us to be very transparent about text classification, named entity recognition and image classification. We also use Prodigy because of the tight integration with spaCy and ability to rapidly move models into production. The ability to move models into production is an important consideration when deciding on which tools to use. Academic tools are often focused on research purposes and any code may be of variable quality in terms of robustness and maintenance. This is not an issue with tools such as Prodigy and spaCy that focus on rapidly moving models into production.</p>
<p>In thinking about classification and other machine learning tasks it is helpful to think in terms of a set of steps. In the case of the drones data that we explored above we can identify three steps that we might want to perform.</p>
<ul>
<li>Step 1: Is this text about drone technology (yes/no)</li>
</ul>
<p>Step 1 is a filtering classification. We build a classification model that we can use to filter out anything in our raw data that does not involve drone technology.</p>
<ul>
<li>Step 2: Identify the main classes of drone technology (multilabel)</li>
</ul>
<p>This is a multilabel classification step. That is, a technology may fall into one or more areas of drone technology. We want to train a model to recognise the types that we are interested in.</p>
<ul>
<li>Step 3: Name Entity Recognition</li>
</ul>
<p>This is not a classification step as such but seeks to identify specific terms of interest in the texts that are selected by the classification models created in Step 1 and Step 2.</p>
<div id="step-1-binary-text-classification" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Step 1: Binary Text Classification<a href="citations.html#step-1-binary-text-classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We will start with a binary classification task where we will classify text from the new <code>dronesr</code> package that updates the data in the older <code>drones</code> package used above.</p>
<p>Prodigy contains a text classification function called <code>textcat.teach</code> that teaches a model what texts to accept and reject. In this case we create a dataset called drones, use a blank English model and import a small set of texts containing valid drone technology documents and a set that are noise. The texts will be labelled as accept/reject for the term DRONE. To assist the model with learning about the texts we have added a set of terms such as “autonomous vehicle”, “musical” and “bee” in a patterns file for the model to spot.</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb109-1"><a href="citations.html#cb109-1" aria-hidden="true" tabindex="-1"></a><span class="ex">prodigy</span> textcat.teach drones blank:en ./textcat_texts.csv <span class="at">--label</span> DRONE <span class="at">--patterns</span> textcat_patterns.jsonl </span></code></pre></div>
<p>The tool will then show each record highlighting the seed terms where relevant as we see in Figure <a href="citations.html#fig:uav">6.8</a>. For a binary task the choices are simply accept (green) or reject (red).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:uav"></span>
<img src="images/ml/uav.png" alt="Binary Classification in Prodigy" width="1712" />
<p class="caption">
Figure 6.8: Binary Classification in Prodigy
</p>
</div>
<p>The process of classification can lead to insights about possible multilabel classification labels, such as navigation in Figure <a href="citations.html#fig:route">6.9</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:route"></span>
<img src="images/ml/route.png" alt="Binary Classification in Prodigy" width="1756" />
<p class="caption">
Figure 6.9: Binary Classification in Prodigy
</p>
</div>
<p>Our terms also pick up one of the false positives on drones as we see in in Figure <a href="citations.html#fig:bee">6.10</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bee"></span>
<img src="images/ml/bee.png" alt="Binary Classification in Prodigy" width="1756" />
<p class="caption">
Figure 6.10: Binary Classification in Prodigy
</p>
</div>
<p>The outcome of the classification process is a set of annotations that take an accept/reject format. An example of an accept and reject text is provided below. The format is new line Json (jsonl) that is increasingly widely user and in this truncated version we can see that the start and end position of the match terms are recorded along with the label and the answer. Other machine learning software commonly adopts a similar format.</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb110-1"><a href="citations.html#cb110-1" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;text&quot;</span><span class="dt">:</span><span class="st">&quot;Newness and distinctiveness is claimed in the features of shape and configuration of an unmanned aerial vehicle as shown in the representations.&quot;</span><span class="op">,</span><span class="st">&quot;_input_hash&quot;</span><span class="dt">:411114389</span><span class="op">,</span><span class="st">&quot;_task_hash&quot;</span><span class="dt">:887216913</span><span class="op">,</span><span class="st">&quot;spans&quot;</span><span class="dt">:[{</span><span class="st">&quot;text&quot;</span><span class="dt">:</span><span class="st">&quot;unmanned aerial vehicle&quot;</span><span class="op">,</span><span class="st">&quot;start&quot;</span><span class="dt">:88</span><span class="op">,</span><span class="st">&quot;end&quot;</span><span class="dt">:111</span><span class="op">,</span><span class="st">&quot;pattern&quot;</span><span class="dt">:-1050519944}]</span><span class="op">,</span><span class="st">&quot;label&quot;</span><span class="dt">:</span><span class="st">&quot;DRONE&quot;</span><span class="op">,</span><span class="st">&quot;meta&quot;</span><span class="dt">:{</span><span class="st">&quot;pattern&quot;</span><span class="dt">:</span><span class="st">&quot;7&quot;</span><span class="dt">}</span><span class="op">,</span><span class="st">&quot;_view_id&quot;</span><span class="dt">:</span><span class="st">&quot;classification&quot;</span><span class="op">,</span><span class="st">&quot;answer&quot;</span><span class="dt">:</span><span class="st">&quot;accept&quot;</span><span class="op">,</span><span class="st">&quot;_timestamp&quot;</span><span class="dt">:1647448549}</span></span>
<span id="cb110-2"><a href="citations.html#cb110-2" aria-hidden="true" tabindex="-1"></a><span class="dt">{</span><span class="st">&quot;text&quot;</span><span class="dt">:</span><span class="st">&quot;The utility model provides a pair of be used for honeybee to breed device is equipped with honeybee passageway and worker bee special channel on the interior box...&quot;</span><span class="op">,</span><span class="st">&quot;_input_hash&quot;</span><span class="dt">:1202848216</span><span class="op">,</span><span class="st">&quot;_task_hash&quot;</span><span class="dt">:-1506636545</span><span class="op">,</span><span class="st">&quot;spans&quot;</span><span class="dt">:[{</span><span class="st">&quot;text&quot;</span><span class="dt">:</span><span class="st">&quot;honeybee&quot;</span><span class="op">,</span><span class="st">&quot;start&quot;</span><span class="dt">:49</span><span class="op">,</span><span class="st">&quot;end&quot;</span><span class="dt">:57</span><span class="op">,</span><span class="st">&quot;pattern&quot;</span><span class="dt">:-1721291956}</span><span class="op">,</span><span class="dt">{</span><span class="st">&quot;text&quot;</span><span class="dt">:</span><span class="st">&quot;honeybee&quot;</span><span class="op">,</span><span class="st">&quot;start&quot;</span><span class="dt">:91</span><span class="op">,</span><span class="st">&quot;end&quot;</span><span class="dt">:99</span><span class="op">,</span><span class="st">&quot;pattern&quot;</span><span class="dt">:-1721291956}</span><span class="op">,</span><span class="dt">{</span><span class="st">&quot;text&quot;</span><span class="dt">:</span><span class="st">&quot;bee&quot;</span><span class="op">,</span><span class="st">&quot;start&quot;</span><span class="dt">:122</span><span class="op">,</span><span class="st">&quot;end&quot;</span><span class="dt">:125</span><span class="op">,</span><span class="st">&quot;pattern&quot;</span><span class="dt">:899165370}]</span><span class="op">,</span><span class="st">&quot;label&quot;</span><span class="dt">:</span><span class="st">&quot;DRONE&quot;</span><span class="op">,</span><span class="st">&quot;meta&quot;</span><span class="dt">:{</span><span class="st">&quot;pattern&quot;</span><span class="dt">:</span><span class="st">&quot;15, 15, 13, 15, 15, 15, 15, 15, 13, 0, 13, 0, 15, 0, 0, 15&quot;</span><span class="dt">}</span><span class="op">,</span><span class="st">&quot;_view_id&quot;</span><span class="dt">:</span><span class="st">&quot;classification&quot;</span><span class="op">,</span><span class="st">&quot;answer&quot;</span><span class="dt">:</span><span class="st">&quot;reject&quot;</span><span class="op">,</span><span class="st">&quot;_timestamp&quot;</span><span class="dt">:1647452612}</span></span></code></pre></div>
<p>Once a number of annotations have been created it is possible to build a model to test out the approach and make adjustments. Model building is often conducted on the command line and involves dividing a set of texts into examples that are used to train the model and a set of examples that are used to test or evaluate the model. The second set is critically important because it is a set of examples with known answers that the model has never seen before.</p>
<p>This exposes a fundamental point about the form of supervised machine learning that we are using. It is extremely important to have data that is already labelled to use in training and evaluation. The creation of annotations is the hidden workload of machine learning.</p>
<p>With the development of transfer learning it is possible to use fewer annotations than before. However, you should still expect to think in terms of hundreds, thousands and possibly many more depending on the task.</p>
<p>An important strength of tools like prodigy is that it is possible to rapidly generate annotations and experiment.</p>
</div>
<div id="step-2-multilabel-text-classification" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Step 2: Multilabel Text Classification<a href="citations.html#step-2-multilabel-text-classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When the number of records has been narrowed down to remove noise we can start to engage in multi-label classification.</p>
<p>Important issue to consider here are the purposes of classification (what is the objective) and following definition of the objective is the number of labels to use. For our toy example with a handful of texts on drone patent documents a number of labels suggest themselves.</p>
<ul>
<li>controls (user and in vehicle)</li>
<li>communication (wifi, bluetooth)</li>
<li>navigation (GPS, collision avoidance, this is distinct from drones tracking objects)</li>
<li>power (batteries, charging pads)</li>
<li>sensing (scanning, imaging etc)</li>
<li>take off and landing</li>
<li>tracking</li>
</ul>
<p>That is quite a number of labels and this list is by no means conclusive. Here we would probably want to check the IPC codes for relevant categories. However, one challenge with the IPC categories is that they may be too broad or too specific. However, a review of IPC codes could assist with establishing categories in the areas above. This would have the advantage that you already have labelled texts in some of these categories and could save valuable time in labelling. That is, if you already have labelled data avoid relabelling unless you really have to.</p>
<p>Let’s take a look at the top IPC descriptions for the drones data to see if any of them match up with our rough list from reviewing the documents in prodigy. We can see the top 10 IPC subclasses by the count for the new <code>dronesr</code> dataset in Table <a href="citations.html#tab:ipc10">6.11</a> below.</p>
<table>
<caption>
<span id="tab:ipc10">Table 6.11: </span>Top 10 IPC descriptions for the drones data
</caption>
<thead>
<tr>
<th style="text-align:left;">
subclass
</th>
<th style="text-align:left;">
description
</th>
<th style="text-align:right;">
n
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
H04W
</td>
<td style="text-align:left;">
WIRELESS COMMUNICATION NETWORKS
</td>
<td style="text-align:right;">
23938
</td>
</tr>
<tr>
<td style="text-align:left;">
B64C
</td>
<td style="text-align:left;">
AEROPLANES
</td>
<td style="text-align:right;">
13108
</td>
</tr>
<tr>
<td style="text-align:left;">
G06F
</td>
<td style="text-align:left;">
ELECTRIC DIGITAL DATA PROCESSING
</td>
<td style="text-align:right;">
10797
</td>
</tr>
<tr>
<td style="text-align:left;">
H04L
</td>
<td style="text-align:left;">
TRANSMISSION OF DIGITAL INFORMATION
</td>
<td style="text-align:right;">
10614
</td>
</tr>
<tr>
<td style="text-align:left;">
H04N
</td>
<td style="text-align:left;">
PICTORIAL COMMUNICATION
</td>
<td style="text-align:right;">
9076
</td>
</tr>
<tr>
<td style="text-align:left;">
G01S
</td>
<td style="text-align:left;">
RADIO DIRECTION-FINDING
</td>
<td style="text-align:right;">
8042
</td>
</tr>
<tr>
<td style="text-align:left;">
G05D
</td>
<td style="text-align:left;">
CONTROLLING NON-ELECTRIC VARIABLES
</td>
<td style="text-align:right;">
6470
</td>
</tr>
<tr>
<td style="text-align:left;">
G06T
</td>
<td style="text-align:left;">
IMAGE DATA GENERATION
</td>
<td style="text-align:right;">
6448
</td>
</tr>
<tr>
<td style="text-align:left;">
B64D
</td>
<td style="text-align:left;">
AIRCRAFT EQUIPMENT
</td>
<td style="text-align:right;">
6372
</td>
</tr>
<tr>
<td style="text-align:left;">
G06Q
</td>
<td style="text-align:left;">
DATA PROCESSING SYSTEMS/METHODS
</td>
<td style="text-align:right;">
5942
</td>
</tr>
</tbody>
</table>
<p>We can immediately see that while the terms do not exactly match out existing list we can see communications (wireless communication, transmission of digital information), navigation (radio direction finding), sensing or imaging (pictorial communication &amp; image data generation) can be detected in the list while control systems for drones are addressed by controlling non-electric variables and computing by electric digital data processing and data processing system and methods. With the exception of aeroplanes these are not very suitable as labels and would ideally be shortened or codes (which are hard to recall would be used).</p>
<p>What this brief discussion makes clear is that the drones data is already labelled by a classification scheme. The IPC is also a multilabel classification scheme because patent documents commonly receive multiple classification codes in order to more accurately describe the content of a document.</p>
<p>This gives us two choices:</p>
<ol style="list-style-type: lower-alpha">
<li>If the existing classification scheme is suitable for our purpose then we can use the existing classification (the IPC) to create a classification model.</li>
<li>If the classification scheme doesn’t work for our purposes (it is either too broad or too specific) then we should focus on either adding specific labels to more finely describe groups of documents or, if the labels are too specific, we should concentrate on grouping.</li>
</ol>
<p>In preparing to create a classification model it is very important to have one or more test runs to assess whether your approach is likely to work. That is, approach the task as an iterative and experimental process. One reason to use a paid tool such as Prodigy is that it is designed to let you experiment. It is normally a mistake to imagine that you will accurately classify hundreds or thousands of documents in one go. It is better to start with experimental sessions and then classify in batches,</p>
<p>We will illustrate this process with our drones dataset by starting out with an initial set of labels and assessing how useful they are. Figure <a href="citations.html#fig:multi1">6.11</a> shows the classification of a patent abstract from the drones set.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:multi1"></span>
<img src="images/ml/multi1.png" alt="Multi Label Classification" width="1461" />
<p class="caption">
Figure 6.11: Multi Label Classification
</p>
</div>
<p>We have chosen a small set of 5 labels to classify the texts. We could have used more labels, however in reality the larger the label set is the more difficult it becomes for a human annotator to manage them. Indeed, the authors of Prodigy suggest that it can often be more efficient to use one label at a time and go through multiple rounds of iteration on each text for the same label. We have tested this approach and it works because it makes the decision-making process easier by reducing the choice to yes/no for a specific label.</p>
<p>It is normal to run into cases that can challenge your classification scheme quite quickly. Figure <a href="citations.html#fig:multi2">6.12</a>. Figure <a href="citations.html#fig:multi2">6.12</a> is for an drone that will land and deliver a liquid to a plant.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:multi2"></span>
<img src="images/ml/multi2.png" alt="Binary Classification in Prodigy" width="1292" />
<p class="caption">
Figure 6.12: Binary Classification in Prodigy
</p>
</div>
<p>In our existing scheme the label that is appropriate here is the single label <code>Imaging</code>. However, this highlights a need to focus on the purpose of the classification and establishing a clear definition of that purpose prior to annotating. That is, what is the objective of classification? It would be easy for example when viewing this type of example to add a label such as Agriculture. In practice, such as label would refer to the proposed USE of a claimed invention. Our classification scheme is directed to identifying the key technologies involved in the document rather than use.</p>
<p>As this brief discussion makes clear, establishing clarity on the purpose of multi-label classification is central to avoiding distractions and successful completion of the task. However, clear definition of the task will often only appear after a period of initial experimentation. For that reason, rather than jumping in with classification it is good practice to define a period of experimentation. That period may involve multiple sessions and experiments.</p>
<p>This process will typically involve reconsideration of the labels and a tighter definition of the label set. For example, what is the work that the Processing label is doing in the set above? Could the label be more tightly defined or should another label be used. For example, by processing we could be referring to machine learning (IPC G06N20/00 under computing). If our specific interest is in machine learning rather than processing in general we should change the label.</p>
<p>At the end of the classification process we will have a set of texts that contain multiple labels. In the case of prodigy and spaCy these will generally be exported as new line json (jsonl) where each document is contained in a single line. Other machine learning tools may use similar or simpler formats. We show two examples below with the long abstract texts abbreviated.</p>
<blockquote>
<p>{“text”:“The invention discloses a driving safety radar for a vehicle, and relates to the technical field drones…”options”:[{“id”:“CONTROLS”,“text”:“CONTROLS”},{“id”:“COMMUNICATION”,“text”:“COMMUNICATION”},{“id”:“IMAGING”,“text”:“IMAGING”},{“id”:“NAVIGATION”,“text”:“NAVIGATION”},{“id”:“PROCESSING”,“text”:“PROCESSING”}],“_view_id”:“choice”,“accept”:[],“config”:{“choice_style”:“multiple”},“answer”:“ignore”,“_timestamp”:1647597250}
{“text”:“The aircraft (FM) according to the invention comprises a helicopter drone (HD) on which a 3D scanner (SC) is mounted via an actively rotatable joint (G). The 3D scanner (SC) has at least one high-resolution camera (C)…”,“options”:[{“id”:“CONTROLS”,“text”:“CONTROLS”},{“id”:“COMMUNICATION”,“text”:“COMMUNICATION”},{“id”:“IMAGING”,“text”:“IMAGING”},{“id”:“NAVIGATION”,“text”:“NAVIGATION”},{“id”:“PROCESSING”,“text”:“PROCESSING”}],“_view_id”:“choice”,“accept”:[“IMAGING”,“PROCESSING”],“config”:{“choice_style”:“multiple”},“answer”:“accept”,“_timestamp”:1647597274}</p>
</blockquote>
<p>At the end of the multi-label classification task we will have set of texts that can be used to train a model. In prodigy and spaCy a few hundred examples will typically be enough to start experiments. As the number of annotations increases so should the accuracy of the model that is trained. At that point the annotator should switch focus to reviewing and correcting the annotations generated by a model rather than annotating from scratch.</p>
<p>Annotating texts is the most labour intensive aspect of machine learning and in some cases is outsourced to specialist companies. Machine learning tools such as Prodigy can take advantage of advances such as transfer-learning which means that it is possible to make fewer annotations to arrive at useful results.</p>
</div>
<div id="step-3-named-entity-recognition" class="section level3 hasAnchor" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> Step 3: Named Entity Recognition<a href="citations.html#step-3-named-entity-recognition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Named entity recognition is the task of training a model to recognise entities such as:</p>
<ul>
<li>dates (DATE)</li>
<li>persons (PERSON)</li>
<li>organisations (ORG)</li>
<li>geographical Political Entities (GPE)</li>
<li>locations (mountains, rivers) (LOC)</li>
<li>quantities (QUANTITY)</li>
</ul>
<p>A considerable amount of time and investment has gone into training what can be called ‘base’ models in multiple languages that will do a pretty good job of identifying these entities. There is therefore no need to start from scratch and most base models are trained on millions of texts (such as Wikipedia or news texts). Rather than seeking to build your own model from scratch the best course of action is to either:</p>
<ul>
<li>focus on improving existing labels if they meet your needs</li>
<li>add new labels for entities using a dictionary of terms as support.</li>
</ul>
<p>We will focus on the second approach. One challenge in Natural Language Processing is that it is commonly difficult for a model to capture all of the different entities that we may be interested in. This is particularly true for entities that involve multiple words. Most machine learning models are token based. That is, they focus on individual words (tokens) and entities that span multiple tokens (e.g. 3 or more words) can be difficult for models to accurately and consistently capture. For that reason, it is often desirable for the user to add a dictionary (thesaurus) of terms that they are interested in with a label. In other words, the machine learning approach is combined with a dictionary approach.</p>
<p>The use of dictionaries or thesauri with a model is a very powerful way of adding new entities and creating annotations that a model can learn from to automate the addition of a new entity type. In Prodigy (an annotation tool) and spaCy (the actual models) it is very easy to add match patterns or an entity ruler.</p>
<p>The reason that it is often desirable to combine machine learning models with dictionary based approach can be easily seen in Figure <a href="citations.html#fig:multi2">6.12</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ents"></span>
<img src="images/ml/ents.png" alt="Entity Recognition with the Medium Sized spaCy English Model" width="1387" />
<p class="caption">
Figure 6.13: Entity Recognition with the Medium Sized spaCy English Model
</p>
</div>
<p>In this case the model has only recognised the date elements of the texts, rather than entities we might be interested in such as the word drone. The reason for this is that the model has not been exposed to the the types of entities that we are interested in.</p>
<p>To improve the recognition we will add a dictionary (in spaCy this is an entity ruler and written in jsonl). For the purpose of illustration we will use similar labels to our classification experiment above. Table <a href="citations.html#tab:entityruler">6.12</a> shows the outcome of turning a column of words and phrases with labels into jsonl that can be used as an entity ruler attached to a spaCy model. Note that we specify that the terms will match on lower and upper case versions. We also include plural versions of the same terms (this can also be addressed using lemmatization rules). If seeking to get started with spaCy we recommend using the <a href="https://spacy.io/api/phrasematcher"><code>Phrase Matcher</code></a> first and the <a href="https://prodi.gy/docs/recipes#terms-to-patterns"><code>terms.to-patterns</code></a> recipe to create patterns files to use with annotations. If that does not meet your needs then you can then move on to creating your own code to write more advanced <a href="https://spacy.io/api/entityruler"><code>entity_ruler</code></a> patterns. <a href="https://explosion.ai/software">Explosion.ai</a> have created a <a href="https://explosion.ai/demos/matcher"><code>Rule-based Matcher Explorer</code></a> that you can use to experiment with different types of patterns.</p>
<table>
<caption>
<span id="tab:entityruler">Table 6.12: </span>Terms converted to jsonl for an entity ruler in spaCy
</caption>
<thead>
<tr>
<th style="text-align:left;">
text
</th>
<th style="text-align:left;">
label
</th>
<th style="text-align:left;">
jsonl
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
﻿Drone
</td>
<td style="text-align:left;">
TYPE
</td>
<td style="text-align:left;">
{"label":"TYPE","pattern":[{"LOWER":"drone"}],"id":"text"}
</td>
</tr>
<tr>
<td style="text-align:left;">
Unmanned aerial vehicle
</td>
<td style="text-align:left;">
TYPE
</td>
<td style="text-align:left;">
{"label":"TYPE","pattern":[{"LOWER":"unmanned"},{"LOWER":"aerial"},{"LOWER":"vehicle"}],"id":"text"}
</td>
</tr>
<tr>
<td style="text-align:left;">
UNMANNED AERIAL VEHICLES
</td>
<td style="text-align:left;">
TYPE
</td>
<td style="text-align:left;">
{"label":"TYPE","pattern":[{"LOWER":"unmanned"},{"LOWER":"aerial"},{"LOWER":"vehicles"}],"id":"text"}
</td>
</tr>
<tr>
<td style="text-align:left;">
unmanned vehicle
</td>
<td style="text-align:left;">
TYPE
</td>
<td style="text-align:left;">
{"label":"TYPE","pattern":[{"LOWER":"unmanned"},{"LOWER":"vehicle"}],"id":"text"}
</td>
</tr>
<tr>
<td style="text-align:left;">
autonomous vehicles
</td>
<td style="text-align:left;">
TYPE
</td>
<td style="text-align:left;">
{"label":"TYPE","pattern":[{"LOWER":"autonomous"},{"LOWER":"vehicles"}],"id":"text"}
</td>
</tr>
<tr>
<td style="text-align:left;">
AIRCRAFT
</td>
<td style="text-align:left;">
TYPE
</td>
<td style="text-align:left;">
{"label":"TYPE","pattern":[{"LOWER":"aircraft"}],"id":"text"}
</td>
</tr>
</tbody>
</table>
<p>We can attach the entity ruler directly to a model or, in this case, when using Prodigy we can add this as a set of patterns to use when matching (as we did above).</p>
<p>If you are using prodigy you could use the following in the terminal with either the <a href="https://prodi.gy/docs/recipes#ner-manual">ner.manual</a> or the the <a href="https://prodi.gy/docs/recipes#ner-teach">ner.teach recipe</a> to create annotations with new entity types that a model can learn from. Recipes will use the patterns file to bring forward texts containing those patterns so they can be annotated. In the code below we have added an empty label “USE” that we will use to start generating an entity recogniser for the types of use that are proposed in drone related patents. Note here that we could adopt a strategy of identifying uses with IPC codes (such as A01H for agriculture) and annotate the texts that fall into a specific area using the IPC as a guide.</p>
<div class="sourceCode" id="cb111"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb111-1"><a href="citations.html#cb111-1" aria-hidden="true" tabindex="-1"></a><span class="ex">prodigy</span> ner.manual drones_entities en_core_web_md ./pat_abstract.csv <span class="at">--label</span> ORG,TYPE,COMM,POWER,IMAGING,USE <span class="at">--patterns</span> ./drones_entity_ruler.jsonl</span></code></pre></div>
<p>In Figure <a href="citations.html#fig:use">6.14</a> below the model has picked up on the term <code>drone</code> from the match patterns as the type throughout the text. We have then expanded the type to <code>variable geometry drone</code> and captured the uses set out in the text.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:use"></span>
<img src="images/ml/use.png" alt="Manual Labelling of Entities with Prodigy" width="1370" />
<p class="caption">
Figure 6.14: Manual Labelling of Entities with Prodigy
</p>
</div>
<p>A second example in Figure <a href="citations.html#fig:use2">6.15</a> reveals how we can expand the range of use types as we proceed through the texts.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:use2"></span>
<img src="images/ml/use2.png" alt="Manual Labelling of Entities with Prodigy" width="1331" />
<p class="caption">
Figure 6.15: Manual Labelling of Entities with Prodigy
</p>
</div>
<p>Through iterative sessions we will be able to generate a range of labels for the data that can then be used to train a named entity recognition model.</p>
<p>As we have mentioned above, it will almost always be better to update an existing model because existing models will have been trained on very large volumes of data. However, where the nature of the texts is radically different another strategy is to create your own vectors (using fasttext or gensim) that become the basis for entity recognition. As we have seen above, it is easy to create vectors with tools such as fasttext and NLP libraries such as spaCy make it very easy to add your own vectors to create new models for specialist purposes. Specialist vectors can also be converted for use with Transformer models as the current state of the art in Natural Language Processing.</p>
</div>
<div id="from-vector-based-models-to-transformers" class="section level3 hasAnchor" number="6.1.4">
<h3><span class="header-section-number">6.1.4</span> From Vector Based Models to Transformers<a href="citations.html#from-vector-based-models-to-transformers" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have focused on vector based models in this chapter because they will be the models you will most commonly encounter <em>and</em> they will run quite happily on a laptop with a limited amount of RAM. In contrast current state of the art transformer models run very slowly on CPU and are designed to be run on GPU which you may not have ready access to.</p>
<p>Transformer models emerged from work at Google in 2017 and have rapidly become the cutting edge of NLP <span class="citation">(<a href="#ref-lens.org/086-980-365-076-590" role="doc-biblioref">Vaswani et al. 2017</a>)</span>. In their 2017 article revealing Transformers the team led by Ashish Vaswani explained that:</p>
<blockquote>
<p>“The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration…. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.” <span class="citation">(<a href="#ref-lens.org/086-980-365-076-590" role="doc-biblioref">Vaswani et al. 2017</a>)</span></p>
</blockquote>
<p>Over the last few years Transformers have become the go to models for Natural Language Processing at scale because of the improvements in accuracy they produce compared with recurrent (RNN) or convolutional (CNN) neural network based models. A range of transformer models have emerged such as BERT, XLNet and GPT-2 as well as a whole range of variants.</p>
<p>A great deal of work has gone in to making these models available for use by ordinary users (rather than the large technology companies that typically produce them). The AI community site operated by <a href="https://huggingface.co/landing/inference-api/startups?utm_source=Google&amp;utm_medium=Search&amp;utm_campaign=Transformers+10x+Faster&amp;utm_id=12055067954&amp;gclid=EAIaIQobChMI-I7pwfnP9gIVCbrtCh12DAQ7EAAYASAAEgJUMfD_BwE">Hugging Face</a> has been particularly important in providing a platform through which transformer models can be integrated into production level NLP tools such as spaCy.</p>
<p>The use of Transformer models such BERT with tools such as spaCy will require some setup. However, in the case of spaCy this is <a href="https://spacy.io/usage/embeddings-transformers#transformers">very well documented and easy to follow</a>. If you would like to try out the transformer models in spaCy on regular CPU then this is also easy by installing the .trf (for transformers) at the time of installation. The models will run but will be very slow. In addition, <a href="https://colab.research.google.com/?utm_source=scs-index">Google Colab</a> makes it possible to use GPU to run a spaCy transformer model for free to test it out. For larger scale tests or production level use, cloud service providers such as Amazon Web Services, Google Cloud Platform or Microsoft Azure offer GPU enabled ML pipelines as do services on these platforms such as Databricks Apache Spark (the author uses Databricks on Azure). We have focused on the Prodigy annotation tool and spaCy in this chapter because they are transparent, well documented an easy to explain. However, there are other libraries for NLP at scale including SparkNLP from John Snow Labs that performs natural language natively in spark (rather than through Python as is the case for spaCy). As such, there are an range of choices beyond those highlighted in this chapter.</p>
<p>The Hugging Face AI community site has become the main site for the sharing of a wide range of machine learning models involving natural language processing. These models include models for text generation, sentiment analysis, classification and named entity recognition and the ability to live test the models to see if they are a fit for your task as we see in Figure <a href="citations.html#fig:hugging">6.16</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hugging"></span>
<img src="images/ml/hugging.png" alt="The Hugging Face AI Community" width="1679" />
<p class="caption">
Figure 6.16: The Hugging Face AI Community
</p>
</div>
<p>Developments at Hugging Face reveal that an important transition is taking place between the users and developers of NLP machine learning software being the same people to a growing user community who want to try out different models to meet their needs and workflows.</p>
</div>
<div id="conclusion-3" class="section level3 hasAnchor" number="6.1.5">
<h3><span class="header-section-number">6.1.5</span> Conclusion<a href="citations.html#conclusion-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This chapter has addressed developments in machine learning for natural language processing and its implications for patent analytics. We have focused on the emergence of vector space based models and their uses for text classification and name entity recognition tasks using the Prodigy annotation tool and spaCy NLP library models from explosion.ai. In the process we have sought to emphasise some of the main insights from our own work with these and similar tools for patent analysts interested in benefiting from machine learning based approaches. In closing this chapter the following points are relevant to decision-making on machine learning and patent analytics.</p>
<ol style="list-style-type: decimal">
<li>Is machine learning based NLP really what I need? Machine learning libraries will typically make a big difference in patent analytics if you want to automate entity identification in your workflow. For small to medium scale projects it make sense as you will arrive at faster outcomes with standard NLP approaches and using VantagePoint. Alternatively, some of the plug and play offerings from cloud services that offer text classification or named entity recognition might be the right fit for you.</li>
<li>If the answer to the first question is yes, the next question is what tool to use for the particular tasks that you have in mind. The author’s work mainly focuses on the extraction of named entities from text that are relatively straightforward for NER models to manage, such as species, common, country and place names. More specialist tasks, such as the recognition of chemical, genetic or other types of entities would merit investigating libraries that have been designed or adapted for these types of entities. For example Allen AI has created the <a href="https://allenai.github.io/scispacy/">SciSpacy</a> set of models for working with biomedical, clinical or scientific texts. The Hugging Face community site may also be a good option for identifying models trained on particular tasks.</li>
<li>In deciding on a machine learning model or set of models focus on the documentation and the size of the user community. Machine learning can be deeply confusing. Identifying packages and models that are well documented and have active communities will allow you to get things done rather than navigating complex layers of documentation or puzzling over what the statistics from a model run mean. The author chose to use spaCy because it is well documented and supported and included a <a href="https://course.spacy.io/en/">free course</a> and <a href="https://github.com/explosion/projects">model project templates</a> for different use cases.</li>
<li>What does success look like? When engaging with machine learning it is easy to start pursuing perfection when the objective should be to pursue a model that does a good general job for the task at hand and saves on time and effort that a human would otherwise have to dedicate to the task.</li>
<li>Define how to evaluate the outcomes. It is extremely easy to create a model that will perform almost perfectly on 100 records using a 80% (training) to 20% (evaluation) split. That model will perform terribly on real world data even if the metrics produced by the model look great. There is a need to introduce measures that allow you to evaluate the real world utility of a model. For this tools such as prodigy are valuable for assessing (and correcting) a model in practice while VantagePoint from Search Technology Inc is very valuable for large scale assessment of model performance (e.g. on hundreds of thousands of tests).</li>
<li>Do not try to be on the cutting edge of machine learning for NLP. In patent analytics, unless you are an academic researcher developing methods, the place to be is generally a year or two behind the cutting edge. In a fast developing field it can take a while to understand what the successful new approaches really are and how you can use them in your work. For example, the present state of the art is represented by transformers but vector based models are much more practical for everyday use on a laptop and can produce excellent results. When investing in NLP libraries it can make very good sense to plan the transition from using one set of models to another. For example, the transition from version 2 to version 3 of spaCy involved some quite radical changes that required planning. It makes good sense to be some distance behind the curve and to plan transitions to newer approaches as they prove their worth, or, in the case of transformers, as they become more accessible to regular users.</li>
</ol>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-lens.org/056-971-725-855-254" class="csl-entry">
<em>Automated Categorization in the International Patent Classification</em>. 2003. Vol. 37. 1. United States: Association for Computing Machinery (ACM). <a href="https://doi.org/10.1145/945546.945547">https://doi.org/10.1145/945546.945547</a>.
</div>
<div id="ref-bojanowski2016enriching" class="csl-entry">
Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. <span>“Enriching Word Vectors with Subword Information.”</span> <em>arXiv Preprint arXiv:1607.04606</em>.
</div>
<div id="ref-Callaert_2011" class="csl-entry">
Callaert, Julie, Joris Grouwels, and Bart Van Looy. 2011. <span>“Delineating the Scientific Footprint in Technology: Identifying Scientific Publications Within Non-Patent References.”</span> <em>Scientometrics</em> 91 (2): 383–98. <a href="https://doi.org/10.1007/s11192-011-0573-9">https://doi.org/10.1007/s11192-011-0573-9</a>.
</div>
<div id="ref-Callaert_2006" class="csl-entry">
Callaert, Julie, Bart Van Looy, Arnold Verbeek, Koenraad Debackere, and Bart Thijs. 2006. <span>“Traces of Prior Art: An Analysis of Non-Patent References Found in Patent Documents.”</span> <em>Scientometrics</em> 69 (1): 3–20. <a href="https://doi.org/10.1007/s11192-006-0135-8">https://doi.org/10.1007/s11192-006-0135-8</a>.
</div>
<div id="ref-Carpenter_1980" class="csl-entry">
Carpenter, Mark P., Martin Cooper, and Francis Narin. 1980. <span>“Linkage Between Basic Research Literature and Patents.”</span> <em>Research Management</em> 23 (2): 30–35. <a href="https://doi.org/10.1080/00345334.1980.11756595">https://doi.org/10.1080/00345334.1980.11756595</a>.
</div>
<div id="ref-Chen_2017" class="csl-entry">
Chen, Lixin. 2017. <span>“Do Patent Citations Indicate Knowledge Linkage? The Evidence from Text Similarities Between Patents and Their Citations.”</span> <em>Journal of Informetrics</em> 11 (1): 63–79. <a href="https://doi.org/10.1016/j.joi.2016.04.018">https://doi.org/10.1016/j.joi.2016.04.018</a>.
</div>
<div id="ref-Cotropia_2013" class="csl-entry">
Cotropia, Christopher A., Mark A. Lemley, and Bhaven Sampat. 2013. <span>“Do Applicant Patent Citations Matter?”</span> <em>Research Policy</em> 42 (4): 844–54. <a href="https://doi.org/10.1016/j.respol.2013.01.003">https://doi.org/10.1016/j.respol.2013.01.003</a>.
</div>
<div id="ref-Cyranoski_2018" class="csl-entry">
Cyranoski, David, and Heidi Ledford. 2018. <span>“Genome-Edited Baby Claim Provokes International Outcry.”</span> <em>Nature</em> 563 (7733): 607–8. <a href="https://doi.org/10.1038/d41586-018-07545-0">https://doi.org/10.1038/d41586-018-07545-0</a>.
</div>
<div id="ref-Ding_2017" class="csl-entry">
Ding, Cherng G., Wen-Chi Hung, Meng-Che Lee, and Hung-Jui Wang. 2017. <span>“Exploring Paper Characteristics That Facilitate the Knowledge Flow from Science to Technology.”</span> <em>Journal of Informetrics</em> 11 (1): 244–56. <a href="https://doi.org/10.1016/j.joi.2016.12.004">https://doi.org/10.1016/j.joi.2016.12.004</a>.
</div>
<div id="ref-Egelie_2016" class="csl-entry">
Egelie, Knut J, Gregory D Graff, Sabina P Strand, and Berit Johansen. 2016. <span>“The Emerging Patent Landscape of <span>CRISPR</span><span></span>cas Gene Editing Technology.”</span> <em>Nature Biotechnology</em> 34 (10): 1025–31. <a href="https://doi.org/10.1038/nbt.3692">https://doi.org/10.1038/nbt.3692</a>.
</div>
<div id="ref-Fukuzawa_2015" class="csl-entry">
Fukuzawa, Naomi, and Takanori Ida. 2015. <span>“Science Linkages Between Scientific Articles and Patents for Leading Scientists in the Life and Medical Sciences Field: The Case of Japan.”</span> <em>Scientometrics</em> 106 (2): 629–44. <a href="https://doi.org/10.1007/s11192-015-1795-z">https://doi.org/10.1007/s11192-015-1795-z</a>.
</div>
<div id="ref-Hall_2012" class="csl-entry">
Hall, Bronwyn H., and Dietmar Harhoff. 2012. <span>“Recent Research on the Economics of Patents.”</span> <em>Annual Review of Economics</em> 4 (1): 541–65. <a href="https://doi.org/10.1146/annurev-economics-080511-111008">https://doi.org/10.1146/annurev-economics-080511-111008</a>.
</div>
<div id="ref-Hall_2005" class="csl-entry">
Hall, Bronwyn H., Adam Jaffe, and Manuel Trajtenberg. 2005. <span>“Market Value and Patent Citations.”</span> <em>The RAND Journal of Economics</em> 36 (1): 16–38. <a href="http://www.jstor.org/stable/1593752">http://www.jstor.org/stable/1593752</a>.
</div>
<div id="ref-Hall_2001" class="csl-entry">
Hall, Bronwyn, Adam Jaffe, and Manuel Trajtenberg. 2001. <span>“The <span>NBER</span> Patent Citation Data File: Lessons, Insights and Methodological Tools.”</span> National Bureau of Economic Research. <a href="https://doi.org/10.3386/w8498">https://doi.org/10.3386/w8498</a>.
</div>
<div id="ref-Harhoff_1999" class="csl-entry">
Harhoff, Dietmar, Francis Narin, F. M. Scherer, and Katrin Vopel. 1999. <span>“Citation Frequency and the Value of Patented Inventions.”</span> <em>The Review of Economics and Statistics</em> 81 (3): 511–15. <a href="http://www.jstor.org/stable/2646773">http://www.jstor.org/stable/2646773</a>.
</div>
<div id="ref-Harhoff_2003" class="csl-entry">
Harhoff, Dietmar, Frederic M Scherer, and Katrin Vopel. 2003. <span>“Citations, Family Size, Opposition and the Value of Patent Rights.”</span> <em>Research Policy</em> 32 (8): 1343–63. <a href="https://doi.org/10.1016/s0048-7333(02)00124-5">https://doi.org/10.1016/s0048-7333(02)00124-5</a>.
</div>
<div id="ref-Hegde_2009" class="csl-entry">
Hegde, Deepak, and Bhaven Sampat. 2009. <span>“Examiner Citations, Applicant Citations, and the Private Value of Patents.”</span> <em>Economics Letters</em> 105 (3): 287–89. <a href="https://doi.org/10.1016/j.econlet.2009.08.019">https://doi.org/10.1016/j.econlet.2009.08.019</a>.
</div>
<div id="ref-Jaffe_2017" class="csl-entry">
Jaffe, Adam B., and Gaétan de Rassenfosse. 2017. <span>“Patent Citation Data in Social Science Research: Overview and Best Practices.”</span> <em>Journal of the Association for Information Science and Technology</em> 68 (6): 1360–74. <a href="https://doi.org/10.1002/asi.23731">https://doi.org/10.1002/asi.23731</a>.
</div>
<div id="ref-Jaffe_1998" class="csl-entry">
Jaffe, Adam, and Manuel Trajtenberg. 1998. <span>“International Knowledge Flows: Evidence from Patent Citations.”</span> National Bureau of Economic Research. <a href="https://doi.org/10.3386/w6507">https://doi.org/10.3386/w6507</a>.
</div>
<div id="ref-Jaffe_2002" class="csl-entry">
———. 2002. <em>Patents, Citations, and Innovations: A Window on the Knowledge Economy</em>. MIT Press.
</div>
<div id="ref-joulin2016fasttext" class="csl-entry">
Joulin, Armand, Edouard Grave, Piotr Bojanowski, Matthijs Douze, Hérve Jégou, and Tomas Mikolov. 2016. <span>“FastText.zip: Compressing Text Classification Models.”</span> <em>arXiv Preprint arXiv:1612.03651</em>.
</div>
<div id="ref-joulin2016bag" class="csl-entry">
Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. <span>“Bag of Tricks for Efficient Text Classification.”</span> <em>arXiv Preprint arXiv:1607.01759</em>.
</div>
<div id="ref-Karvonen_2013" class="csl-entry">
Karvonen, Matti, and Tuomo Kässi. 2013. <span>“Patent Citations as a Tool for Analysing the Early Stages of Convergence.”</span> <em>Technological Forecasting and Social Change</em> 80 (6): 1094–1107. <a href="https://doi.org/10.1016/j.techfore.2012.05.006">https://doi.org/10.1016/j.techfore.2012.05.006</a>.
</div>
<div id="ref-Ledford_2016" class="csl-entry">
Ledford, Heidi. 2016. <span>“Bitter Fight over <span>CRISPR</span> Patent Heats Up.”</span> <em>Nature</em> 529 (7586): 265–65. <a href="https://doi.org/10.1038/nature.2015.17961">https://doi.org/10.1038/nature.2015.17961</a>.
</div>
<div id="ref-Ledford_2017" class="csl-entry">
———. 2017. <span>“Broad Institute Wins Bitter Battle over <span>CRISPR</span> Patents.”</span> <em>Nature</em> 542 (7642): 401–1. <a href="https://doi.org/10.1038/nature.2017.21502">https://doi.org/10.1038/nature.2017.21502</a>.
</div>
<div id="ref-Ledford_2018" class="csl-entry">
———. 2018. <span>“Pivotal <span>CRISPR</span> Patent Battle Won by Broad Institute.”</span> <em>Nature</em>, September. <a href="https://doi.org/10.1038/d41586-018-06656-y">https://doi.org/10.1038/d41586-018-06656-y</a>.
</div>
<div id="ref-Meyer_2000" class="csl-entry">
Meyer, Martin. 2000. <span>“Does Science Push Technology? Patents Citing Scientific Literature.”</span> <em>Research Policy</em> 29 (3): 409–34. <a href="https://doi.org/10.1016/s0048-7333(99)00040-2">https://doi.org/10.1016/s0048-7333(99)00040-2</a>.
</div>
<div id="ref-lens.org/104-512-929-235-758" class="csl-entry">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Efficient Estimation of Word Representations in Vector Space.”</span> <a href="https://128.84.21.199/abs/1301.3781?context=cs; https://arxiv.org/abs/1301.3781; https://arxiv.org/pdf/1301.3781; https://lens.org/104-512-929-235-758">https://128.84.21.199/abs/1301.3781?context=cs; https://arxiv.org/abs/1301.3781; https://arxiv.org/pdf/1301.3781; https://lens.org/104-512-929-235-758</a>.
</div>
<div id="ref-Narin_1995" class="csl-entry">
Narin, F., K. S. Hamilton, and D. Olivastro. 1995. <span>“Linkage Between Agency-Supported Research and Patented Industrial Technology.”</span> <em>Research Evaluation</em> 5 (3): 183–87. <a href="https://doi.org/10.1093/rev/5.3.183">https://doi.org/10.1093/rev/5.3.183</a>.
</div>
<div id="ref-Narin_1997" class="csl-entry">
Narin, Francis, Kimberly S. Hamilton, and Dominic Olivastro. 1997. <span>“The Increasing Linkage Between u.s. Technology and Public Science.”</span> <em>Research Policy</em> 26 (3): 317–30. <a href="https://doi.org/10.1016/s0048-7333(97)00013-9">https://doi.org/10.1016/s0048-7333(97)00013-9</a>.
</div>
<div id="ref-Oldham483826" class="csl-entry">
Oldham, Paul D, and Stephen Hall. 2018. <span>“Synthetic Biology: Mapping the Patent Landscape.”</span> <em>bioRxiv</em>. <a href="https://doi.org/10.1101/483826">https://doi.org/10.1101/483826</a>.
</div>
<div id="ref-lens.org/078-034-591-343-369" class="csl-entry">
Risch, Julian, and Ralf Krestel. 2019. <span>“Domain-Specific Word Embeddings for Patent Classification.”</span> <em>Drug Testing and Analysis</em> 53 (1): 108–22. <a href="https://doi.org/10.1108/dta-01-2019-0002">https://doi.org/10.1108/dta-01-2019-0002</a>.
</div>
<div id="ref-Rizzo_2018" class="csl-entry">
Rizzo, Ugo, Nicolò Barbieri, Laura Ramaciotti, and Demian Iannantuono. 2018. <span>“The Division of Labour Between Academia and Industry for the Generation of Radical Inventions.”</span> <em>The Journal of Technology Transfer</em>, August. <a href="https://doi.org/10.1007/s10961-018-9688-y">https://doi.org/10.1007/s10961-018-9688-y</a>.
</div>
<div id="ref-Scotchmer_1991" class="csl-entry">
Scotchmer, Suzanne. 1991. <span>“Standing on the Shoulders of Giants: Cumulative Research and the Patent Law.”</span> <em>Journal of Economic Perspectives</em> 5 (1): 29–41. <a href="https://doi.org/10.1257/jep.5.1.29">https://doi.org/10.1257/jep.5.1.29</a>.
</div>
<div id="ref-lens.org/086-980-365-076-590" class="csl-entry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <a href="https://lens.org/086-980-365-076-590">https://lens.org/086-980-365-076-590</a>.
</div>
<div id="ref-Webb_2005" class="csl-entry">
Webb, Colin, Hélène Dernis, Dietmar Harhoff, and Karin Hoisl. 2005. <span>“Analysing European and International Patent Citations: A Set of EPO Patent Database Building Blocks.”</span> OECD Science, Technology and Industry Working Papers 2005/9. OECD Publishing. <a href="https://EconPapers.repec.org/RePEc:oec:stiaaa:2005/9-en">https://EconPapers.repec.org/RePEc:oec:stiaaa:2005/9-en</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="23">
<li id="fn23"><p>To download the zipped file: [<a href="http://s3.amazonaws.com/data-patentsview-org/20180528/download/otherreference.tsv.zip" class="uri">http://s3.amazonaws.com/data-patentsview-org/20180528/download/otherreference.tsv.zip</a>][<a href="http://s3.amazonaws.com/data-patentsview-org/20180528/download/otherreference.tsv.zip" class="uri">http://s3.amazonaws.com/data-patentsview-org/20180528/download/otherreference.tsv.zip</a>]<a href="citations.html#fnref23" class="footnote-back">↩︎</a></p></li>
<li id="fn24"><p><span class="citation">Egelie et al. (<a href="#ref-Egelie_2016" role="doc-biblioref">2016</a>)</span> refer to this document by its family member US20140068797.<a href="citations.html#fnref24" class="footnote-back">↩︎</a></p></li>
<li id="fn25"><p>[Source: Broad Communications 2022<a href="https://www.broadinstitute.org/crispr/journalists-statement-and-background-crispr-patent-process">FOR JOURNALISTS: STATEMENTS AND BACKGROUND ON THE CRISPR PATENT PROCESS</a>, Updated February 28] <a href="citations.html#fnref25" class="footnote-back">↩︎</a></p></li>
<li id="fn26"><p><a href="https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/" class="uri">https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/</a><a href="citations.html#fnref26" class="footnote-back">↩︎</a></p></li>
<li id="fn27"><p>Available for download from Github at <a href="https://github.com/JasonHoou/USPTO-2M">https://github.com/JasonHoou/USPTO-2M</a> and at <a href="http://mleg.cse.sc.edu/DeepPatent/">http://mleg.cse.sc.edu/DeepPatent/</a><a href="citations.html#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p>Accessible from: <a href="https://hpi.de/naumann/projects/web-science/deep-learning-for-text/patent-classification.html">https://hpi.de/naumann/projects/web-science/deep-learning-for-text/patent-classification.html</a>, last accessed: 2019-09-17<a href="citations.html#fnref28" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="future.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["handbook.pdf", "handbook.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"show_codefolding_buttons": true
});
});
</script>

</body>

</html>
