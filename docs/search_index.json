[["index.html", "The WIPO Patent Analytics Handbook Preface", " The WIPO Patent Analytics Handbook Paul Oldham 2022 Preface The WIPO Patent Analytics Handbook provides an introduction to advanced methods for patent analytics and focuses on tools and skills that patent analysts can use in their everyday work. The Handbook builds on the WIPO Manual for Open Source Patent Analytics which provides an introduction to working with patent data using a range of free tools to obtain, clean and visualize patent data. The handbook aims to address two challenges. The first of these challenges is that anyone seeking to start work in patent analytics is confronted by a lack of reliable practical guidance on how to develop simple descriptive patent statistics. The OECD Patent Statistics Manual is required reading for anyone seeking to engage with patent statistics and is an invaluable resource (OECD Patent Statistics Manual 2009). However, it focuses on the issues we need to think about rather than practical demonstration. The Handbook addresses this problem by working through first principles in the development of patent counts for descriptive statistics and provides basic illustrations of the use of linear regression and forecasting models. In the process the Handbook aims to build a bridge to more sophisticated approaches to working with patent data at scale in fields such as econometrics and points to useful resources in these areas. The ability to generate descriptive patent statistics is only one aspect of patent analytics. Recent years have witnessed an explosion in the availability of different data types that can be integrated with patent data to better inform and enrich analysis. The second and major challenge addressed by the Handbook is integrating different data types from the scientific literature, to geographic information and the results of text mining into patent analytics. In turn the range of methods that are available to patent analysts for working with patent data promises to be transformed by the emergence of accessible machine learning tools for use across a range of topics such as applicant name cleaning, text mining and image classification. In common with many other fields of research the emergence of machine learning appears to hold considerable promise for patent analytics but it remains to be seen whether this promise will be realised. The Handbook is therefore intended to be used by researchers and professionals who are relatively new to working with patent data. It is also intended to be of interest for experienced researchers and professionals who are interested in expanding their skills in working with patent and related data at different scales. One important challenge that has emerged in recent years with the growth of patent analytics and patent landscape analysis is the problem of reproducibility (Smith et al. 2017). Patent analysts typically work with data from a number of different databases and use a number of different methods in their analysis. However, the precise details of the coverage of different sources, the methods used, and the limitations of different approaches are often not made explicit. This makes it difficult for others to reproduce the results and to assess the quality of the analysis presented. The Handbook takes the approach that patent analysis should be reproducible. The Handbook addresses this issue by using examples from standardised open access datasets created for this purpose or from public sources. The online version of the Handbook is an example of literate programming and all chapters are accompanied by the code used to develop the examples. The chapters in Rmarkdown format containing all code are available from the public GitHub repository at https://github.com/wipo-analytics/handbook References "],["about-the-author.html", "About the Author", " About the Author The Handbook was written by Paul Oldham under the coordination of Irene Kitsara at WIPO. Paul Oldham holds a PhD. from the London School of Economics and Political Science and is the Director of One World Analytics. He is a Senior Visiting Fellow and Industrial Fellow at the Manchester Institute of Innovation Research, Alliance Manchester Business School, Manchester University. Irene Kitsara is an Intellectual Property Lawyer who formerly worked with Deloitte and a Patent Analytics expert. She is presently an Intellectual Property Information Officer at WIPO with responsibility for Patent Analytics. "],["acknowlegements.html", "Acknowlegements", " Acknowlegements The WIPO Patent Analytics Handbook was written with the generous financial support of the Patent Office of Japan (JPO). The Handbook was prepared under the direction of Mr. Yo Takagi (Assistant Director General) and under the supervision of Mr. Alejandro Roca Campaña (Senior Director) and Mr. Andrew Czajkowski (Head of Section). Irene Kitsara (IP Information Officer) coordinated the preparation and review of the Handbook. The author thanks Craig D’souza and Lakshmi Supriya at WIPO for their comments and corrections on the draft of the Handbook. "],["how-to-use-the-handbook.html", "How to use the Handbook", " How to use the Handbook This Handbook consists of self standing chapters on different topics and is intended to be used in two ways. It can be used as a reference guide to a topic with worked examples for illustration and key literature sources to guide further reading on a topic. The Handbook can also be used as practical guide by downloading the datasets and reproducing the worked examples to help you apply the methods to your own analysis. Patent analytics involves a wide range of skills across different disciplines and one aim of the Handbook is to point to important sources of further information and training for each topic. If you wish to reproduce the examples please use the following instructions. The handbook and its examples were written in RStudio with examples drawn from VantagePoint by Search Technology Inc. RStudio is an easy to use and powerful platform for patent analytics and preparing data and reports for publication. To use RStudio you need to start by installing R for your operating system by visiting this http://cran.rstudio.com/. We will use the free version of RStudio that can be downloaded https://www.rstudio.com/products/rstudio/download/. VantagePoint from Search Technology Inc is specialist analytics software that is available under different price plans for students. It is the premiere tool for patent analytics and does not require programming knowledge. This Handbook is open access and each chapter and the code used to develop the examples can be downloaded free of charge from Github at https://github.com/wipo-analytics/handbook. The Handbook can be opened in RStudio using the handbook.Rproj file. The Handbook makes use of a number of R packages. To install all packages used in the Handbook run the following lines in the console. Note that this can take some time. The core package across all chapters is the tidyverse. The libraries needed to run code in the chapters is present in the code chunks in the individual chapters. install.packages(&quot;tidyverse&quot;) install.packages(&quot;rmarkdown&quot;) install.packages(&quot;collapsibleTree&quot;) install.packages(&quot;devtools&quot;) install.packages(&quot;fable&quot;) install.packages(&quot;fabletools&quot;) install.packages(&quot;feasts&quot;) install.packages(&quot;formattable&quot;) install.packages(&quot;ggmap&quot;) install.packages(&quot;ggraph&quot;) install.packages(&quot;googleway&quot;) install.packages(&quot;igraph&quot;) install.packages(&quot;janitor&quot;) install.packages(&quot;kableExtra&quot;) install.packages(&quot;knitr&quot;) install.packages(&quot;leaflet&quot;) install.packages(&quot;lubridate&quot;) install.packages(&quot;networkD3&quot;) install.packages(&quot;printr&quot;) install.packages(&quot;qdapRegex&quot;) install.packages(&quot;readxl&quot;) install.packages(&quot;stringi&quot;) install.packages(&quot;sunburstR&quot;) install.packages(&quot;tidytext&quot;) install.packages(&quot;tidymodels&quot;) install.packages(&quot;textstem&quot;) install.packages(&quot;textclean&quot;) install.packages(&quot;tokenizers&quot;) install.packages(&quot;tsibble&quot;) install.packages(&quot;udpipe&quot;) install.packages(&quot;usethis&quot;) install.packages(&quot;vroom&quot;) install.packages(&quot;widyr&quot;) install.packages(&quot;wordnet&quot;) The placement package requires a separate installation: library(devtools) install_github(&quot;DerekYves/placement&quot;) To load the libraries use: library(tidyverse) library(collapsibleTree) library(devtools) library(fable) library(fabletools) library(feasts) library(formattable) library(igraph) library(ggmap) library(ggraph) library(googleway) library(igraph) library(janitor) library(kableExtra) library(knitr) library(leaflet) library(lubridate) library(networkD3) library(placement) library(printr) library(qdapRegex) library(readxl) library(stringi) library(sunburstR) library(tidytext) library(tidymodels) library(textstem) library(textclean) library(tokenizers) library(tsibble) library(udpipe) library(usethis) library(vroom) library(widyr) library(wordnet) "],["note-to-readers.html", "Note to Readers", " Note to Readers We would like to ensure that the Handbook remains a useful resource for the patent analytics community. If you would like to correct or comment on entries in the Handbook please raise an issue on Github here. Alternatively please email poldham at mac dot com. "],["datasets.html", "Datasets", " Datasets All datasets used in the Handbook are publicly available through an Open Science Framework repository at https://osf.io/jr87e/ and are arranged by chapter. The Handbook makes extensive use of a data relating to drone technology. The drones dataset. This is a set of training datasets used in examples. The core dataset consists of 15,557 patent applications involving the term drone or drones somewhere in the text. You can download the data as a zip file from the repository at https://osf.io/download/zubd4/ Users of Rstudio can install the drones package directly using the following code. Note that the devtools package must be installed (included in the packages above). #install.packages(&quot;devtools&quot;) devtools::install_github(&quot;wipo-analytics/drones&quot;) When the drones package is installed review the contents of each dataset in the package documentation (see Packages in Rstudio) and load the data into your work space using the following. library(drones) drones &lt;- drones::drones A new version of the drones dataset package that we call dronesr has been created using data from The Lens database and its API. If you would like to use updated data to test the approaches provided in the Handbook then install the dronesr package. For those who are not using R the new data can be downloaded as a single zipped file from the Open Science Framework repository at https://osf.io/download/yngqc/](https://osf.io/download/yngqc/). #install.packages(&quot;devtools&quot;) # from github devtools::install_github(&quot;wipo-analytics/dronesr&quot;) The dronesr data contains patent data and scientific literature and is available in two public Lens collections: The patent collection https://www.lens.org/lens/search/patent/list?collectionId=199031 The literature collection https://www.lens.org/lens/search/scholar/list?collectionId=199039 The dronesr data is particularly valuable for readers interested in exploring the relationship between the scientific and the patent literature and citations. "],["introduction.html", "Chapter 1 Introduction", " Chapter 1 Introduction Patent analytics is a growing field that encompasses the analysis of patent data, analysis of the scientific literature, data cleaning, text mining, machine learning, geographic mapping and data visualisation. The WIPO Patent Analytics Handbook provides an introduction to advanced methods and tools for patent analytics. The Handbook complements the WIPO Manual on Open Source Patent Analytics which provides an introduction to tools and methods in patent analytics. The Handbook focuses on more advanced methods and approaches using commercial and free tools and databases. The fields of patent search, patent statistics and patent analytics have been transformed in recent years by the growing availability of free and commercial databases and software for data mining, data visualisation and geographic mapping. The increasing availability of a wide range of web services or Application Programming Interfaces for access to patent data, the scientific literature and cloud computing services for machine learning or geocoding mean that today a patent analyst has access to an unprecedented and cost effective range of tools to facilitate their work. Chapter 2 focuses on researching the scientific literature as a foundation for in depth patent research and analysis. This chapter begins by highlighting the growing accessibility of scientific publications and data arising from an increasing emphasis on open access publication. The chapter then focuses on the role of exploratory searches of the scientific literature in defining key word search strategies. The chapter then explores the main issues that arise when working with the scientific literature and how they can be addressed. The chapter concludes by considering strategies for joining together the scientific literature and patent literature. Chapter 3 examines geocoding of the scientific literature to develop geographic maps. Increasingly, it is possible to link different types of data on the same map using online geolocation services and to present the results in interactive form. This chapter provides a basic introductory guide to geocoding using the Google Maps API and packages that can be used to access the API. Chapter 4 provides an in depth exploration of methods for counting patent data as a basis for creating descriptive patent statistics and statistical models. Methodologies for patent counts have received remarkably little attention outside a highly specialised literature and this chapter aims to provide a step by step introduction to the issues involves in developing descriptive patent statistics. The chapter ranges through counts by priority, counts of patent applications, and counts by family. The chapter then provides a gentle introduction to linear regression using popular models as a basis for exploring predictive modelling by forecasting trends in PCT applications at WIPO. Chapter 5 addresses the importance of understanding patent classification systems as the key tool for supporting patent analytics. The patent system is supported by a range of classification schemes that are designed to assist patent examiners with identifying and retrieving patent documents. These classification schemes commonly take the form of alphanumeric codes organised from general to specific categories. This chapter discusses the use of the International Patent Classification (IPC) and the closely related Cooperative Patent Classification (CPC) in patent analytics. The chapter provides an in depth introduction to the International Patent Classification (IPC) with a case study of using the IPC to examine patent activity for animal genetic resources and concludes with a discussion of the growing use of classification systems in technology mapping. Chapter 6 explores the important role that patent citations play in patent analytics and the strengths and weaknesses of different approaches to patent citation analysis. The chapter begins with a description of the two types of patent citation (backwards and forward citations), the sources of patent citations and their impacts before considering different approaches to citation counts based on citations of individual documents and citations of patent families. The chapter uses the example of gene editing CRISPR patents and citations as a case study and concludes by exploring research on main path analysis with citation data. Chapter 7 provides an in depth introduction to text mining as a powerful tool in the patent analysts toolbox. Building on the discussion in Chapter 2 the chapter moves through the basics of text mining with patent data and concludes with a growing emphasis on machine learning approaches such as the popular Word2Vec algorithm. Chapter 8 examines the opportunities presented by machine learning to advance patent analytics. Machine learning or artificial intelligence approaches are increasingly being applied to text classification and named entity recognition and image classification. The application of machine learning in patent analytics remains at an early stage with the USPTO pioneering the application of machine learning algorithms to inventor and applicant name cleaning while Clarivate Analytics has recently applied machine learning to enhance the cleaning of applicant names. In future years we are likely to see the application of machine learning across the spectrum of patent analysis tasks. However, it can be very difficult to separate the hype around machine learning and artificial intelligence from the reality of what is available and achievable now. This chapter moves from the basics of machine learning approaches in Natural Language Processing to a worked example using the popular spaCy library in Python. Chapter 9 concludes this edition of the WIPO Patent Analytics Handbook with a discussion of the possible future(s) of patent analytics in the context of increasing access to patent and related data at scale and the rise of machine learning. "],["literature.html", "Chapter 2 Scientific Literature 2.1 Accessing the Scientific Literature 2.2 Searching Literature Databases 2.3 Precision vs. Recall 2.4 Processing Scientific Literature 2.5 Visualizing the Scientific Literature 2.6 Linking the Scientific Literature with Patent Analysis 2.7 Linking Citations with Patent Literature 2.8 Conclusion", " Chapter 2 Scientific Literature This chapter examines the role of research involving the scientific literature and patent analytics. Analysis of scientific literature is a specialised field in its own right in the form of bibliometrics or scientometrics with its own specialist journals such as Scientometrics and other publications (An Introduction to Bibliometrics 2018). These fields cover a wide range of topics involving statistical analysis of scientific literature such as indicators for science and technology, exploration of the impacts of scientific research, research networks, and the mobility of researchers. These field are characterised by a combination of qualitative and quantitative methods and are frequently oriented towards the understanding of trends in science and technology to inform research and innovation policies. The relationship between science and technological innovation is an important focus of research and links analysis of the scientific literature with patent literature. This chapter focuses on how analysis of the scientific literature can inform patent analytics in three main ways: By informing search strategies; By identifying actors who are active inside or outside the scientific literature as part of landscape analysis; Identifying potential opportunities for economic development to address the needs of developing countries. A key historic constraint in the ability to develop patent analysis informed by the scientific literature has been the lack of access to scientific data at scale. This situation is changing radically as a result of efforts to make metadata on scientific publications publicly accessible. The prime example of this is OpenAlex from Our Research as the replacement for Microsoft Academic Graph, that builds on work by organisations such as Crossref, The Directory of Open Access Journals to make data openly accessible. Other important initiative include the work of PubMed in making publicly funded research in the medical and biosciences openly accessible and CORE which makes the full texts of scientific publications openly accessible. We will begin with a brief overview of ways to access the scientific literature before turning to a discussion of scientific data fields using data from Clarivate Analytics Web of Science as an example. We will then explore how the scientific literature can be used to inform search strategies through the identification of terms from the scientific literature for use in patent searches. We will then look at methods for matching actors from the scientific literature into the patent literature using data from ASEAN countries as an example. Finally, we will look at how comparisons between the scientific literature and the patent literature can assist developing countries with identifying opportunities for economic development to address their needs. 2.1 Accessing the Scientific Literature The main means for accessing the scientific literature is through databases of scientific literature and increasingly through open access databases using web services of application programming interfaces (APIs). Researchers based in Universities will generally be familiar with two of the largest of the commercial databases of the scientific literature, Web of Science/Web of Knowledge from Clarivate Analytics or Elsevier’s Scopus. Open access databases such as PubMed and Crossref (containing metadata on over 96 million publications) are increasingly popular and link to initiatives such as core.ac.uk that, at the time of writing, make the full texts of over 113 million publications publicly available. Databases such as Google Scholar are a popular open access source of information on the scientific literature and access to copies of texts while social network sites for researchers such as Research Gate provides a means for scholars to share their research and create shared projects. An important feature of recent developments in scientific publication is a shift in emphasis towards open access publications on the part of researchers and funding agencies. This is reflected in services such as core.ac.uk noted above and in services such as Unpaywall which provides a browser plugin to identify open access versions of articles. At present Unpaywall contains links to over 19 million scientific publications. An important aspect of this shift in emphasis towards open access is cross service integration. Thus Unpaywall is based on and resolves article identifiers to the content of Crossref while the commercial Web of Science database provides links to Unpaywall in its results to allow free retrieval of articles. Other important emerging services include tools such as Open Academic Graph which provides access to meta data on over 330 million publications. As this makes clear the landscape for accessing scientific literature is changing as a result of the rise of web service enabled database and cross-service integration tools. In practical terms this means that access to the scientific literature is no longer entirely dependent on fee based databases. It is important to emphasise that publication databases normally have strengths and weaknesses in terms of: Coverage of journals, books and other publications The languages covered and availability of translations The range of fields available for analysis (authors, affiliations, titles, abstracts etc.) The basis of any statistical counts (e.g. counts of citing articles) The number of records that can be downloaded The format in which records can be downloaded These issues impose constraints on what can be searched and downloaded from scientific databases. For example, in our experience Web of Science permits for the downloaded of a wider range of data fields than Scopus, while open access databases enjoy the advantage of being free but are more limited in terms of the data fields that are available and the consistency of coverage, such as abstracts. When seeking to carry out literature research as part of a wider patent analytics project it is therefore important to consider the strengths and weaknesses of particular databases and to use multiple sources where necessary. 2.2 Searching Literature Databases 2.2.1 Stemming When searching a literature database it is important as a first step to understand the available search fields and search operators (such as OR and AND). Many databases now offer what is called word “stemming” that will look for similar words or phrases based on the root of the terms used during input… for example if we input the word “drone”, a stemmed version based on the root “drone” would include words like “drones”, “droned” and “droning”. In technical terms words like “drones”, “droned” and “droning” are lemmas. Word stemming is a powerful tool for expanding the range of searches and can be extended to using synonyms. Specialist tools such as WordNet, a lexical database of English words and synonyms, can be used to identify synonyms on a search term (Fellbaum 2015). WordNet can be used in a range of programming languages or using the free online tool. The results of a search of WordNet for the word Drone are presented below: Noun S: (n) drone (stingless male bee in a colony of social bees (especially honeybees) whose sole function is to mate with the queen) S: (n) monotone, drone, droning (an unchanging intonation) S: (n) dawdler, drone, laggard, lagger, trailer, poke (someone who takes more time than necessary; someone who lags behind) S: (n) drone, pilotless aircraft, radio-controlled aircraft (an aircraft without a pilot that is operated by remote control) S: (n) drone, drone pipe, bourdon (a pipe of the bagpipe that is tuned to produce a single continuous tone) Verb S: (v) drone (make a monotonous low dull sound) “The harmonium was droning on” S: (v) drone, drone on (talk in a monotonous voice) The use of a stemming tool helps to reveal the range of possible uses of a search term. In the case of the word drone we can see references to bees, to sound, to a part of a musical instrument and for pilotless aircraft. The range of the uses of these terms suggests a need for caution. Thus, the use of the term drone in a scientific database is likely to return results on all of these potential uses of the work drone. Stemming algorithms can both aid and hinder information retrieval. For example, if stemming is automatically turned on then the word “droning” would automatically be included and thus populate the results with data on the irrelevant subject of sound for those interested in drone technology. In contrast, where the stemming tool displays synonyms we might wish to include pilotless aircraft in the original search. In practice, when initiating a search of a database of the scientific literature on an unfamiliar subject it is generally best to turn off stemming and to focus on downloading a test set of results for review. The aim here is to use a limited set of terms to identify other potentially relevant terms and terms that can be excluded from a sample. This approach can be used with a wide range of software tools, including simple tools such as Excel or free online tools. The objective is to take available fields such as the title, abstract, and author keywords and to break them down into their constituent words and phrases. This is a process known as tokenizing text fields into words, phrases (ngrams), sentences and paragraphs of various lengths and is a fundamental feature of computational linguistics, text mining and machine learning. We will look in greater detail at text mining in Chapter 7. One powerful tool in working with both scientific and patent data is VantagePoint from Search Technology Inc. VantagePoint is available in a student edition and 32 and 64 bit versions for Windows. VantagePoint is able to import a wide range of different data sources and automatically tokenize text fields into words and phrases. Table 2.1 below presents the combined top 50 terms in the titles, abstracts, and author keywords from a search of Web of Science for the words drone or drones between 2010 and 2017. Table 2.1: Top Words and Phrases for Drones in Web of Science records terms multi_word 479 drones 0 241 drone 0 240 results 0 181 study 0 170 use 0 141 article 0 126 rights reserved 1 91 number 0 89 queens 0 88 development 0 87 workers 0 86 Apis mellifera 1 85 data 0 85 UAV 0 84 one 0 81 colonies 0 69 analysis 0 62 time 0 60 first 0 56 effects 0 When we inspect the results in Table 2.1 we see a full list of words and phrases. Many of these will not be relevant to drones as such, for example the words article or study and common noisy terms such as “and, or, of, for” are commonly excluded as stop words. In other cases references to the word “queens” or its root “queen” along with “Apis mellifera” and “colonies” suggests that we have a lot of data on bees somewhere in the data. In our next iteration of the search we would probably want to explicitly exclude these terms from the search. However we can also detect other words that we might want to include in our search such as “UAV” for Unmanned Aerial Vehicle. In practice multi word phrases (ngrams) commonly express concepts and bring us closer to the terms that we will want to use in a search. Table 2.2 ranks the data based on multi-word phrases. Table 2.2: Top Phrases in Web of Science Data on Drones records terms multi_word 126 rights reserved 1 86 Apis mellifera 1 50 honey bee 1 49 unmanned aerial vehicles 1 44 honey bees 1 42 Unmanned Aerial Vehicles (UAVs) 1 40 United States 1 32 remote sensing 1 29 unmanned aerial vehicle (UAV) 1 28 unmanned aerial vehicle 1 28 Varroa destructor 1 25 drone strikes 1 24 recent years 1 23 Elsevier Ltd 1 22 experimental results 1 20 drone warfare 1 20 present study 1 479 drones 0 241 drone 0 240 results 0 Table 2.2 reveals irrelevant phrases such as “honey bee” and its plural “honey bees” along with Varroa destructor, a mite that parasitises bees. We also observe the prevalence of unmanned aerial vehicles and their plurals linked to the term UAV and applications of drone technology such as “drone warfare” and “drone strikes”. A review of the terms captured in initial exploratory research can thus go a long way to refining a search strategy without a huge investment in time or software tools. 2.2.2 Using Search Operators Many databases include options for operator based searching. The common operators are: OR, AND and NOT. this OR that this AND that this NOT that The OR operator is an open operator for example we could search for drone OR drones OR droning This will locate texts that contain any of these terms. If we wanted to restrict the search to those that contain all of the terms we would use AND. drone AND drones AND droning Note that this is a more restrictive form of search because the documents must contain all three words. In contrast, if we were interested in drone technology and not in other uses of the word drone such as in musical instruments we would use NOT. drone OR drones NOT droning This would only return documents where the search terms drone or droning appeared without the word droning. Another perhaps more precise strategy would be to also exclude music. drone OR drones NOT (bee OR bees OR “Apis mellifera” OR “honey” OR droning OR music) The parentheses in the above search query are important because they specify that documents containing either be or bees etc. should be excluded from the results. The use of NOT based searching is a powerful way of excluding irrelevant documents. Boolean operators are extremely important when constructing search terms and may be expressed in other ways if using databases programmatically. For example in R both | and || mean OR and &amp; or &amp;&amp; means AND. In contrast Python uses logical OR, logical AND, logical NOT. A growing number of databases are powered by the Java based Apache Lucene or Solr (now best known perhaps from the widely used Elasticsearch). In Lucene and Solr in addition to the standard operators there is also “+” which specifies that a document must contain a term and may contain another term. +drone “unmanned aerial vehicle” To force both to appear we would use or the regular AND. +drone +“unmanned aerial vehicle” Note that the parentheses in the above version are important because it articulates that one or the other term should be excluded from the results. For more advanced programmatic uses, for example, using Elasticsearch or the PatentsView API you will frequently need to construct queries using the popular JavaScript Object Notation language (JSON). For example, the popular Lens patent database from Cambia in Australia offers a public interface and an API powered by Elasticsearch. Use of the API requires familiarisation with the construction of queries written in JSON. While this can be intimidating at first, worked examples, such as those provided in the Lens API documentation and the Elasticsearch documentation provide a set of signposts to writing your own queries for programmatic use with APIs. 2.2.3 Proximity Operators Proximity operators focus on the distance between words in a search term for example the operator NEAR with Web of Science allows the user to specify the distance between words such as (drone OR drone) NEAR/10 droning would find texts containing the word drone or drone within 10 words of the word droning. Another option, again from Web of Science is SAME. This is used in searches of the author affiliation field to treat two words as the same during search such as in the address field AD=(McGill Univ SAME Quebec SAME Canada). This search will treat the word Quebec and Canada as the same. Proximity operators can provide powerful tools for targeting a search of the scientific literature. However, when preparing to develop your search it is important to check the default settings used by the database and whether that meets your needs. In addition, it is important to note the operators that are available and the form that is expected. These will typically vary across the different databases. For example, many databases turn on stemming by default, use AND (rather than OR) as the default Boolean and may use ADJ (adjacent) as the default operator. Checking these settings at the beginning is important for avoiding confusing results when trying to establish a clear baseline search and in particular when working across multiple data sources. 2.2.4 Regular Expressions The use of regular expressions will be covered in greater detail in the discussion of text mining. However, it is worth noting that common regular expressions that you may be able to use in a literature databases include ^ starts with * any character wildcard. For example dron* would capture drone, drones, droned, droning etc. This can be used at the beginning, middle or the end of a term but is commonly used at the end. The wildcard should be used with caution. For example, a search for genomics related literature using the root genom and the wild card genom* will capture a potentially large number of results for the common German word genommen (took). $ ends with. For example drone$ would exclude drones and other close terms, however it would capture terms containing drone such as the Italian word “androne” (meaning entrance or entrance hall). As such, beware of unexpected results. Regular expressions can be combined in a whole variety of ways. One of the most useful is exact matching. ^drone$ This will exactly match the word drone and no other term. Very basic engagement with regular expressions is a powerful tool and it is well worth learning the basics. A good place to start is the long standing Regular Expressions Tutorial. However, be warned that regular expressions can become complex and difficult to understand quite rapidly. A well known quote about regular expressions is attributed to former Netscape Engineer Jamie Zawinski in a Usenet discussion group from 1997. Some people, when confronted with a problem, think “I know, I’ll use regular expressions.” Now they have two problems. 1 The point that is being made here is that regular expressions should not be the tool of first resort for every problem. Regular expressions can rapidly become very complex and difficult for a reader, including its author, to understand. Having said this a basic understanding of regular expressions is a very important part of the patent analysts toolbox as it allows you to precisely control what you are searching for and to parse the results. Tokenization of texts discussed above is an example of this that is typically based on word boundary matches (such as \\\\b) while named entity recognition in texts is often based on the identification of capitalized terms such as ^[[:upper]] or the equivalent ^[A-Z] to identify proper nouns (which are marked by the use of capital letters at the start of the word) such as people, place and other entity names. The precise form of a regular expression often depends on the language being used with Open Refine’s GREL or Google Regular Expression Language providing a nice practical introduction to using regular expressions as discussed in Chapter 10 of the WIPO Manual on Open Source Patent Analytics. Programming languages such as R include dedicated packages such as stringr that make it easier to work with regular expressions and cheat sheets have been developed to assist in remembering regular expressions.2. Websites such as the Regular Expressions Tutorial and https://regex101.com/ allow you to test out regular expressions in a range of different programming languages. 2.3 Precision vs. Recall At the end of the testing phase with scientific literature a set of new candidate terms and exclusion terms is the desirable outcome. For example, a more refined approach to the development of a search query for drone technology might look something like this. “drone” OR “drones” OR “UAV” OR “UAVs” OR “Unmanned Aerial Vehicle” OR “Unmanned Aerial Vehicles” OR “Pilotless Aircraft” NOT (“bee” OR “bees” OR “Apis mellifera” OR “honey” OR “droning” OR “music”) The use of the quotation marks in this case is intended to prevent a database from stemming the individual terms and the effect is to increase the level of precision in the inclusion and exclusion of terms. While it would be possible to modify this in a variety of ways using the wildcard or boundary markers, the importance of this type of approach is that it is simple, transparent, easy to reproduce and easy to modify in a way that can be tested. The discussion above is linked to a much larger body of literature on the distinction between Precision and Recall in information theory. For a literature database an example of this would be entering in a set of terms where the database returns 30 pages on drones that carry pizza boxes of which only 20 are relevant but fails to return the other 40 relevant documents. That is a precision rate of 20/30 = 2/3 as only 2 thirds of the returned documents are on topic. In contrast the recall rate is 20/60 or 1/3 because the database only returned a third of the actual relevant documents.3 The first measure is about the accuracy of the results and the second is about the completeness of the results. In practice precision vs. recall is about striking a good balance between accuracy (precision) and recall (completeness). One strategy for dealing with this is to start by favouring completeness by attempting to capture the universe of things relating to a topic and then filtering the data to arrive at more precise results to address the topic in question. In the next section we will use examples from the 2019 WIPO Patent Landscape Report on Marine Genetic Resources that used exactly this strategy. The starting point for the research was to capture all scientific publications that contained an author from one of ten South East Asian countries or that contained a reference to the country in the title, abstract or author keywords of a publication. This aim of this approach was to capture the universe of things that needed to be captured. That universe proved to be 391,380 publications after filtering some of the larger country datasets on subject categories to reduce irrelevant subject areas. The second stage of this exercise involved text mining the titles, abstracts and author keywords for marine species names. This radically reduced the dataset to 6,659 scientific publications. As this makes clear, the use of this method can initially be costly in terms of the requirements of initial data retrieval but has the advantage of capturing the universe of relevant documents. On that basis the marine species in the data could be accurately targeted. One very significant constraint when working with the scientific literature as opposed to patent data is that it is very rare that the full text of a scientific article is available for search. This has a major but not readily quantifiable impact on recall because the major body of the text is not readily accessible. This situation may change as databases such as CORE https://core.ac.uk/k make over 100 million full text scientific publications available for wider analysis. Debates around precision and recall and related concepts such as relevance are important across a wide range of computer and information retrieval fields including, for example, text and image classification in machine learning approaches. As regular users of search engines, patent analysts like other regular users, will encounter the outcomes of underlying and possibly hidden decisions on the database side about how to handle the balance between precision and recall. This will affect the total number (recall) and precision of the results that are presented to the searcher. It is therefore important to understand that what we receive from a particular database is likely to reflect not only its content (coverage) but these underlying decisions about precision and recall. On balance, in the author’s opinion, for patent analytics it is generally better to favour precision in analysis while at the same time clearly situating that analysis in the often fuzzier context of broader activity. We will return to this topic in the discussion of machine learning. 2.4 Processing Scientific Literature Databases of the scientific literature commonly return a range of different fields when data is downloaded. These can vary widely but will commonly include most of the following: Author Name Author Affiliation Title Abstract Author Keywords Document Identifier (e.g. doi, issn, isbn, openalexid) Funding Acknowledgements (limited coverage) Cited references Citation counts (documents citing the reference document) Subject categories or Concepts (derived) Researcher identifiers (ORCID, Researcher ID, PubMed Id, openalexid, other ID) This data is mainly extracted from the front page of publication or, in the case of references, the end of the document. However, subject category or concepts (e.g. for OpenAlex) are commonly added by the publication database itself. In some cases, such as ISI Web of Science, the classification of subject categories is based on classification of the subject areas covered by a journal and not the individual article. In contrast, building on the work of Microsoft Academic Graph, OpenAlex employs a machine learning based multi-label classification of individual articles (Wang et al. 2020). There are likely to be strengths and weaknesses to each of these approaches. At the time of writing, research in scientometrics and bibliometrics has been dominated by the use of journal subject areas as the basis for subject categories . More recent approaches focusing on article level classification embodied in OpenAlex may well prove superior but, to our knowledge, have yet to be evaluated. We will focus here on Web of Science. At the time of writing Web of Science used 252 Subject Categories and Scopus groups journals into 4 broad subject areas and 334 fields.4. A journal may be classified in more than one subject area with some such as Science, Nature and PLOS classified as interdisciplinary. The use of subject categories combined with citation analysis has been central to initiatives in the scientometrics community to develop maps of science Bollen et al. (2009b) including online interactive maps such as the https://www.scimagojr.com/shapeofscience/ using Scopus data with a gallery of maps on the structure of science from a range of sources made available through the http://scimaps.org/. The processing of scientific literature follows a pattern that is very similar to patent data as discussed in Chapter 4. These steps can be described as follows: Deduplicate the records using document identifiers (such as Web of Science ISI Unique Identifier, Lens Ids, OpenAlex ids or equivalent) to ensure that no records is over counted. Review the dataset for noise and exclude noise as required. Clean author names Clean affiliation/organisation names Clean funding information to focus on funding organisations Visualise the data The deduplication of the data is important to avoid over counting and can readily be achieved using document identifiers. Note that the best source for this is often the internal identifiers used by the databases as they are guaranteed to have 100% coverage unlike the doi field which is normally confined to journal articles. The exclusion of noise from the dataset will commonly involve reviewing the data by subject category (or concepts in the case of databases using OpenAlex). Table 2.3 below displays the top subject categories in a sample of 1400 publications for the term drone or drones from Web of Science. Table 2.3: Top Web of Science Subject Categories for Drones Sample Data Records Subject Category keep review exclude 150 Entomology 0 0 1 136 Multidisciplinary Sciences 0 1 0 112 International Relations 1 0 0 100 Engineering, Electrical &amp; Electronic 1 0 0 82 Political Science 1 0 0 70 Law 1 0 0 64 Telecommunications 1 0 0 58 Ecology 0 1 0 52 Environmental Sciences 0 1 0 46 Zoology 0 0 1 43 Remote Sensing 1 0 0 40 Engineering, Aerospace 1 0 0 38 Robotics 1 0 0 37 Biology 0 1 0 36 Computer Science, Information Systems 1 0 0 Here we can see that we have a significant number of publications in Entomology, and Zoology that are highly likely to be about bees rather than drone technology. When using VantagePoint or other tools including Excel one approach to data cleaning is to create three groupings to assist with the review. Keep Review Exclude In working through the data each record should be placed on one and only one of the groups. The review grouping is important because it allow you to maintain a focus on the keep and exclude groups and placing anything that is hard to decide about into Review. In the case of journal subject categories the review group is important because journal subject categories are somewhat crude. For example, Multidisciplinary sciences will include publications on drone technology and on bees. Agriculture related subjects are also a likely review category because bees and drone technology may appear in this category, for example for monitoring fields. The purpose of the review group is to allow time to view the records in particular categories with the aim of allocating all records to either keep or exclude at the end of this process. When working with data such as subject categories or concepts, the best place to start looking for noise is at very high frequency or very low frequency scores (the head and tail of data ranked by number of records). However, a complementary second step is to look at the sources of the publications. A sample of this data for drone technology from Web of Science is presented in Table 2.4. Table 2.4: Top Web of Science Sources for Drones Sample Data Records Source Title keep review exclude 49 NEW SCIENTIST 0 1 0 40 APIDOLOGIE 0 0 1 26 PLOS ONE 0 1 0 23 JOURNAL OF APICULTURAL RESEARCH 0 0 1 20 SENSORS 1 0 0 18 AEROSPACE AMERICA 1 0 0 17 FOREIGN AFFAIRS 1 0 0 17 REMOTE SENSING 1 0 0 15 INTERNATIONAL AFFAIRS 1 0 0 14 IEEE SPECTRUM 1 0 0 13 JOURNAL OF APICULTURAL SCIENCE 0 0 1 12 ETHICS &amp; INTERNATIONAL AFFAIRS 1 0 0 12 JOURNAL OF INTELLIGENT &amp; ROBOTIC SYSTEMS 1 0 0 12 NEW YORK REVIEW OF BOOKS 1 0 0 11 INSECTES SOCIAUX 0 0 1 11 NATION 1 0 0 11 SCIENTIFIC REPORTS 0 1 0 10 CHEMICAL &amp; ENGINEERING NEWS 1 0 0 10 COMPUTER LAW &amp; SECURITY REVIEW 1 0 0 10 JOURNAL OF ECONOMIC ENTOMOLOGY 0 0 1 10 JOURNAL OF EXPERIMENTAL BIOLOGY 0 0 1 9 SCIENCE 0 1 0 9 SECURITY DIALOGUE 1 0 0 8 BULLETIN OF THE ATOMIC SCIENTISTS 1 0 0 8 INTERNATIONAL JOURNAL OF REMOTE SENSING 1 0 0 In this case we can see that publications such as Apidologie can readily be excluded where as journals such as PLOS ONE that publish across a range of fields would require review. We can also see that a number of social science and humanities subjects are entering into the picture and depending on our purpose we might want to focus publications down to those relating to remote sensing, engineering and related subjects. As part of this review process it is important not to second guess the technology area. For example, we should not assume that everything associated with biology should be excluded. Biomimicry is for example an important area of inspiration in some areas of drone technology (such as swarming behaviour) while some publications that refer to drones and biology refer to the use of drone technology in anti-poaching and conservation biology. It is precisely because of the lack of predictability of new and emerging areas of technology that an approach concentrating initially on recall and then on precision is often the most successful route to accurate analytics. The alternative is for analysts to impose a definition of a new technology area on the field of research and thus potentially exclude important features of the technology field and debates around those fields (such as military drone strikes). The outcome of this review process is that each record falls into a keep or exclude category and a smaller dataset is generated containing the data the analyst wishes to keep. At this stage the main body of data cleaning focusing on author organisations (in the author affiliation) and author names along with the text in funding acknowledgements can begin. The basic procedure for name cleaning has been described in Chapter 10 of the WIPO Manual on Open Source Patent Analytics using the free Open Refine software tool. However, accuracy in name cleaning is best achieved using multiple match criteria to address cases where an author shares a name with another author but is a distinct person. VantagePoint from Search Technology Inc provides a means to achieve this by linking a fuzzy logic name cleaning algorithm that clusters names based on similarity scores with a setting that allows another field to be used to match the data. That is a search for John Smith that is run without match criteria will group different John Smiths together. A clean up that is run by grouping John Smiths using the author affiliation will distinguish between John Smiths working at say the University of California or John Smiths working at London University (Carley, Porter, and Youtie 2019). As this example also suggests name cleaning is often a multi-step process because in reality multiple John Smiths may work at the University of California. In that case a second step might be to use shared co-authors or subject categories as a basis for decision making using the keep, review, exclude method described above. The same approach is then applied to the applicant organisation where particular attention is required to organisations that share similar names but are distinct entities. Thus, Washington University and the University of Washington are distinct entities. When cleaning organisation names note that decisions need to be made on how to address regional and international organisations and to provide notes in the resulting report or publication on decision-making to inform the reader. In considering the clean up process for author names described above note that it is often easier to begin by cleaning up the author affiliation names and then to clean author names using the cleaned organisation names as the match criteria. An important development in recent years has been the increasing use of author identifiers in publication records. A number of author identifier systems exist such as Researcher ID from Web of Science or Scopus ID and PubMed ID but the most important of these is ORCID which is a non-profit open access researcher identifier system. Where a researcher identifier is available these identifiers can be used to cluster variations of names with a degree of certainty that they are the same person or that persons with the same name with distinct ORCID IDs will in fact be distinct persons. At a higher level of detail ORCID ID public profiles can be looked up online to assist with assessing whether a researcher listed as belonging to one institution has moved to another. Cases of author movement will frequently involve a researcher working in the same area of research but listing more than one affiliation. ORCID identifiers help to resolve these cases. However, while ORCID may assist with addressing knotty author name disambiguation problems it is important to note that it is not perfect. For example, ORCIDs may be misassigned or a single person may end up with more than one ORCIC (for example one created by the author and another by the institution). This reveals that no approach to author name disambiguation will ever be perfect. More recent approaches to author identifies, notably OpenAlex building on author ids in Microsoft Academic Graph, merit further evaluation as efforts continue to improve disambiguation at scale at the database level. Funding data is a relatively new feature in publication databases and the presence of this data, which commonly appears in the Acknowledgements field, can be spectacularly messy. For example: The COLOSS (Prevention of honey bee COlony LOSSes) network aims to explain and prevent massive honey bee colony losses. It was funded through the COST Action FA0803. COST (European Cooperation in Science and Technology) is a unique means for European researchers to jointly develop their own ideas and new initiatives across all scientific disciplines through trans-European networking of nationally funded research activities. Based on a pan-European intergovernmental framework for cooperation in science and technology, COST has contributed since its creation more than 40 years ago to closing the gap between science, policy makers and society throughout Europe and beyond. COST is supported by the EU Seventh Framework Programme for research, technological development and demonstration activities (Official Journal L 412, 30 December 2006). The European Science Foundation as implementing agent of COST provides the COST Office through an EC Grant Agreement. The Council of the European Union provides the COST Secretariat. The COLOSS network is now supported by the Ricola Foundation - Nature &amp; Culture. Literature databases are attempting to parse relevant information from this data such as the name of the funder and the contract or award number with varying degrees of success as follows: COST Action, FA0803 | EU Seventh Framework Programme, - | Ricola Foundation - Nature Culture, - In considering the brief discussion of regular expressions above note the focus in the parsing of this data on Nouns and Proper Nouns and numeric entries. It is likely that dictionary based approaches and machine learning based name entity recognition are increasingly being applied to this type of problem. The Research Organization Registry (ROR) Community is directed towards the disambiguation of research organization names and releases regular updates of a standard and open access organisation register that is now included in OpenAlex and other databases (building on the earlier GRID initiative). While not directed explicitly to funding organisations, funding and other organisations (such as the European Commission) can appear in ROR entries and thus assist with cleaning up funding information. An important challenge when dealing with funding information is determining whether data should be grouped or not. For example should funding from the European Commission under the Framework programmes and those under European regional or sectoral funds be grouped together? The answer to this question will depend in part on the level of detail required by the research. However, this can be very important when visualising data on funding, such as comparison between the EU and the US where a failure to aggregate at the right level may lead to inadvertent misrepresentation of the data. In general the approach taken, such as grouping all EU level funding together, should be made clear in an explanatory note to the reader when presenting the results of the data. As noted above the cleaning of funding data is closely related with cleaning organisation names. The development of the ROR dataset, encompassing over 100,000 organisation names and related acronyms and variants is an important step forward. The inclusion of ROR entries in OpenAlex (as with the earlier GRID identifiers in Microsoft Academic Graph) takes away a significant portion of the pain of name cleaning for scientific organisations. However, in the case of OpenAlex and earlier MAG a significant portion of organisation names may not have an ROR id or not map to an existing name or variant. Cleaning of this data requires investigation of the raw organisation data (institutions in OpenAlex) and the use of tools such as VantagePoint. Nevertheless, the advantage of the increasing application of ROR ar the database level is that over time an increasing number of organisation names will be cleaned up and (as we will see below) will become available for other tasks such as geospatial mapping. One important observation on cleaning data is to consider how detailed the cleaning operation needs to be. For example, if only the top ten or top 20 results will be shown to the reader it is important to ensure that person, organisation or funding organisation have been cleaned to capture all relevant name variants to ensure the accuracy of counts. Having said this, there is no point whatsoever expending energy cleaning data that will never be used or seen by someone else. Finding the right balance in name cleaning between ensuring accuracy of reporting and avoiding wasting time and effort is an important skill but will often depend on the requirements of the specific task and the data at hand. We will return to the subject of name cleaning elsewhere in the handbook. However, in closing this discussion we would emphasise the importance of resources such as ROR in setting a standard for the use of names and the efforts of the USPTO PatentsView team in creating disambiguation tables for applicant and inventor names (available through the PatentsView data service for grants and applications or pregrants). These initiatives, despite their weaknesses, can save patent analyst many hours of work and readily lend themselves to use in automated routines (e.g. SQL table joins). We now turn to some basic feature of visualisation of the scientific literature. 2.5 Visualizing the Scientific Literature A wide range of options are available for visualising data from the scientific literature. Typically this will include basic data on trends, geographic distribution of records, subject areas, top ranking organisations and researchers. When working to visualize data it is a very good idea to become familiar with some of the excellent literature on this topic notably the classic book The Visual Display of Quantitative Information by Edward Tufte (1983) and the more recent Stephen Few (2012) Show Me the Numbers: Designing Tables and Graphs to Enlighten. To illustrate some approaches to visualising data from the scientific literature we will use data from the 2019 WIPO Patent Landscape Report on Marine Genetic Resources. 2.5.1 Dashboards Dashboards are a powerful and popular way of summarising data. The creation of basic dashboards using Tableau is covered in detail in the Chapter 11 of the WIPO Manual on Open Source Patent Analytics. Figure 2.1 shows a summary of the overall data on scientific research on marine genetic resources in South East Asia. Figure 2.1: Overview of Research on Marine Genetic Resources in ASEAN Countries Figure 2.2 displays details of the species, subject areas, organisations and authors. Figure 2.2: Overview of Marine Species, Organisations and Authors The effect of the use of dashboards is to convey the principle factual information in an easily digestible form. As readers will commonly scan from left to right, the first panel should contain the key “focus” information that you wish to convey. In the first case above the aim of the first panel is to draw attention to the fact that the data is from South East Asia. In the second panel the aim is to draw attention to the marine species as the key to interpreting what the data is about. Note that attention may be required to issues such as the size of fonts and the number of panels in communicating results to the reader. Tableau was a pioneer in the use of dashboards, but many other software providers now offer variants of dashboards and it is possible to write dashboards in code using R (for example flexdashboards) and in Python (for example using Dash from Plotly) among others. While Tableau Public and Tableau Desktop provide easy options for creating interactive versions of dashboards (based on SQL joins between shared fields), programming languages provide for much greater customisation and flexibility. It is also possible to have the best of both worlds, for example by using Tableau for fast exploratory prototyping and then writing your own code, or by using extensions such as the R Shiny Tableau extension. One issue with visualisations of data in this way is that they are vertical. We do not see the relationships between entities in the data when in practice scientific research is commonly conducted as part of networks of collaboration on different levels. Network visualisations address this problem. 2.5.2 Network Visualisation Figure 2.3 displays a network view of the relationships between authors involved in scientific research on marine genetic resources in South East Asia. The dots are sized based on the number of publications associated with an author. The lines or edges represent co-authored publications. The network has been limited to display authors with 20 or more publications. Figure 2.3: Research Networks for researchers with 20 or more publications on marine genetic resources These network images are important because they display relationships that are difficult to see in any other way. A particularly good example of this is networks of funding organisations as set out in Figure 2.4. Note that in Figure 2.4 the size of the dots represents the number of publications where the funding agency appears in the acknowledgements ad does not reflect the size of financial investments. The lines represent publications where different funding agencies appear in the acknowledgements. Figure 2.4: Network of Funding Organisations Supporting Research on Marine Genetic Resources The full extent of network relationships is typically invisible to network participants. This is particularly true for networks of funding organisations. However, network visualisation is a powerful tool for engagement with researchers and audiences interested in a particular subject. The network visualisations presented above were created using the free Gephi software and a practical guide to creating these networks is provided in Chapter 10 of the WIPO Manual on Open Source Patent Analytics. 2.5.3 Other forms of visualisation Data visualisation has advanced rapidly in recent years and the D3 JavaScript library has been responsible for a virtual explosion in creativity with interactive graphics. Examples of visualisation possibilities can be viewed in the D3 gallery on Github https://github.com/d3/d3/wiki/Gallery. One among other possibly fruitful options for data visualization is the Sankey diagram a form of dendrogram that aims to display the flow of energy between entities. Figure 2.5 displays the flow of research publications on a marine species in South East Asia into journals by subject area. Figure 2.5: A Sankey Diagram showing flows of research on marine genera into journals by subject areas This type of visualisation serves the useful purpose of showing the flow of research effort represented by publications as outputs into different subject areas. A particular strength of this visualisation is that we can see the proportion of research on a particular genus of marine organisms such as prawns in the genus Penaeus such as the Giant Tiger Prawn into journals on particular subjects. For Penaeus, a major focus of aquaculture in South East Asia, we can see that the flow of research energy is channeled towards fisheries, marine and freshwater biology and Veterinary Sciences (to address diseases with an effect on this commercially important genus such as viruses in the genus Vibrio). When viewed online this diagram is interactive and will highlight flows from a particular genus to a subject area. The ability to create Sankey diagrams depends in a large measure on a willingness to engage with a programming language such as Javascript, R or Python that provide libraries to make the calculations and generate the Javascript diagrams. Thus the diagram above was generated with the D3network package in R. However, a number of online services offer the ability to create Sankey diagrams and these may meet your needs. 2.6 Linking the Scientific Literature with Patent Analysis Analysis of the scientific literature is important because it allows us to understand the landscape of research for a particular topic. In the case of drone technology we saw that exploratory searches could assist in identifying key words for the construction of more refined search strategies and to progressively exclude noise from the results. In the data we have presented above on research on marine genetic resources in South East Asia we processed the data to answer the following fundamental questions: Who (and with whom?) What Where When How These are standard questions in empirical research. The final question requires detailed attention to the literature itself in terms of understanding the precise subject matter of research by a particular individual or a research team. However, this type of landscape analysis allows us to investigate whether research has the potential to be transformed into a commercial product, method or process and therefore brings us to the patent and wider intellectual property system. This type of research can be useful on a range of different levels: universities may be interested in identifying research outputs that may have potential to turn into useful products, methods or processes companies active in particular sectors may be seeking to develop new products and are seeking to identify relevant existing research Funding agencies may be seeking to understand the existing outcomes of research investments and to identify relevant areas of priority research that promise to result in new and useful products. In many cases analysis of the scientific landscape will take place at a lower level than the ten countries covered by the research on South East Asian countries covered above. However, this example illustrates the possibility of using these methods and approaches to answer empirical questions at scale and then to drill down into the fine grained detail of research. One major question that arises here is how to link together research on the scientific literature with research in the patent system. There are two main answers to this. To use keywords and phrases identified in research from the scientific literature as the basis for searches of the patent literature. This is likely to be the most common approach. As discussed above access to sections of the literature such as titles, abstracts and author keywords allows for the application of basic text mining approaches to breaking texts into words and phrases. This in turn allows for the literature to be classified and refined to identify targets of interest. In software such as VantagePoint this is commonly done by sorting the data into groups. For example, in the case of drone technology one important area of research focuses on sensors while another separate area of research focuses on wireless devices to supply power to a drone while a third focuses on devices such as headsets and other devices for controlling a drone in flight. To focus on identifying individual researchers who are active in a research field who are also active in the patent system This approach to linking scientific research with patent data is rarer for the straightforward reason that it is much harder to do at scale than an approach using keywords. However, it has the advantage of providing a clearer view of researchers who are already active in commercial research and development with a high degree of precision. 2.6.1 Mapping Authors to Inventors The identifying researchers who are active in the patent system involves a three step process Joining a dataset with the scientific literature to a patent dataset and combining the inventor and author name fields. Identifying match criteria to establish whether an author and inventor are the same person Applying the match criteria to arrive at a dataset that includes authors who are also inventors Reviewing and summarising the data. The first step in the process involves identifying the appropriate approach to creating a patent dataset. This could involve the use of a broad set of terms to capture the likely universe of patent activity using the scientific literature as a guide to term selection. For example, in the case of drone technology it would be logical to create a working dataset using terms discussed above such as drone and unmanned aerial vehicle. The primary issue here is spreading the net wide enough to capture the universe of activity while narrowing the data sufficiently to avoid using a vast dataset. For research on national level activity, such as in the case of the landscape for research on marine genetic resources in South East Asia the approach taken was to identify patent activity from the national collections and patent activity worldwide linked to a South East Asian inventor or applicant followed by text mining the data for marine species and treating that data as the working dataset. This approach required access to patent data at scale and the ability to process that data (performed in VantagePoint and R). VantagePoint is an important tool for joining datasets of different types and creating a common field. Thus, in the research on South East Asia the scientific data and patent data were combined into one dataset . In the next step the authors full name field and the inventor name field were combined together. In both cases the names had previously been cleaned. In the case of the scientific literature there were a total of 17,625 names and in the case of the patent data there were 9,832 names. The next step in the approach is the use of match criteria. In this case the following criteria were used. An author and a co-author appeared as inventors in the same patent document The name of an author and the organisation listed as the applicant matched with the author affiliation. An author name matched with an inventor name and the marine species name appeared in both the scientific publication and a patent document The purpose of these criteria is to identify author-inventors and it is important to note that the third match criteria can vary from dataset to dataset. The important point is to identify and use match criteria. In order to qualify as an author inventor the record was required to meet at least one and preferably two of the criteria above. Experience has revealed that the names of co-authors who appear as inventors is the most accurate match criteria. One exception to this is East Asian names where, in accordance with traditional naming practices, the names of co-authors and inventors may be very common. This can result in false positive matches and it is therefore important to isolate such cases to test against the other match criteria. As discussed above, one useful method for working with large amounts of data is to allocate records to keep, review and exclude groups and adopt a method of multiple passes. At the end of the first pass the review group will typically be large because it marks up those cases where there was an element of uncertainty on the match criteria. For example, where the author and co author names appear to match with inventors but are very common names. Alternatively, the first criteria might have been met but the affiliation and organisation records did not match. Finally, for review include instances where there is an author to inventor match but only the species names are shared. During the second and potentially multiple other passes, the review group is progressively allocated to either keep or exclude. While the same method can be used with programming languages VantagePoint is designed to facilitate this type of close work and has the advantage that other data fields can be reviewed for matching as the cleaning process proceeds. At the end of the process a total of 290 authors of research on marine genetic resources in South East Asia who are also inventors were identified. As this suggests, developing this type of analysis involves reviewing a large number of records with the expectation of a low number of results. A particular advantage of the use of match criteria is that it limits the high probability of false positive matches if match criteria are not used. As this also suggests mapping researchers from the scientific literature into the patent literature can be very time consuming. This is a particular problem when research is on a large scale such as the level of millions of records. The problem of name disambiguation and corresponding challenges with name cleaning have proved to be a persistent challenge in both the scientific literature and in the patent literature. However, there are signs that the situation may be at least improving even if it is not solved. In the case of the scientific literature the growing us of free ORCID identifiers promises to help improve but not solve the challenge of name disambiguation. At the time of writing over 5 million ORCID identifiers have been issued and an increasing number of funding organisations and publishers are either requiring or requesting an ORCID identifier. Clarivate Analytics has also made the relationship between its long standing Researcher ID system and ORCID seamless. In an innovative move the Lens Patent Database now encourages researcher-inventors to associate their ORCID. In addition, the Lens has linked over 10 million non-patent literature citations to ORCID records so that researchers can see patent literature that cites their research. This suggests that ORCID identifiers may potentially have an important role to play in name disambiguation across the scientific and patent literature if uptake by researchers continues to increase rapidly. In a separate development in 2015 the USPTO hosted an Inventor Disambiguation Workshop to discuss the problem of inventor name disambiguation.5 As a result of a competition held by the USPTO a team from the University of Massachusetts Amherst led by Andrew McCallum and Nicholas Monath developed an algorithm using discriminative hierarchical co-reference or in essence a decision tree model for clustering inventor names based on co references to other data fields in the record (for a detailed description of the approach see Wick, Singh, and McCallum 2012). The outcome of this research was applied both the the USPTO inventor and assignee field and the creation of new tables with links to the original raw tables. While it is not expected to be error free the PatentsView data tables may offer opportunities to more easily establish linkages between data from the scientific literature and patent data, at least for USPTO data because it is freely available and has been pre-processed. The ability to match names between the scientific literature and patent literature at scale remains as a significant and time consuming challenge. However, in the case of ASEAN countries it revealed author-inventors such as Baldomera Olivera from the Philippines who pioneered research on cone snail toxins that would feature on the front page of Science magazine and lead to an approved pharmaceutical. Other researchers identified through this approach included husband and wife team Hu Bow and Ding Jeak Ling from the National University of Singapore who identified a recombinant cDNA factor from the Horseshoe Crab that is now used in endotoxin assays and biosensors. In short, the approach yields detailed evidence of researchers who have successfully licensed inventions that have become products on the market. In practice, the majority of research activity does not result in patent activity. However, combining analysis of the scientific literature with the patent literature can lead to the identification of potential candidates to be taken forward for development and examples of successful licensing of research and inventions that can serve as positive examples for researchers and policy makers elsewhere. We will close this discussion of methods and approaches for linking analysis of scientific and patent literature with a recent development to link literature citations and patent data. 2.7 Linking Citations with Patent Literature An alternative way to think about the relationship between the scientific literature and patent activity is to focus on non-patent literature citations (Callaert, Van Looy, et al. 2006). In a recent development the open access Lens patent database has done extensive work to link document identifiers in the non-patent literature to the Crossref database of metadata on over 96 million publications and to link records with PubMed and Microsoft Academic. The effect is to create a bridge based on identifiers between the scientific and the patent literature. Figure 2.6: Literature Citations Linking to Patent Citations and to external data sources in the Lens Figure 2.6 shows the top ranking literature citation across the data for the well known Basic Local Alignment Search Tool or BLAST that is widely used in fields such as genomics. Each entry links to a summary table include Medical Subject Headings (MeSH) terms where appropriate and a Citations page that will reveal Patent citations and Literature citations. Registered users, registration is free, can store and then export the results. Table 2.5 presents a sample of fields from the top 5 of the 9000 exported results from a search of Lens Scholar for “synthetic biology”. Table 2.5: Lens Scholar Exported Results for Synthetic Biology Title Referenced by Patent Count Publication Year Citation IDs Author/s Accurate multiplex gene synthesis from programmable DNA microchips. 188 2004 (magid) mag2027912527; (doi) 10.1038/nature03151; (pmid) 15616567 Jingdong Tian; Hui Gong; Nijing Sheng; Xiaochuan Zhou; Erdogan Gulari; Xiaolian Gao; George M. Church De novo biosynthetic pathways: rational design of microbial chemical factories. 91 2008 (pmid) 18725289; (magid) mag2092471565; (doi) 10.1016/j.copbio.2008.07.009 Kristala L. J. Prather; Collin H. Martin Harnessing homologous recombination in vitro to generate recombinant DNA via SLIC. 81 2007 (doi) 10.1038/nmeth1010; (pmid) 17293868; (magid) mag2102440675 Mamie Z. Li; Stephen J. Elledge Microfluidic PicoArray synthesis of oligodeoxynucleotides and simultaneous assembling of multiple DNA sequences 65 2004 (pmcid) pmc524290; (pmid) 15477391; (magid) mag2146545072; (doi) 10.1093/nar/gkh879 Xiaochuan Zhou; Shi-Ying Cai; Ailing Hong; Qimin You; Peilin Yu; Nijing Sheng; Onnop Srivannavit; Seema Muranjan; Jean Marie Rouillard; Yongmei Xia; Xiaolin Zhang; Qin Xiang; Renuka Ganesh; Qi Zhu; Anna Matejko; Erdogan Gulari; Xiaolian Gao Gene Designer: a synthetic biology tool for constructing artificial DNA segments 59 2006 (pmcid) pmc1523223; (pmid) 16756672; (magid) mag1760665500; (doi) 10.1186/1471-2105-7-285 Alan Villalobos; Jon E. Ness; Claes Gustafsson; Jeremy Minshull; Sridhar Govindarajan As this suggests, growing trends towards the federation of the scientific and patent literature present important opportunities for designing search strategies and more flexible approaches to the analysis and communication of results building on both the scientific and the patent literature. 2.8 Conclusion This chapter has focus on methods for working with data from the scientific literature, using analysis of the scientific literature to build up a search strategy, the development of a scientific landscape study and methods for linking scientific literature to patent analytics. It is important to emphasise that a major transition is taking place in the availability of the scientific literature and our ability to link scientific literature to patent data. What may be described as traditional databases that focus on closed subscription based access with severe limitations on data downloads are being replaced by large open access databases such as Microsoft Academic Graph, Crossref, PubMed, core.ac.uk and the Lens. These databases typically offer access to data at scale for free or use freemium subscription models for higher levels of access or the use of APIs while maintaining a generous free tier. How these business models will work out over the longer term remains to be seen. However, in practice the major constraint in accessing scientific publications, notably meta data about publications, is no longer the cost and download limitations placed on users by commercial providers. Rather, the main constraint is the capacity to manage and process data at the level of hundreds of millions of records. In many cases patent analysts will not need to process hundreds of millions of records. Rather, what will be of interest is identifying the relationship between scientific literature and patent data through the exploration of citations. The Lens database has pioneered the integration of citations between the scientific literature and patent data and developments in the open source community such as the PATCIT will further open up the linkages between patent activity and underlying research activity. A key feature of these emerging developments is the increasing application of machine learning approaches to the scientific and patent literature. We turn to these developments in Chapter 8. References "],["geocoding.html", "Chapter 3 Geocoding 3.1 Getting Started 3.2 Getting set up with the Google Maps API 3.3 Using the API 3.4 The Source Data 3.5 Lookup the Records 3.6 Using placement 3.7 Using ggmap 3.8 Using Googleway 3.9 Reviewing Initial Results 3.10 Tackling Abbreviations 3.11 Lookup edited names 3.12 Bringing the data together 3.13 Assessing the Quality of Geocoding 3.14 Preprocess the Data and Rerun the Query 3.15 Duplicated Affiliation Names 3.16 Mapping the Data 3.17 Round Up", " Chapter 3 Geocoding In this Chapter we will explore geocoding, Geocoding is the process of turning location names such as organisation names or addresses into georeferences in the form of latitude and longitude coordinates for representation on a map 6. Geocoding is a relatively recent and popular way to map activity in geographic space, such as research organisations, patent applicants and patent inventors. Basic geocoding such as clustering records by country can be performed using country codes or country names in free tools such as Tableau Public. This chapter addresses a more advanced form of geocoding using web services, and specifically the Google Maps API to geocode thousands of organisation names from Web of Science data. The Google Maps API can be accessed in a range of programming languages such as Python or R. We will focus on illustrating the issues involved in geocoding using the placement, ggmap, and googleway packages in R. We will work with some raw data from Clarivate Analytics Web of Science database of scientific literature. Many universities have access to Web of Science and it is an important tool in fields such as bibliometrics/scientometrics. Geocoding is the process of taking a name and address and looking up the geographic coordinates expressed in latitude and longitude. This is normally done using a web service. There are plenty of example walkthroughs on how to do this. However, many of them start with data that is already clean. We will be working with data that is really rather messy. What we are attempting to do is to obtain the addresses and coordinates from the author affiliations field in Web of Science records. Our dataset is from a set of queries for scientific literature for South East Asia (ASEAN) countries that involve marine organisms. We have a table with 5,206 author affiliation details containing the names of organisations, the city and the country. This data is not clean and contains multiple minor variations of the same organisation name. The data also contains variations in geographic locations such as references to a district within a city rather than the name of the city itself. To follow the walk through you can download the data from Github here. It simply contains the author affiliation name and a count of the number of records. One of the issues with Web of Science data is that the names of organisations are abbreviated/stemmed (so that University becomes Univ, Institute becomes Inst and so on and so on). Until recently this made geocoding a significant headache. However, as we will see below the Google Maps API now seems to do a very good job of handling these issues but considerable care is needed when interpreting the results. In this Chapter we will go step by step through the process of geocoding and deal with the issues we encounter along the way. At the end of the article we will pull the code together to identify a more efficient way to deal with geocoding Web of Science and similar data. By the end of this Chapter you will be familiar with what geocoding is and how to carry out geocoding using the placement, ggmap and googleway packages in R with RStudio. You will also be familiar with the Google Maps API and be able to identify and retrieve missing data using packages from the tidyverse. We will take what we learned and combine it into more efficient code for solving the problem and finish off with a quick map of the results. 3.1 Getting Started If you are new to R and RStudio then first we need to get set up. To install R for your operating system choose the appropriate option here and install R. Then download the free RStudio desktop for your system here. We will be using a suite of packages called the tidyverse that make it easy to work with data. When you have installed and opened RStudio run these lines in your console to install the packages that we will be using. install.packages(&quot;tidyverse&quot;) install.packages(&quot;devtools&quot;) install.packages(&quot;usethis&quot;) install.packages(&quot;googleway&quot;) The placement package is no longer available on CRAN and needs to be installed from Github as follows. library(devtools) ## Loading required package: usethis install_github(&quot;DerekYves/placement&quot;) ## Skipping install of &#39;placement&#39; from a github remote, the SHA1 (95e08cec) has not changed since last install. ## Use `force = TRUE` to force installation For ggmap we will load the latest version 2.7 that includes register_google() for authentication and install it from Github as follows. devtools::install_github(&quot;dkahle/ggmap&quot;) Next load the libraries. library(tidyverse) library(ggmap) library(placement) library(usethis) library(googleway) You will now see a range of messages as the packages are loaded. You should now be good to go. If you would like to learn more about R then try the DataCamp online courses or read Garrett Grolemund and Hadley Wickham’s R for Data Science. Learning to do things in R will make a huge difference to your ability to work with patent and other data and to enjoy the support of the R community in addressing new challenges. There is never a better time to start learning to do things in R than right now. The placement, ggmap and recent googleway packages all provide functions for geocoding with the Google Maps API. The placement package by Derek Darves was created in 2016 and provides straightforward access to the Google Maps API and additional tools for address cleaning, calculating distances and driving times. As Derek explains here. I found it remarkably easy to use and it does not require any complicated code. The function we will be using is geocode_url() and geocode_pull(). That is basically it. While placement mainly focuses on geocoding, ggmap is a bigger package for mapping in R that includes geocoding. The package is a complement to ggplot2 and a Data Camp course by Charlotte Wickham Working with Geospatial Data in R will get you started in no time with ggmap and other mapping packages. As we will see below, I ran in to some tricky issues when trying to geocode with ggmap and you may also want to give googleway a try. We will mainly use the placement package because I like the simplicity of the package, but which you use will depend on your purpose and you will probably want to experiment with the wider functionality of ggmap or the more recent googleway. 3.2 Getting set up with the Google Maps API To use the Google Maps API you will need to: Sign in to a Google account Get a free API key from here. This involves pressing the Get a Key button and creating a project (app) that you will query by following these steps. Create a new project and wait a short while while Google spins it up. You will then see your API key. Note that you will see a link to restrict access to your API. It is a good idea to follow this and use your IP address to limit access to your IP address under Application restrictions. This will prevent other people from using the account if they discover the API key. We will not go down that route right now. Take a copy of your API key (say into a text file in R Studio). What you do next is up to you. Save the text file somewhere sensible and copy it into the functions below when needed. With usethis either: usethis::edit_r_environ() to open your local environment file and enter something like GOOGLE_MAPS_KEY=“yourkey” and then restart R. You will be able to access the key using Sys.getenv(\"GOOGLE_MAPS_KEY\"). usethis::edit_r_profile() and enter google_maps_key=“your key”, inside the existing options() chunk, save and restart R. Call the key with getOption(\"google_maps_key\") For discussion on the above try reading the R startup section of Efficient R Programming or follow the very useful ROpenSci instructions. usethis makes life much easier because it knows where the files are! We will go with the usethis::edit_r_environ() environment option, so let’s store the key in our working environment for the moment using the imaginatively named key. key &lt;- Sys.getenv(&quot;GOOGLE_MAPS_KEY&quot;) 3.3 Using the API Note that API queries are limited to a free 2500 per day. It costs 50 cents per 1000 queries after that. As this is not expensive we signed up for a billing account to run the full list. As we will see below signing up for an API key is a good idea to avoid problems with the return resulting from pressure on the free service. When you sign up for the API key you still get the 2500 results but make sure you put your API key somewhere safe and do not make it public. Below we will briefly show how to use the placement, ggmap and newer googleway packages to retrieve geocode data. Unfortunately the return from the Google API with placement also includes a column called input_url. I say unfortunate because the input_url includes your private API key! So, if you are planning to make any of this data public you should exclude the input_url column. 3.4 The Source Data Next let’s take a quick look at the source data. When we send the addresses to the Google Maps API with placement it will return the original search terms in a column called locations. To make our life easier we renamed the original column in our source dataset. Note that the records field refers to the number of publications associated with an address and will allow us to size dots on any map we produce with the results. We can import the data directly from Github. affiliation_records &lt;- read_csv(&quot;https://github.com/wipo-analytics/data-handbook/raw/master/affiliation_records.csv&quot;) head(affiliation_records) ## # A tibble: 6 × 3 ## records locations id ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 1 AAHL, Vic, Australia 1 ## 2 1 AAHRI, Bangkok, Thailand 2 ## 3 1 Aarhus Univ Biosci, Roskilde, Denmark 3 ## 4 1 Aarhus Univ Hosp, Aarhus, Denmark 4 ## 5 13 Aarhus Univ, Aarhus C, Denmark 5 ## 6 3 Aarhus Univ, Aarhus, Denmark 6 3.5 Lookup the Records In this section we will look up some of the records with each of the three packages to show how easy it is. Purely from personal preference we will use placement for the rest of the work. Note that placement is no longer being actively maintained and the examples below are used for illustration. We now recommend switching to ggmap or googleway depending on your needs. 3.6 Using placement The placement package can do more than we will attempt here. For example, you can attempt address cleaning or calculating driving distances with placement. For our purposes the main event is the geocode_url() function, We pass the data in the locations column to the function along with the authentication route and the private key. The clean = TRUE argument applies the address_cleaner function before encoding the URL to send to the API. The default is set to TRUE and you may want to experiment with setting this value to FALSE. We also add the date of search as it is always useful to know when we carried out the search and we set verbose to TRUE to receive more information. Note that other arguments such as dryrun can be useful for debugging problem addresses. Note that the key can be entered directly into geocode_url() as privkey = Sys.getenv(\"GOOGLE_MAPS_KEY\"). However, I found that this sometimes returned an error message on long runs. For that reason we might copy it into our local environment (and be careful not to expose it). key &lt;- Sys.getenv(&quot;GOOGLE_MAPS_KEY&quot;) library(placement) coordaffil &lt;- geocode_url(affiliation_records$locations, auth = &quot;standard_api&quot;, privkey = key, clean = TRUE, add_date = &#39;today&#39;, verbose = TRUE) 3.7 Using ggmap We can perform the same lookup using ggmap and the geocode() function. Note that the function defaults to the free allocation of 2500 queries. There are options to return “latlon” and “latlona”” or “more” or “all”. In the case of “all” this returns a list with entries of differing lengths that you will need to wrangle. In general use latlon, latlona or more as this will return a data frame. Here we will just test 100 records. geocode() does not return the input URL with our private key (which is good). library(ggmap) coord_ggmap &lt;- geocode(location = affiliation_records$locations[1:100], output = &quot;more&quot;, source = &quot;google&quot;, messaging = FALSE) When using ggmap I encountered a significant number of OVER_QUERY_LIMIT entries in the return. Why is something of a mystery although as discussed here this may because we are sharing the call to the free service with others. It is therefore better to get a key if you are going to be using this service. To authenticate using ggmap (2.7 only) create a key based on the key in your environment file. Pass it to register_google() and then you are ready to make the call. key &lt;- Sys.getenv(&quot;GOOGLE_MAPS_KEY&quot;) register_google(key = key) It will now work smoothly. library(ggmap) ggmap1 &lt;- geocode(location = affiliation_records$locations[201:300], output = &quot;more&quot;, source = &quot;google&quot;, messaging = FALSE) This overcame the limitation and returned a data.frame with 100 entries. ggmap1 %&gt;% select(1:4) %&gt;% head() ## lon lat type loctype ## 1 -93.631913 42.03078 locality approximate ## 2 3.707011 51.05376 establishment rooftop ## 3 -1.386919 50.90853 establishment geometric_center ## 4 142.384141 43.72986 establishment rooftop ## 5 142.384141 43.72986 establishment rooftop ## 6 127.680932 26.21240 administrative_area_level_1 approximate 3.8 Using Googleway An alternative to placement or ggmap is also available using the googleway package. googleway includes access to the Google APIs for directions, distance, elevation, time zones, places, geocoding and reverse geocoding and so has a wider set of uses. However, googleway is expecting an address field of length 1 (meaning it takes one address at a time) whereas placement and ggmap are vectorised. The return from googleway returns a list object containing a data frame with the results and the status of the return. Here is one quick example. library(googleway) googleway &lt;- google_geocode(address = &quot;Aarhus Univ Biosci, Roskilde, Denmark&quot;, key = Sys.getenv(&quot;GOOGLE_MAPS_KEY&quot;), simplify = TRUE) For long lists we would therefore need to use an approach such as lapply() or purrr::map() to make the call as a set and then look at ways to bind the results together. googleway2 &lt;- purrr::map(affiliation_records$locations[1:2], google_geocode, key = Sys.getenv(&quot;GOOGLE_MAPS_KEY&quot;), simplify = TRUE) As this makes clear, you have at least three choices for geocoding and which you prefer will depend on your needs. I found ggmap rather awkward because the existing CRAN version (2.6) does not provide the register_google() function in the long standing 2.7 development version. While this is a bit awkward ggmap provides some very powerful features that you will want to use. On the other hand googleway would involve some more work to vectorise over the list as we started exploring above. placement on the other hand is fine with the only disadvantage being the return of the API key in the input URL that we have to remember. 3.9 Reviewing Initial Results When we originally started working with the Google API in 2017 the API returned 3,937 results from the 5,206 names. This then required a lot of additional work to retrieve the remaining numbers by cleaning up abbreviations and country names. However, the Google Maps API seems to have improved rather radically in the meantime. Let’s take a look at the issues that can arise with the return from the Google Maps API. For the moment we will focus on the completeness of the data revealed in status and error messages. coordaffil %&gt;% select(location_type, status, error_message) %&gt;% head() ## location_type status error_message ## 1 ROOFTOP OK ## 2 GEOMETRIC_CENTER OK ## 3 ROOFTOP OK ## 4 GEOMETRIC_CENTER OK ## 5 ROOFTOP OK ## 6 ROOFTOP OK The return from placement is a data.frame that is exactly the same length as our input. What we need to watch out for are the entries in the status column and the error message column. Here we need to be cautious because most of the time the API returns either “OK” or “ZERO_RESULTS”. However, there are additional status codes listed here and they are also listed in the documentation for geocode_url(). They are: “OK” “ZERO_RESULTS” “OVER_QUERY_LIMIT” “REQUEST_DENIED” “INVALID_REQUEST” “UNKNOWN_ERROR” “CONNECTION_ERROR” (added) When running a long set of addresses the CONNECTION_ERROR can creep into the data, so be aware of this. We can now join our data sets together. We will use left_join() for convenience and specify the column to join on as the shared locations column. results &lt;- dplyr::left_join(affiliation_records, coordaffil, by = &quot;locations&quot;) We can identify the results found so far by filtering on the status field which will show “OK” where there is a return and “ZERO_RESULTS” where the geocoding did not work: results %&gt;% filter(., status == &quot;OK&quot;) ## # A tibble: 5,187 × 10 ## records locati…¹ id lat lng locat…² forma…³ status error…⁴ geocode_dt ## &lt;int&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 1 AAHL, V… 1 -38.2 144. ROOFTOP 5 Port… OK &quot;&quot; 2018-05-22 ## 2 1 AAHRI, … 2 13.8 101. GEOMET… 50, กร… OK &quot;&quot; 2018-05-22 ## 3 1 Aarhus … 3 56.2 10.2 ROOFTOP Nordre… OK &quot;&quot; 2018-05-22 ## 4 1 Aarhus … 4 56.2 10.2 GEOMET… Nørreb… OK &quot;&quot; 2018-05-22 ## 5 13 Aarhus … 5 56.2 10.2 ROOFTOP Nordre… OK &quot;&quot; 2018-05-22 ## 6 3 Aarhus … 6 56.2 10.2 ROOFTOP Nordre… OK &quot;&quot; 2018-05-22 ## 7 1 Abasyn … 7 34.0 71.6 GEOMET… Ring R… OK &quot;&quot; 2018-05-22 ## 8 1 Abdul W… 8 34.2 72.0 GEOMET… Nowshe… OK &quot;&quot; 2018-05-22 ## 9 1 Abertay… 9 56.5 -2.97 GEOMET… Bell S… OK &quot;&quot; 2018-05-22 ## 10 1 Aberyst… 10 52.4 -4.07 GEOMET… Pengla… OK &quot;&quot; 2018-05-22 ## # … with 5,177 more rows, and abbreviated variable names ¹​locations, ## # ²​location_type, ³​formatted_address, ⁴​error_message ## # ℹ Use `print(n = ...)` to see more rows For the results that were not found it is safest not to simply filter for ZERO RESULTS but instead to filter for anything that is not OK using !=. This can save on endless hours of confusion where you have multiple messages in the status column. lookup &lt;- results %&gt;% filter(., status != &quot;OK&quot;) nrow(lookup) ## [1] 19 So, we have 19 records with no results. That is pretty good from just over 5000 results. lookup %&gt;% select(-id) ## # A tibble: 19 × 9 ## records locations lat lng locat…¹ forma…² status error…³ geocode_dt ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; ## 1 2 Aomori Prefect… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 2 1 FOOD CROPS RES… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 3 2 Hunan Agr Univ… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 4 1 Hunan Fisherie… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 5 1 Hunan Univ Chi… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 6 2 Indonesian Ins… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 7 1 Inst Oceanog V… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 8 1 Inst Oceanog, … NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 9 6 Inst Oceanog, … NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 10 2 ISME, Okinawa,… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 11 1 Kitasato Univ,… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 12 1 Main Off Educ … NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 13 1 Nha Trang Inst… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 14 1 Okinawa Prefec… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 15 1 Ryukoku Univ, … NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 16 1 UNIV WESTMINST… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 17 6 Vietnam Acad S… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 18 1 VNIO, Nha Tran… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## 19 1 Xi Consultancy… NA NA &lt;NA&gt; &lt;NA&gt; ZERO_… &quot;&quot; 2018-05-22 ## # … with abbreviated variable names ¹​location_type, ²​formatted_address, ## # ³​error_message When dealing with thousands of records it is often a good idea to add a cut off threshold. For example we can see above that with two exceptions the entries are all for 1 or 2 records. As these will be barely visible on a map you may want to set a cut off point to focus in on the more important records. However, the lookup table highlights an issue that the Google Maps API previously struggled to deal with: abbreviations. When working with scientific literature abbreviations in author affiliations along with acronyms are common. So, lets look at how to deal with that. 3.10 Tackling Abbreviations Here we have created a simple file containing some of the major Web of Science organisation abbreviations and their matches. It is probably not complete but is a good start. Next we added a column with word boundaries that we will use to find and replace the abbreviations. You can download the the file directly from Github. wos_abbreviations &lt;- read_csv(&quot;https://github.com/wipo-analytics/data-handbook/raw/master/wos_abbreviations.csv&quot;, col_types = cols(abbreviation = col_character(), text = col_character())) A simple word boundary regular expression was added to assist with matching. wos_abbreviations$regex &lt;- paste0(&quot;\\\\b&quot;, wos_abbreviations$abbreviation, &quot;\\\\b&quot;) ## # A tibble: 6 × 3 ## abbreviation text regex ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Univ University &quot;\\\\bUniv\\\\b&quot; ## 2 Natl National &quot;\\\\bNatl\\\\b&quot; ## 3 Inst Institute &quot;\\\\bInst\\\\b&quot; ## 4 Sci Science &quot;\\\\bSci\\\\b&quot; ## 5 Ctr Centre &quot;\\\\bCtr\\\\b&quot; ## 6 Res Research &quot;\\\\bRes\\\\b&quot; To replace the abbreviations we will want to temporarily separate out the city and the country names in the locations column. This helps us to avoid transforming them by accident. We will bring the edited version back together later. Web of Science data uses a comma to separate out the entities and so we use that in a call to separate. We also keep the original column by specifying remove = FALSE as the default removes the input column. lookup &lt;- lookup %&gt;% separate(., locations, c(&quot;organisation&quot;, &quot;city&quot;, &quot;country&quot;), sep = &quot;,&quot;, remove = FALSE) ## # A tibble: 6 × 6 ## records locations organ…¹ city country id ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 2 Aomori Prefectural Agr &amp; Forestry Res Ctr… Aomori… &quot; Ao… &quot; Japa… 161 ## 2 1 FOOD CROPS RES INST, HAI HUNG, VIETNAM FOOD C… &quot; HA… &quot; VIET… 1215 ## 3 2 Hunan Agr Univ, Hunan, Peoples R China Hunan … &quot; Hu… &quot; Peop… 1521 ## 4 1 Hunan Fisheries Sci Inst, Hunan, Peoples … Hunan … &quot; Hu… &quot; Peop… 1522 ## 5 1 Hunan Univ Chinese Med, Hunan, Peoples R … Hunan … &quot; Hu… &quot; Peop… 1523 ## 6 2 Indonesian Inst Sci, Ambon, Indonesia Indone… &quot; Am… &quot; Indo… 1597 ## # … with abbreviated variable name ¹​organisation Next we want to iterate over the list of our organisation names and replace the abbreviations. There are a variety of ways to do that such as the qdap package function multigsub() or mgsub(). We like qdap a lot but installation of the package can be a bit awkward due to a dependency on rJava.7 Instead we are going to use a simple for loop (although a purrr solution would be an improvement). replaceabbr &lt;- function(pattern, replacement, var) { replacement &lt;- rep(replacement, length(pattern)) for (i in seq_along(pattern)) { var &lt;- gsub(pattern[i], replacement[i], var) } var } One issue with cleaning names is capitalisation. For example, in our WOS abbreviations file we have used Univ as the most common abbreviation for University. However, this will not match UNIV and so we will be better off regularising the text. A common convention is to convert everything to lower case using tolower() at the start of working with the data. Here we don’t want to do that. We will use the extremely useful stringr package to convert the organisation name to to title case in a new field that we will call organisation_edited. The reason that we are not editing our original column is that at some point we will want to join the table back on to our original dataset…so we don’t want to touch our original column. We will do this using mutate() from dplyr(). lookup &lt;- lookup %&gt;% mutate(organisation_edited = str_to_title(.$organisation)) lookup %&gt;% select(organisation_edited) ## # A tibble: 19 × 1 ## organisation_edited ## &lt;chr&gt; ## 1 Aomori Prefectural Agr &amp; Forestry Res Ctr ## 2 Food Crops Res Inst ## 3 Hunan Agr Univ ## 4 Hunan Fisheries Sci Inst ## 5 Hunan Univ Chinese Med ## 6 Indonesian Inst Sci ## 7 Inst Oceanog Vast ## 8 Inst Oceanog ## 9 Inst Oceanog ## 10 Isme ## 11 Kitasato Univ ## 12 Main Off Educ &amp; Teaching Area ## 13 Nha Trang Inst Oceanog ## 14 Okinawa Prefectural Fisheries &amp; Ocean Res Ctr ## 15 Ryukoku Univ ## 16 Univ Westminster ## 17 Vietnam Acad Sci &amp; Technol ## 18 Vnio ## 19 Xi Consultancy Next, we transform the abbreviations using replaceabbr. lookup$organisation_edited &lt;- replaceabbr(wos_abbreviations$regex, wos_abbreviations$text, lookup$organisation_edited) lookup %&gt;% select(organisation_edited) ## # A tibble: 19 × 1 ## organisation_edited ## &lt;chr&gt; ## 1 Aomori Prefectural Agriculture &amp; Forestry Research Centre ## 2 Food Crops Research Institute ## 3 Hunan Agriculture University ## 4 Hunan Fisheries Science Institute ## 5 Hunan University Chinese Medical ## 6 Indonesian Institute Science ## 7 Institute Oceanography Vast ## 8 Institute Oceanography ## 9 Institute Oceanography ## 10 Isme ## 11 Kitasato University ## 12 Main Office Education &amp; Teaching Area ## 13 Nha Trang Institute Oceanography ## 14 Okinawa Prefectural Fisheries &amp; Ocean Research Centre ## 15 Ryukoku University ## 16 University Westminster ## 17 Vietnam Academy Science &amp; Technology ## 18 Vnio ## 19 Xi Consultancy This is not perfect, for example we encounter issues with Agriculture and Agricultural and so on. We also encounter issues of capitalisation in the city and the country field that we are presently ignoring. However, it is good enough for the time being. Rather than focus on resolving a small number of remaining items the next step is to reunite the fields we separated into a field we will call locations edited using the tidyr unite function. lookup &lt;- lookup %&gt;% unite(., locations_edited, c(organisation_edited, city, country), sep = &quot;,&quot;, remove = FALSE) lookup %&gt;% select(organisation, city, country, locations_edited) ## # A tibble: 19 × 4 ## organisation city country locat…¹ ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Aomori Prefectural Agr &amp; Forestry Res Ctr &quot; Aomori&quot; &quot; Japa… Aomori… ## 2 FOOD CROPS RES INST &quot; HAI HUNG&quot; &quot; VIET… Food C… ## 3 Hunan Agr Univ &quot; Hunan&quot; &quot; Peop… Hunan … ## 4 Hunan Fisheries Sci Inst &quot; Hunan&quot; &quot; Peop… Hunan … ## 5 Hunan Univ Chinese Med &quot; Hunan&quot; &quot; Peop… Hunan … ## 6 Indonesian Inst Sci &quot; Ambon&quot; &quot; Indo… Indone… ## 7 Inst Oceanog VAST &quot; Nha Trang&quot; &quot; Viet… Instit… ## 8 Inst Oceanog &quot; Nha Trang Ci… &quot; Viet… Instit… ## 9 Inst Oceanog &quot; Nha Trang&quot; &quot; Viet… Instit… ## 10 ISME &quot; Okinawa&quot; &quot; Japa… Isme, … ## 11 Kitasato Univ &quot; Aomori&quot; &quot; Japa… Kitasa… ## 12 Main Off Educ &amp; Teaching Area &quot; Tehran&quot; &quot; Iran&quot; Main O… ## 13 Nha Trang Inst Oceanog &quot; Khanh Hoa Pr… &quot; Viet… Nha Tr… ## 14 Okinawa Prefectural Fisheries &amp; Ocean Res Ctr &quot; Okinawa&quot; &quot; Japa… Okinaw… ## 15 Ryukoku Univ &quot; Okinawa&quot; &quot; Japa… Ryukok… ## 16 UNIV WESTMINSTER &quot; LONDON W1M 8… &quot; ENGL… Univer… ## 17 Vietnam Acad Sci &amp; Technol &quot; Nha Trang&quot; &quot; Viet… Vietna… ## 18 VNIO &quot; Nha Trang&quot; &quot; Viet… Vnio, … ## 19 Xi Consultancy &quot; Delft&quot; &quot; Neth… Xi Con… ## # … with abbreviated variable name ¹​locations_edited Note that rather than creating a separate character vector we made life easier by simply adding locations_edited to our lookup data.frame (because the vectors are of the same length) using unite(). 3.11 Lookup edited names We now send the cleaned up version off to the Google API. library(placement) coordlookup &lt;- geocode_url(lookup$locations_edited, auth = &quot;standard_api&quot;, privkey = key, clean = TRUE, add_date = &#39;today&#39;, verbose = TRUE) Let’s take a look. coordlookup %&gt;% select(locations, status) ## locations ## 1 Aomori Prefectural Agriculture &amp; Forestry Research Centre, Aomori, Japan ## 2 Food Crops Research Institute, HAI HUNG, VIETNAM ## 3 Hunan Agriculture University, Hunan, Peoples R China ## 4 Hunan Fisheries Science Institute, Hunan, Peoples R China ## 5 Hunan University Chinese Medical, Hunan, Peoples R China ## 6 Indonesian Institute Science, Ambon, Indonesia ## 7 Institute Oceanography Vast, Nha Trang, Vietnam ## 8 Institute Oceanography, Nha Trang City, Vietnam ## 9 Institute Oceanography, Nha Trang, Vietnam ## 10 Isme, Okinawa, Japan ## 11 Kitasato University, Aomori, Japan ## 12 Main Office Education &amp; Teaching Area, Tehran, Iran ## 13 Nha Trang Institute Oceanography, Khanh Hoa Prov, Vietnam ## 14 Okinawa Prefectural Fisheries &amp; Ocean Research Centre, Okinawa, Japan ## 15 Ryukoku University, Okinawa, Japan ## 16 University Westminster, LONDON W1M 8JS, ENGLAND ## 17 Vietnam Academy Science &amp; Technology, Nha Trang, Vietnam ## 18 Vnio, Nha Trang, Vietnam ## 19 Xi Consultancy, Delft, Netherlands ## status ## 1 ZERO_RESULTS ## 2 ZERO_RESULTS ## 3 OK ## 4 OK ## 5 OK ## 6 OK ## 7 ZERO_RESULTS ## 8 OK ## 9 OK ## 10 ZERO_RESULTS ## 11 ZERO_RESULTS ## 12 OK ## 13 ZERO_RESULTS ## 14 OK ## 15 OK ## 16 OK ## 17 OK ## 18 ZERO_RESULTS ## 19 ZERO_RESULTS So, 8 of our revised names have failed to produce a return. In some cases this is a little surprising. For example the private Kitasato University would be expected to come up, but the reference to Aomori seems to have confused the mapper (as the University is listed as located in Minato). In the case of the Institute Oceanography Vast we can see that there is duplication (Vast refers to the Vietnam Academy of Science and Technology as the parent organisation of the institute) with the second and third entries being recognised. Other variants such as Nha Trang Institute Oceanography, Khanh Hoa Prov, Vietnam and the acronym Vnio, Nha Trang, Vietnam are also missed. How far you want to push with fixing addresses is up to you and will depend on your purposes. As mentioned above, to avoid a long tail of unresolved addresses for low frequency data you may want to use a cut off on the number of records. 3.12 Bringing the data together To join the data back together we need to do some tidying up on the lookup and coordlookup table first. Recall that we sent edited names to Google and those were returned as locations. This means that they will not match with the names in our original table. We also created some additional columns. To create tables that will match the original table we need to tidy up by: selecting the original columns in lookup plus locations_edited (our join field) renaming locations to locations_edited in the lookup results (the join field) join the tables drop the locations-edited column lookup &lt;- lookup %&gt;% select(records, locations, locations_edited, id) coordlookup &lt;- coordlookup %&gt;% rename(locations_edited = locations) res &lt;- left_join(lookup, coordlookup, by = &quot;locations_edited&quot;) %&gt;% select(-locations_edited) To join the data back together we now need to do two things. First we filter the results from the original search to those that are status == \"OK\" and then bind the res table to the end. results_complete &lt;- results %&gt;% filter(., status == &quot;OK&quot;) %&gt;% bind_rows(., res) We will write the results to an Excel and csv file that we can use in other programmes such as Tableau for mapping (we will briefly look at mapping with R below). writexl::write_xlsx(results_complete, path = &quot;asean_geocode_complete.xlsx&quot;) write_csv(results_complete, path = &quot;asean_geocode_complete.csv&quot;) We now have a complete set of geocoded results with 5,198 locations from 5,206. That is pretty good. However, having obtained the geocoded data and joined it onto our original data.frame we now need to look at the quality of the return. 3.13 Assessing the Quality of Geocoding So far we have focused on getting geocoded data without really looking at it. To assess the quality of the data that has been returned we should take a look at the location type field. The API documentation for these entries can be found here and in the geocode_url() documentation. results_complete %&gt;% drop_na(location_type) %&gt;% count(location_type, sort = TRUE) %&gt;% mutate(prop = prop.table(n)) ## # A tibble: 3 × 3 ## location_type n prop ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; ## 1 ROOFTOP 2155 0.415 ## 2 GEOMETRIC_CENTER 1848 0.356 ## 3 APPROXIMATE 1195 0.230 The API documentation fills us in on what is going on here. “location_type stores additional data about the specified location. The following values are currently supported: “ROOFTOP” indicates that the returned result is a precise geocode for which we have location information accurate down to street address precision. “RANGE_INTERPOLATED” indicates that the returned result reflects an approximation (usually on a road) interpolated between two precise points (such as intersections). Interpolated results are generally returned when rooftop geocodes are unavailable for a street address. “GEOMETRIC_CENTER” indicates that the returned result is the geometric center of a result such as a polyline (for example, a street) or polygon (region). “APPROXIMATE” indicates that the returned result is approximate.” What this tells us is that Google believes it has reached rooftop accuracy for 2155 records but has selected the geometric centre or an approximate value for around 58% of the entries. Lets take a closer look at the geometric center data. results_complete %&gt;% filter(location_type == &quot;GEOMETRIC_CENTER&quot;) %&gt;% select(locations, lat, lng, formatted_address) ## # A tibble: 1,848 × 4 ## locations lat lng formatted_address ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 AAHRI, Bangkok, Thailand 13.8 101. 50, กรมประมง, ถนนพหลโยธิ… ## 2 Aarhus Univ Hosp, Aarhus, Denmark 56.2 10.2 Nørrebrogade, 8000 Aarh… ## 3 Abasyn Univ, Peshawar, Pakistan 34.0 71.6 Ring Road, Charsadda Li… ## 4 Abdul Wali Khan Univ, Mardan, Pakistan 34.2 72.0 Nowshera Mardan Rd, Mus… ## 5 Abertay Univ, Dundee DD1 1HG, Scotland 56.5 -2.97 Bell St, Dundee DD1 1HG… ## 6 Aberystwyth Univ, Ceredigion, Wales 52.4 -4.07 Penglais Campus, Pengla… ## 7 Aberystwyth Univ, Dyfed, Wales 52.4 -4.07 Penglais Campus, Pengla… ## 8 ABRII, Karaj, Iran 35.8 51.0 Karaj, Alborz Province,… ## 9 Absyn Univ Peshawar, Peshawar, Pakistan 34.0 71.6 Ring Road, Charsadda Li… ## 10 Acad Ciencias Cuba, C Habana, Cuba 23.1 -82.4 Havana, Cuba ## # … with 1,838 more rows ## # ℹ Use `print(n = ...)` to see more rows A review of these results suggests that the geometric center data is pretty good. In the past we might have ended up in a different country. But what about the approximate results? results_complete %&gt;% filter(location_type == &quot;APPROXIMATE&quot;) %&gt;% select(locations, lat, lng, formatted_address) ## # A tibble: 1,195 × 4 ## locations lat lng forma…¹ ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 Acad Sci Czech Republic, Brno, Czech Republic 49.2 16.6 Brno, … ## 2 Acad Sci Czech Republic, Ceske Budejovice, Czech Repu… 49.0 14.5 Ceske … ## 3 Acad Sinica, Beijing, Peoples R China 39.9 116. Beijin… ## 4 Achva Acad Coll, Mobile Post Shikmim, Israel 31.7 34.6 Shikmi… ## 5 ADAS UK Ltd, Cambs, England 52.2 0.122 Cambri… ## 6 Adv Choice Econ Pty Ltd, Bateman, Australia -25.3 134. Austra… ## 7 AFRIMS Entomol Lab, Kamphaeng Phet, Thailand 16.5 99.5 Kampha… ## 8 Agcy Consultat &amp; Res Oceanog, La Roche Canillac, Fran… 45.2 1.97 19320 … ## 9 Agcy Marine &amp; Fisheries Res Indonesia, Jakarta, Indon… -6.18 107. Jakart… ## 10 Agcy Marine &amp; Fisheries Res, Jakarta, Indonesia -6.18 107. Jakart… ## # … with 1,185 more rows, and abbreviated variable name ¹​formatted_address ## # ℹ Use `print(n = ...)` to see more rows The approximate results are a mixed bag, in some cases the coordinates focus on a city or town. In other cases such as Adv Choice Econ Pty Ltd, Bateman, Australia the coordinate is for a country and so on. 3.14 Preprocess the Data and Rerun the Query This suggests to me at least that while the geocoding is OK the prevalence of geometric centre and approximate results suggests that we might want to run this again but this time edit the location names first to see if we can improve the accuracy of the results. We now know that we can geocode pretty much all of this data. What we are interested in now is whether we can improve the accuracy of the geocoding. # import data and separate out the organisation country and city into new columns affiliation2 &lt;- read_csv(&quot;https://github.com/wipo-analytics/data-handbook/raw/master/affiliation_records.csv&quot;) %&gt;% separate(locations, c(&quot;organisation&quot;, &quot;city&quot;, &quot;country&quot;), sep = &quot;,&quot;, remove = FALSE) # import abbreviations wos_abbreviations &lt;- read_csv(&quot;https://github.com/wipo-analytics/data-handbook/raw/master/wos_abbreviations.csv&quot;, col_types = cols(abbreviation = col_character(), text = col_character())) # function to replace the abbreviations replaceabbr &lt;- function(pattern, replacement, var) { replacement &lt;- rep(replacement, length(pattern)) for (i in seq_along(pattern)) { var &lt;- gsub(pattern[i], replacement[i], var) } var } # regularise organisation names affiliation2 &lt;- affiliation2 %&gt;% mutate(organisation_edited = str_to_title(.$organisation)) %&gt;% mutate(city = str_to_title(.$city)) %&gt;% # added mutate(country = str_to_title(.$country)) #added # fix abbreviations affiliation2$organisation_edited &lt;- replaceabbr(wos_abbreviations$regex, wos_abbreviations$text, affiliation2$organisation_edited) # unite cleaned up fields affiliation2 &lt;- affiliation2 %&gt;% unite(., locations_edited, c(organisation_edited, city, country), sep = &quot;,&quot;, remove = FALSE) # run the search run1 &lt;- placement::geocode_url(affiliation2$locations_edited, auth = &quot;standard_api&quot;, privkey = key, clean = TRUE, add_date = &#39;today&#39;, verbose = TRUE) # drop the input-url and rename for join run1 &lt;- run1 %&gt;% select(-8) %&gt;% rename(locations_edited = locations) # join to the input table res_complete &lt;- left_join(affiliation2, run1, by = &quot;locations_edited&quot;) res_complete &lt;- res_complete %&gt;% mutate(duplicate_id = duplicated(id)) %&gt;% filter(duplicate_id == &quot;FALSE&quot;) When we join the two tables together we discover that we arrive at 5232 rather than 5,206 results. The reason for this is that the name harmonisation has created duplicated names from formerly distinct names. The Google API returns duplicate entries in these cases. These duplicate entries have been filtered out above. We will come on to other forms of duplication below. OK let’s take a look at our results to assess whether this is an improvement. run1 %&gt;% drop_na(location_type) %&gt;% count(location_type, sort = TRUE) %&gt;% mutate(prop = prop.table(n)) ## location_type n prop ## 1 ROOFTOP 2280 0.4394757 ## 2 GEOMETRIC_CENTER 1927 0.3714341 ## 3 APPROXIMATE 981 0.1890902 What this has done is improved the rooftop resolution by a couple of percentage points and improved the geometric centre results by about the same. The approximate score has dropped to 19% from 23% so this is definitely progress. In total 214 records have moved up from the approximate to the rooftop or geometric centre location_types. As this suggests, improving the quality of geocoding matters and it is therefore worth putting the effort into improving the resolution of the results. 3.15 Duplicated Affiliation Names It will not have escaped your attention that in reality our original input data contained a significant amount of duplication on organisation names. This becomes obvious when we review the organisation edited field. We can rapidly see multiple entries. affiliation2 %&gt;% count(organisation_edited, sort = TRUE) ## # A tibble: 4,042 × 2 ## organisation_edited n ## &lt;chr&gt; &lt;int&gt; ## 1 University Putra Malaysia 19 ## 2 Chinese Academy Science 15 ## 3 Mahidol University 15 ## 4 Prince Songkla University 14 ## 5 University Philippines 14 ## 6 Cnrs 12 ## 7 Department Fisheries 12 ## 8 Fisheries Research Agency 12 ## 9 Indonesian Institute Science 12 ## 10 Ministry Health 12 ## # … with 4,032 more rows ## # ℹ Use `print(n = ...)` to see more rows There are a number of reasons for this. In some cases researchers may list different departments or institutes along with the name of their organisation. In other cases an organisation (such as the Chinese Academy of Science or CNRS) may have multiple offices within or outside a particular country. In still other cases, such as Department Fisheries or Ministry Health we are lumping together organisations that share the same name but are distinct entities. Lets take a closer look at this. affiliation2 %&gt;% select(locations, organisation_edited) %&gt;% head(20) ## # A tibble: 20 × 2 ## locations organisation_edited ## &lt;chr&gt; &lt;chr&gt; ## 1 AAHL, Vic, Australia Aahl ## 2 AAHRI, Bangkok, Thailand Aahri ## 3 Aarhus Univ Biosci, Roskilde, Denmark Aarhus University … ## 4 Aarhus Univ Hosp, Aarhus, Denmark Aarhus University … ## 5 Aarhus Univ, Aarhus C, Denmark Aarhus University ## 6 Aarhus Univ, Aarhus, Denmark Aarhus University ## 7 Abasyn Univ, Peshawar, Pakistan Abasyn University ## 8 Abdul Wali Khan Univ, Mardan, Pakistan Abdul Wali Khan Un… ## 9 Abertay Univ, Dundee DD1 1HG, Scotland Abertay University ## 10 Aberystwyth Univ, Ceredigion, Wales Aberystwyth Univer… ## 11 Aberystwyth Univ, Dyfed, Wales Aberystwyth Univer… ## 12 Abo Akad Univ, Turku, Finland Abo Akad University ## 13 ABRII, Karaj, Iran Abrii ## 14 Absyn Univ Peshawar, Peshawar, Pakistan Absyn University P… ## 15 Acad Ciencias Cuba, C Habana, Cuba Academy Ciencias C… ## 16 Acad Nat Sci Philadelphia, Philadelphia, PA USA Academy Natural Sc… ## 17 Acad Sci Czech Republ, Ceske Budejovice, Czech Republic Academy Science Cz… ## 18 Acad Sci Czech Republic, Brno, Czech Republic Academy Science Cz… ## 19 Acad Sci Czech Republic, Ceske Budejovice, Czech Republic Academy Science Cz… ## 20 Acad Sci Czech Republic, Prague, Czech Republic Academy Science Cz… In the case of Aarhus University, we can see that we have Aarhus University Bioscience, Aarhus University Hospital and an Aarhus University. In some cases the entities belong to the organisation but might otherwise be regarded as distinct (Aarhus University Hospital) while in another the Bioscience reference refers to a department (but gives the impression that it may be a separate University as for Agricultural cases). To add to this we note that there are locations in Aarhus and Roskilde and a minor variant (Aarhus C) in the address field. As this makes clear address field data in scientific names is pretty messy because authors choose how to denote their affiliations, and are perhaps rebelling against the tyranny of performance indicators and endless research assessment exercises. Cleaning up author affiliation and author names is generally a painful process. One challenge with name cleaning is the availability of criteria to determine if a name can be merged. For example, we could comfortably merge some of the Aarhus University references above but we might want to keep distinct locations distinct (for example Aarhus is around 150km by road from Roskilde). The availability of georeferenced data, bearing in mind the approximates issue, could provide us with additional information for informed decision making during name cleaning. Let’s take a quick look at the formatted address field in our results. res_complete %&gt;% select(formatted_address, organisation_edited) %&gt;% head(20) ## # A tibble: 20 × 2 ## formatted_address organ…¹ ## &lt;chr&gt; &lt;chr&gt; ## 1 5 Portarlington Road, Newcomb VIC 3219, Australia Aahl ## 2 50, กรมประมง, ถนนพหลโยธิน, ลาดยาว จตุจักร Bangkok 10900, Thailand Aahri ## 3 Aarhus University, 150, Frederiksborgvej 399, 4000 Roskilde, Denmark Aarhus… ## 4 Nørrebrogade, 8000 Aarhus, Denmark Aarhus… ## 5 Langelandsgade 140, 8000 Aarhus, Denmark Aarhus… ## 6 Langelandsgade 140, 8000 Aarhus, Denmark Aarhus… ## 7 Ring Road, Charsadda Link، Near Patang Chowk، Ashrafia Colony, Pesha… Abasyn… ## 8 Nowshera Mardan Rd, Muslimabad, Mardan, Khyber Pakhtunkhwa 23200, Pa… Abdul … ## 9 Bell St, Dundee DD1 1HG, UK Aberta… ## 10 Penglais Campus, Penglais, Aberystwyth SY23 3FL, United Kingdom Aberys… ## 11 Penglais Campus, Penglais, Aberystwyth SY23 3FL, United Kingdom Aberys… ## 12 Domkyrkotorget 3, 20500 Åbo, Finland Abo Ak… ## 13 Karaj, Alborz Province, Iran Abrii ## 14 Ring Road, Charsadda Link، Near Patang Chowk، Ashrafia Colony, Pesha… Absyn … ## 15 Havana, Cuba Academ… ## 16 1900 Benjamin Franklin Pkwy, Philadelphia, PA 19103, USA Academ… ## 17 Branišovská 1645/31A, České Budějovice 2, 370 05 České Budějovice, C… Academ… ## 18 Palackého tř. 1946/1, 612 42 Brno-Královo Pole, Czechia Academ… ## 19 Branišovská 1645/31A, České Budějovice 2, 370 05 České Budějovice, C… Academ… ## 20 Žitná 609/25, 110 00 Praha-Nové Město, Czechia Academ… ## # … with abbreviated variable name ¹​organisation_edited Here we can see that the Google data suggests that some of these entities share an address. Based on this we may want (with appropriate attention to the location type field as a guide) to merge or not merge names in our list. If we take a look at the counts for shared addresses it becomes clear that we may want to use a step wise approach depending on the level of confidence in the location type field. res_complete %&gt;% filter(location_type == &quot;ROOFTOP&quot;) %&gt;% count(formatted_address, sort = TRUE) ## # A tibble: 1,653 × 2 ## formatted_address n ## &lt;chr&gt; &lt;int&gt; ## 1 113 Soi Klong Luang 17, Tambon Khlong Nung, Amphoe Khlong Luang, Chang… 16 ## 2 18 Hoàng Quốc Việt, Nghĩa Đô, Cầu Giấy, Hà Nội, Vietnam 14 ## 3 169 Long Had Bangsaen Rd, Tambon Saen Suk, อำเภอ เมืองชลบุรี Chang Wat Ch… 11 ## 4 15 Karnjanavanit Soi 7 Rd, Kho Hong, Amphoe Hat Yai, Chang Wat Songkhl… 9 ## 5 999 Phutthamonthon Sai 4 Rd, Tambon Salaya, Amphoe Phutthamonthon, Cha… 9 ## 6 Jl. Pasir Putih Raya No.1, RT.8/RW.10, Kota Tua, Pademangan Tim., Pade… 9 ## 7 New Administration Building, Miagao, 5023 Iloilo, Philippines 9 ## 8 02 Nguyễn Đình Chiểu, Vĩnh Thọ, Thành phố Nha Trang, Vĩnh Thọ Thành ph… 8 ## 9 Nørregade 10, 1165 København, Denmark 8 ## 10 Pesthuislaan 7, 2333 BA Leiden, Netherlands 8 ## # … with 1,643 more rows ## # ℹ Use `print(n = ...)` to see more rows 3.16 Mapping the Data To finish off lets quickly map the data. We will focus on mapping in more detail in other articles in the Handbook. For the moment we will use the leaflet package for this. install.packages(&quot;leaflet&quot;) library(leaflet) mapdata &lt;- res_complete %&gt;% filter(., status == &quot;OK&quot;) mapdata &lt;- leaflet(mapdata) %&gt;% addTiles() %&gt;% addCircleMarkers(~lng, ~lat, popup = .$locations_edited, radius = mapdata$records / 20, weight = 0.1, opacity = 0.2, fill= TRUE, fillOpacity = 0.2) mapdata As this makes clear it is relatively straightforward to generate quick maps with R and even easier to export the data to tools such as Tableau for publication quality and interactive maps. We will go into mapping in more depth in a future article. 3.17 Round Up In this article we looked at three R packages for geocoding data on research affiliations from the scientific literature using Web of Science. We focused on the use of the placement package as it is very easy to use. However, your needs may differ with packages such as ggmap and googleway offering different functionality. Packages may also come and go and at the time of writing placement is now archived with ggmap as the obvious replacement. The main take away message is that geocoding using the Google Maps API will normally be an iterative process that may requires multiple passes and adjustments to the data to arrive at accurate results. One things should now also be clear, while the Google Maps API has dramatically improved in its ability to offer geocoded results (including on messy names) these results should not be taken at face value. Instead, and depending on your purpose, multiple iterations may be needed to improve the resolution of the results. In this article we have not gone all the way with this but have hopefully provided enough pointers to allow you to take it further. R is a functional programming language meaning that it will be feasible to construct a function that brings together the functions used to process the data in the above steps. We will not go there today, but to round up lets think about some of the elements that we might want to use to address this in a single R function based on the steps that we have taken above. import dataset address case issues separate organisation, city, country resolve abbreviations on organisation names unite organisation, city and country into a new field send the cleaned field to the API and retrieve results adjust column names to match join results to original review the location type adjust and rerun as needed to improve rooftop and geometric centre results vs. approximate results In many cases it will make sense to choose a threshold based on counts of records before sending the data to the API. For example where dealing with publications (as in this case) it could make sense to exclude records where there is only one record. For example, in our original input table 1,624 entries only had one record. If no one is ever likely to look at data points with only one record you may wish to filter them out and concentrate on the accuracy of geocoding for scores above the threshold. We have also seen that while the focus of geocoding is logically on mapping, in reality geocoding services may offer new opportunities for the vexed problem of accurate name cleaning when working with the scientific literature or patent data. Reverse geocoding is the process of converting coordinates into named places but will not be covered in this chapter↩︎ If you would like to install qdap but run into problems with rjava on a Mac the instructions here can solve installation problems.↩︎ "],["patents.html", "Chapter 4 Counting Patent Data 4.1 The structure of patent numbers 4.2 Preparing to Count Patent Data 4.3 Understanding Priority Numbers 4.4 Counting Priority Applications 4.5 Counting Applications 4.6 Trends by Country using Publication Data 4.7 Patent Families 4.8 Modelling Data 4.9 Forecasting 4.10 Conclusion", " Chapter 4 Counting Patent Data This chapter provides an in depth introduction to the development of descriptive patent statistics. The most important existing resource for the development of patent statistics is the OECD Patent Statistics Manual and a series of OECD working papers (OECD Patent Statistics Manual 2009). However, a practical step by step guide to the development of patent counts using real world data has been lacking. This chapter aims to fill this gap by working up from the analysis of the structure of patent numbers through to the creation of counts of priority or first filings. We then focus on exploring patent families and conclude by graphing patent trends. Our aim is to provide a solid platform in issues involved in counting patent data that will allow you to explore more advanced techniques in the future (such as modelling and forecasting). The Chapter makes extensive use of the drones patent dataset. The drones dataset is a set of patent documents from Clarivate Analytics Derwent Innovation database relating to the term drone or drones created as a toy or training dataset for experiments in developing patent statistics. Instructions on how to install it are provided the How to Use the Handbook section at the front of the book and on the dataset home page. 4.1 The structure of patent numbers Patent numbers are the key identifiers for patent documents. At the time of writing there are over 107 million patent and related documents within the European Patent Office central DOCDB. The key to working with and counting these documents is an understanding of the structure of patent numbers. Table 4.1 presents the main patent numbers as they are commonly retrieved from patent databases. Table 4.1: Examples of Types of Patent Numbers row priority_number application_number publication_number 1 US2016578323F 2016-09-20 US2016578323F 2016-09-20 USD801224S1 2 US15360203A 2016-11-23 US15360203A 2016-11-23 US9807726B1 3 US62133061P 2015-03-13 US15069675A 2016-03-14 US9804596B1 4 US14622134A 2015-02-13 US14622134A 2015-02-13 US9802728B1 5 JP2015122335A 2015-06-17; WO2016JP67809A 2016-06-15 US15322008A 2016-12-23 US9802691B2 6 US15346251A 2016-11-08 US15346251A 2016-11-08 US9805273B1 For each of these numbers we observe the following structure A two letter country code such as US (United States) or KR (South Korea) A numeric identifier that, for more recent years, may include the year e.g 2016578323 or 15263985 A letter or combination of a letter and a number such as A, A1, B1, B2, S or P denoting what is called the Kind code for the document The date in Year - Month - Day format (known as YYYY-MM-DD) It is important to note that the patent numbers retrieved from patent databases are not necessarily presented in the way they are stored. For example, the country code, the numeric identifier, the kind code and the date are often stored in separate columns and are then combined together. When working with different databases this can be reflected in spaces or underscores between the country code, the number and the kind code. In addition, the original entries on patent application forms often include forward slashes /. In some cases databases will present numbers containing the forward slash character and in others they will be deleted as they do not add to the distinctiveness of the identifier. Be aware that some patent databases, notable Derwent Innovation from Clarivate Analytics, add padding 000s in the middle of some patent numbers to make them uniform and may add the year field at the beginning of patent numbers. For this reason using patent numbers across different databases is not as straightforward as it might be. Slight variations in the format will be the most common reason that a patent number is found in one database but not in another. 4.1.1 The country code The two letter country code denotes either the country of filing, the country of application or the publication country. For example, in row 5 of Table 4.1 above, JP2015122335A 2015-06-17 was first filed as an application in Japan (JP) and then went forward to the Patent Cooperation Treaty as WO2016JP67809A 2016-06-15 (where WO denotes the Patent Cooperation Treaty). That application was then submitted as US15322008A 2016-12-23 in the United States (US) and published as US9802691B2. A full list of two letter country codes, including the codes for the Patent Cooperation Treaty (WO) and regional instruments is available in the WIPO Handbook on Industrial Property Information and Documentation under standard ST.3 Recommended Standard on two-letter Codes for the Representation of States, Other Entities and Intergovernmental Organizations. The same table is also available on Wikipedia. Software tools such as VantagePoint provide thesauri that will convert country codes to country names. Software packages such as countrycode in R are designed to facilitate data science work and often include various formats for country code systems that you might encounter as well as providing country names in relevant languages. The country codes tell us where a document is filed and published. This provides the foundation for identifying trends by country using a range of different counts and for geographic mapping. 4.1.2 The numeric identifier The second element of a patent number is a numeric identifier. As noted above, it is important to note that these identifiers may be edited by databases. For example, the Derwent World Patent Index often uses either a single or padding zero between the year in a field and the number. This probably arose from an effort to make patent numbers a uniform length but has the effect that these documents cannot easily be retrieved from databases that do not use padding zeros (such as esp@cenet). In other cases the numbers may include characters such as “/” that are not formally speaking part of the identifier. The kind code may also be added in some databases but not in others. 4.1.3 Kind Codes Kind Codes appear as letter and number combinations after the numeric identifier. Kind codes describe the type of document and its publication level. Kind codes are documented in WIPO Standard ST.16 Recommended Standard Code for the Identification of Different Kinds of Patent Documents. Formally speaking Kind Codes are applied to: “patents for invention, inventors’ certificates, medicament patents, plant patents, design patents, utility certificates, utility models, patents or certificates of addition, utility certificates of addition, and published applications therefor” (WIPO ST.16 page 3.16.1, October 2016) In connection with the publication level, the definition explains that: “a “publication level” is defined as the level corresponding to a procedural stage at which normally a document is published under a given national industrial property law or under a regional or international industrial property convention or treaty.” So, to take a common example, a patent document may be published at the application stage. This is commonly the first procedural stage and is the first publication level. In many countries, standard patent applications will receive the kind code A at this first publication level. In many cases, when a patent is granted the document enters the second publication level and it is published with kind code B. Other types of patent documents commonly receive their own kind codes denoting their type. For example Utility Models receive the kind code U while design patents received kind code S. Kind codes are often accompanied by a number that adds additional information about the type of document. Within the WIPO standard these numbers range from 1 to 7 with numbers 8 and 9 reserved for corrections to the bibliographic data (e.g. A8) or to any part of the document (e.g. A9). WIPO Examples and Kinds of Patent Documents forms part of the WIPO Handbook on Industrial Property Information and Documentation setting out common Kind Codes and is recommended reading for patent analysts. Quick reference tables are also produced by Clarivate Analytics and national patent offices and are available online. The use of patent kind codes has evolved over time and this can make accurate interpretation of a kind code difficult. In practice: The use of a kind code may vary in the same country over time. For example in the United States prior to 2001 patents were only published when they were granted (first publication level) and they were awarded kind code A. From 2001 onwards the United States adopted common practice elsewhere and published applications which now receive kind code A (as the first publication level) while patent grants are now the second publication level (and receive kind code B); The use of kind codes varies between countries. As a very rough rule of thumb, publications with kind code A commonly mean the publication of an application. Publications with kind code B mean publication of a patent grant. But, this is not always true and it can only be described as a rough rule of thumb. This rule can normally be used when working with data from the main jurisdictions such as the United States (bearing in mind the pre and post 2001 changes considered below), the European Patent Office and the Patent Cooperation Treaty (which only covers first level publications or applications) and Japan. However, when working with country level data to elaborate trends for applications and grants it is important to review the use of kind codes in each country to be covered and to identify changes in the use of kind codes over time. When developing data on statistical trends for applications and grants a note should normally be added to inform readers that the data is approximate in cases where the use of kind codes has not been thoroughly explored and documented. Patent kind codes denoting publication level and the kind of document (e.g. U) are important for statistical purposes because they allow for the identification of duplicates and, depending on the purpose of the analysis, the removal of unwanted data types (such as Utility models, design patents and plant patents) or the isolation of specific types of document. For most purposes kind codes are important for identifying patent applications and grants and critically for identifying republications of the same document (duplicates) The most common kind codes encountered in patent data are: First publication level (commonly but not exclusively patent applications) A1 A2 A3 Second publication level (commonly but not exclusively patent grants) B1 B2 B3 This means, to take a fictional example from the USPTO and its common kind codes reference table, that: US1234A1 Patent Application Publication US1234A2 Patent Application Publication (Republication) US1234A9 Patent Application Publication (Corrected Publication) US1234B1 Granted Patent (not previously published as a patent application) US1234B2 Granted Patent (previously published as a patent application) In this example we have 3 potential publications of the same application and one publication of a patent grant (although corrections to a granted patent are possible). For the purpose of counting patent data how we deal with these republications or duplicates depends on the question we are trying to answer. If we wanted to identify the priority application that is closest to the investment in research and development we would choose the earliest application number available to us (that may or may not have been published). In other cases we may want to count all applications that stem from a first filing or set of filings while in others we may want to identify applications and grants but remove administrative republications (simple republications and corrections or publication of the international search report) from the counts. The main issue that arises here is determining what we wish to include and exclude from counts. One general approach to this issue is to simply remove all republications of the same document (count the document only once on the earliest in the series) . For example, if we were interested in counting patent applications and patent grants in the United States we would count US1234A1 and we would count the patent grant (US1234B2) and exclude the republications of the application. If we are only interested in counting patent applications we might simply remove the kind code denoting the different publication levels to count document US1234 only once. In practice, basic patent statistics commonly involves the use of multiple counting methods as we will see in more detail below. As the discussion above makes clear when working with patent data to elaborate patent counts we must address multiplier effects. These take two main forms: Republication of the same basic document in the same jurisdiction as applications, grants, or with modifications as continuations, continuations in part, divisionals or corrections. Submission of applications under regional patent instruments or the Patent Cooperation Treaty (WO) and their republication as applications, grants, other administrative publications or divisionals (in relevant jurisdictions). Thus, under the Patent Cooperation Treaty an applicant may submit the same patent application for consideration in up to 156 Contracting States. In practice this is rare but in theory a single application could potentially lead to the republication of the same document as an application and grant up to 312 times (assuming a single publication of an application and the publication of a grant in each country). While this would be a very unusual case, it reveals the potential multiplier effects in patent counts introduced by regional instruments such as the European Patent Convention and the Patent Cooperation Treaty. As we will see below, in some cases a single document may be linked to over 1000 publications. 4.2 Preparing to Count Patent Data The patent identifiers discussed above provide the key for tracking patent documents around the world, using the concept of patent families discussed below, and for elaborating patent counts. The most important single piece of information when thinking about patent counts using identifiers is that patent patent data involves multiplier effects that often leads to the duplication or republication of the same document or a document that has been modified based on an earlier version. Patent identifiers provide the basis for navigating these multiplier effects. The basis for patent counts commonly consists of removing duplicates or deduplication at different levels or combining documents in a variety of ways. We will use the drones dataset as an example of this. Table 4.2 shows a sample of different patent numbers. Table 4.2: Priority, Applications, Publication and INPADOC Family Members row priority_number application_number publication_number inpadoc_family_members 9 US2016620248F 2016-09-02 US2016620248F 2016-09-02 USD800603S1 USD800603S1 20171024 10 US14835329A 2015-08-25 US14835329A 2015-08-25 US9801234B2 US20170064599A1 20170302; US9801234B2 20171024 11 US15174819A 2016-06-06 US15174819A 2016-06-06 US9800796B1 US9800796B1 20171024 12 US14530548A 2014-10-31; US2013898275P 2013-10-31 US14530548A 2014-10-31 US9800517B1 US9800517B1 20171024 13 US2015274112P 2015-12-31; US2016341797A 2016-11-02; US2016341809A 2016-11-02; US2016341813A 2016-11-02; US2016341818A 2016-11-02; US2016341824A 2016-11-02; US2016341831A 2016-11-02; US62274112P 2015-12-31 US15341809A 2016-11-02 US9800321B2 CN106878672A 20170620; CN106982345A 20170725; CN107040754A 20170811; CN107046710A 20170815; CN107070531A 20170818; CN107071794A 20170818; CN206481394U 20170908; CN206517444U 20170922; EP3188474A1 20170705; EP3188475A1 20170705; EP3188476A1 20170705; EP3188477A1 20170705; EP3190788A1 20170712; EP3190789A1 20170712; US20170193556A1 20170706; US20170193820A1 20170706; US20170195038A1 20170706; US20170195048A1 20170706; US20170195627A1 20170706; US20170195694A1 20170706; US9786165B2 20171010; US9800321B2 20171024; WO2017114496A1 20170706; WO2017114501A1 20170706; WO2017114503A1 20170706; WO2017114504A1 20170706; WO2017114505A1 20170706; WO2017114506A1 20170706 We can see here that one or more priority numbers are linked to application numbers. In some cases those numbers are identical while in other cases they are distinct. The application number is linked to one or more publication numbers. Patent databases commonly return data based on publication numbers. However, this is often only a partial picture of the set of documents linked to an application or set of applications and their underlying priority filings. In the rows 10 and 13 in Table 4.2 we can see that we have one publication number. However, we can see that under the INPADOC Family Member Number there are multiple patent publications that are not captured in the publication number field. There are two reasons for this. A search of a patent database commonly returns patent publications based on criteria such as limiting the search by jurisdiction. This does not reveal all documents linked to a filing or set of filings worldwide. The INPADOC Family Members column groups publications based on a particular definition of a patent family. The number of documents will vary here depending on the definition of the patent family used in the data and whether the documents are deduplicated. Table 4.3 summarises the data by showing the counts of the different types of documents. Table 4.3: Sample of Patent Counts linked to the earliest priority number earliest_priority priority_count application_count family_count US2016620248F 2016-09-02 1 1 1 US14835329A 2015-08-25 1 1 2 US15174819A 2016-06-06 1 1 1 US2013898275P 2013-10-31 2 1 1 US2015274112P 2015-12-31 8 18 28 US201476360P 2014-11-06 7 4 7 Table 4.3 helps to make clear that we may be dealing with simple cases (one priority or first filing) leads to one application and one family member. Or, we may be dealing with a group of priorities leading to multiple applications and multiple family members. This helps to clarify that, when working with patent data we are often dealing with many to many relationships. In the next section we will progressively move up from counting patent documents using priority numbers and finish by using counts of INPADOC family members to elucidate trends for drone related technology. 4.3 Understanding Priority Numbers When a patent application is filed for the first time anywhere in the world it becomes the priority document or first filing. The use of this term is based on the 1883 Paris Convention. The WIPO summary of the key provisions of the Paris Convention explains the right of priority introduced by the Convention as follows. The Convention provides for the right of priority in the case of patents (and utility models where they exist), marks and industrial designs. This right means that, on the basis of a regular first application filed in one of the Contracting States, the applicant may, within a certain period of time (12 months for patents and utility models; 6 months for industrial designs and marks), apply for protection in any of the other Contracting States. These subsequent applications will be regarded as if they had been filed on the same day as the first application. In other words, they will have priority (hence the expression “right of priority”) over applications filed by others during the said period of time for the same invention, utility model, mark or industrial design. Moreover, these subsequent applications, being based on the first application, will not be affected by any event that takes place in the interval, such as the publication of an invention or the sale of articles bearing a mark or incorporating an industrial design. One of the great practical advantages of this provision is that applicants seeking protection in several countries are not required to present all of their applications at the same time but have 6 or 12 months to decide in which countries they wish to seek protection, and to organize with due care the steps necessary for securing protection.8 The OECD Patent Statistics Manual describes the priority number and the priority date as follows: Priority number. This is the application or publication number of the priority application, if applicable. It makes it possible to identify the priority country, reconstruct patent families, etc. Priority date. This is the first date of filing of a patent application, anywhere in the world (usually in the applicant’s domestic patent office), to protect an invention. It is the closest to the date of invention. (OECD 2009: 25) The most important issue here from the perspective of patent counts is the priority date. Table 4.3 above revealed that patent applications may be linked to multiple underlying priority applications. The earliest filing, as shown in Table 4.3, is the Paris priority in the sense that it is the first of a set of filings giving rise to a claimed invention. In the simple cases shown in rows 9-11 of Table 4.2, the priority number and the application number are the same. Where the priority number and the application number are the same, we have identified the priority or first filing. However, in rows 12-13 in Table 4.2 we see more complex cases involving multiple priorities. In row 5 of Table 4.4 we see a common case where a national level filing gives rise to an international filing under the Patent Cooperation Treaty that leads to an application in another country. In the remainder of this section we will walk through a number of examples. 1. National Filing to International Filing First let’s look at the example in row 5 of Table 4.2. Table 4.4: Priority, Applications, Publication and INPADOC Family Members priority_number application_number publication_number inpadoc_family_members US2016578323F 2016-09-20 US2016578323F 2016-09-20 USD801224S1 NA US15360203A 2016-11-23 US15360203A 2016-11-23 US9807726B1 NA US62133061P 2015-03-13 US15069675A 2016-03-14 US9804596B1 NA US14622134A 2015-02-13 US14622134A 2015-02-13 US9802728B1 NA JP2015122335A 2015-06-17; WO2016JP67809A 2016-06-15 US15322008A 2016-12-23 US9802691B2 NA We can see in row 5 that JP2015122335A 2015-06-17 lists a second priority number for a Patent Cooperation Treaty filing WO2016JP67809A 2016-06-15. At first sight this leads to US patent application US15322008A 2016-12-23 that is published as US9802691B2 for a Buoyant Aerial Vehicle. However, when working with priority numbers we commonly encounter multiple applications arising from the priorities. Table 4.5 displays counts of the priorities and applications linked to JP2015122335A 2015-06-17. Table 4.5: Applications arising from JP2015122335A 2015-06-17 earliest_priority priority_count application_count JP2015122335A 2015-06-17 2 4 The second priority number is the Patent Cooperation Treaty WO2016JP67809A 2016-06-15. Table 4.6 shows the sequence of applications arising from the two priorities and the INPADOC patent family. Table 4.6: Patent Applications arising from JP2015122335A with PCT application WO2016JP67809A priority_number application_number publication_number inpadoc_family_members JP2015122335A 2015-06-17 US15322008A 2016-12-23 US9802691B2 NA JP2015122335A 2015-06-17 EP2016811654A 2016-06-15 EP3150483A1 EP3150483A1 20170405; EP3150483A4 20170920; JP05875093B1 20160302; JP2017007411A 20170112; US20170137104A1 20170518; WO2016204180A1 20161222 JP2015122335A 2015-06-17 WO2016JP67809A 2016-06-15 WO2016204180A1 EP3150483A1 20170405; EP3150483A4 20170920; JP05875093B1 20160302; JP2017007411A 20170112; US20170137104A1 20170518; WO2016204180A1 20161222 JP2015122335A 2015-06-17 JP2015122335A 2015-06-17 JP05875093B1 EP3150483A1 20170405; EP3150483A4 20170920; JP05875093B1 20160302; JP2017007411A 20170112; US20170137104A1 20170518; WO2016204180A1 20161222 The first point to note is that Japanese priority application JP2015122335A 2015-06-17 becomes application JP2015122335A 2015-06-17 and is then published as a patent application in Japan as JP2017007411A 20170112 in January 2017 (first publication level) and also as a granted patent JP05875093B1 20160302.9 However, the Patent Cooperation Treaty application WO2016JP67809A 2016-06-15 filed a year later triggers applications (on the same date) in the United States and at the European Patent Office. Application number US15322008A 2016-12-23 is published in May 2017 as US20170137104A1 20170518 (first publication level) and on the second publication level as US9802691B2. European application EP2016811654A 2016-06-15 is published as EP3150483A1 20170405 (first publication level with the search report) and as EP3150483A4 20170920 with a supplementary search report. Note here that the database entry for the INPADOC family members for the United States application is blank (NA stands for Not Available) indicating that this record had not been updated or correctly updated. An up to date view of the patent family is available from esp@cenet and includes the US records and more recent patent activity. The priority data in this case reflects the decision taken by the applicant on the filing route to pursue protection in other countries in this case the United States using the Patent Cooperation Treaty. The Patent Cooperation Treaty has the advantage of extending the time that applicants enjoy before deciding where else to pursue protection from 12 months under the Paris Convention to up to 30 months. Applicants also enjoy reduced costs compared with the Paris Convention because a single application is submitted that may then go forward for consideration in other Contracting States. Note that the filing route can be detected in the priority number WO2016JP67809A 2016-06-15 which contains the country code JP as part of the application number. It is commonly the case that the listing of priorities numbers follows a sequence where the first priority number listed on a document is the earliest, followed by later applications. This common pattern may give rise to the temptation to simply take the first priority number in a sequence of priorities as the earliest priority. However, experience demonstrates that databases are not consistent in listing the earliest filing in this way. The temptation should therefore be resisted. As this example makes clear when working with patent data we are typically following a pattern consisting of: priority number (earliest) &gt; application number &gt; publications (under different family definitions) At each step the publications associated with the original filings multiply. In this case we are observing the distinctive filing route arising from a single priority filing. The filing route has nothing to do with the invention per se. However a more complex example reveals that inventions may arise from multiple claimed inventions. 2. Multiple Inventions We have seen above that in some cases patent applications involve multiple priority numbers. Most will follow the pattern identified above. However, in some technology areas, notably those involving computing, multiple priority numbers may be quite common. In the case of the drones dataset a single application US14815121A 2015-07-31 lists no less that 146 priorities and has given rise, at the time of writing, to an INPADOC patent family consisting of 340 publications. This example concerns a Wireless Power System for an Electronic Display with Associated Impedence Matching Network. It can be identified in esp@cenet using publication number US2015357831A1 and a summary is presented in Table 4.7. Table 4.7: Multiple Priorities and US Provisional and Continuation Filings priority_country n AU 1 TW 1 US 144 Table 4.7 reveals that the majority of priorities are domestic US priorities (144) with two foreign priorities in the form of Australia and Taiwan. The earliest priority filing listed in the set of priorities is US2007647705A 2007-12-28 and the latest are in 2017 signifying that the underlying filings linked to this application spanned nearly a decade. Of the 143 priorities linked to the application originating from the United States 107 carry kind code A and 36 contain kind code P for a Provisional application. This therefore appears to be a case dominated by US provisional and so called continuation, continuation in part and divisional applications. Provisional applications are a common type of patent application in the United States where an applicant may submit and claim priority to what is effectively an outline of a full application that does not contain patent claims. These applications serve to establish an early priority date but this does not take effect until an actual application is filed. Provisional applications are not published. Details of the procedures for these types of patent applications are found in the USPTO Manual of Patent Examining Procedure (MPEP). In the United States applicants are also allowed to file continuation and continuation in part applications. In the case of continuations the applicant adds new claims that claim priority to the earlier filed application. In the case of continuation in part, these applications add new subject matter focusing on enhancements to the original application. Divisional applications claim priority to the original application but claim distinct new inventions rather than adding new claims or subject matter. Divisional applications often arise where the examiner determines that an application contains more than one invention. The use of continuation and continuation in part applications in the United States has been a significant focus of debate and criticism (Hegde, Mowery, and Graham 2007). As Dechezlepretre et. al. have recently highlighted: “At the national or regional levels, applicants can in turn use second domestic filings, including divisional and continuing applications, to delay a patent grant. By filing a divisional application while the parent application is still pending, applicants can obtain a second (or possibly more) divisional patent(s) granted later, and meanwhile maintain some uncertainty on the claims. In the U.S. patent system, continuations and continuations-in- part can be filed after the examination, and aim precisely at adding more claims to a patent (Hegde et al. 2009). Filing a first application with narrow claims thus makes it possible for the applicant to obtain several patents on the same invention, thereby gradually extending the overall scope of the claims and even, in the case of continuations-in-part, the duration of the patent family.” (Dechezleprêtre, Ménière, and Mohnen 2017 at 802) While the cases where have discussed above are straightforward where this is one priority number, when focusing on US continuation filings the question that arises here is what should be counted? Having gained an understanding of some of the potential issues involved in preparing to count patent data we now turn to methods for counting priority applications 4.4 Counting Priority Applications Counts of priority filings are widely used in patent statistics and economic analysis because they focus attention on the relationship between the filing of an application for an invention and the underlying investment in Research and Development leading to the invention. Viewed from this perspective, and as highlighted in the OECD Patent Statistics Manual, the earliest filing of a patent application provides a proxy indicator for investments in Research and Development in a particular technology area. This information can therefore be used as a basis for identifying and exploring trends. Within the economics literature the preference therefore is for identifying and counting the earliest priority filing in a set of priorities. This is the approach that we will adopt here. However, the discussion of the Witricity case above involving multiple filings also reveals that we should be aware of some of the potential limitations of that approach. Thus, as mentioned above the priority filing of a patent application normally corresponds with the identical application number in the priority field. However, in the Witricity case the earliest filing was listed in December 2007 while the application was in mid-2015 roughly 7 years after the original filing. This raises the question of whether the date corresponding with the specific claimed invention should be taken or whether the earliest date should be taken as the basis for the proxy indicator. In practice, in purely methodological terms, it is easiest to identify the earliest priority in a set as the basis for elucidating trends in priority filings. However, this type of issue helps to illustrate that counts of priority filings are a proxy for underlying investments in Research and Development or in formal terms an output indicator and depending on your needs may merit refinement to more closely match the required granularity. Counting patent filings by priority involves 8 basic steps Checking the priority field for missing priority data (we can’t count missing data); Separating out the concatenated column containing priority numbers. These numbers are commonly separated with a semi-colon; Removing extra white space that is revealed by the separation process; Separating out the priority number and the priority date (commonly using the space between these fields); Grouping the priority numbers by the application number and rank them from 1 to n with 1 being the earliest; Filter the priority numbers to the earliest (rank 1); Detect duplicate priority numbers arising from those that share application numbers in the source set and remove them; Graph the results. The methodological steps required for this task can be performed using tools such as Open Refine which can easily separate out the data or in VantagePoint. The key challenge is in grouping and ranking the priority numbers by the earliest data. This is most easily achieved using a programming language (such as an SQL GROUP BY, PARTITION BY and RANK) or in a language such as Python or R. In the case of R this can be achieved in the following lines of code. The code is commented to show the steps identified above. earliest_priority &lt;- numbers %&gt;% select(priority_number, application_number) %&gt;% drop_na(priority_number) %&gt;% # drop empty priority fields separate_rows(priority_number, sep = &quot;;&quot;) %&gt;% # separate priority numbers on &quot;;&quot; mutate(priority_number = str_trim(priority_number, side = &quot;both&quot;)) %&gt;% # trim white space separate(priority_number, into = c(&quot;priority&quot;, &quot;priority_date&quot;), sep = &quot; &quot;, remove = FALSE) %&gt;% # extract the date mutate(priority_date = lubridate::ymd(priority_date)) %&gt;% mutate(year = lubridate::year(priority_date)) %&gt;% # add the priority year field mutate(priority_number = str_trim(priority_number, side = &quot;both&quot;)) %&gt;% # trim white space mutate(priority_country = str_sub(.$priority_number, 1,2)) %&gt;% # extract the priority country group_by(application_number) %&gt;% # group by application number mutate(filing_order = rank(priority_date, ties.method = &quot;first&quot;)) %&gt;% # rank application numbers by priority date ungroup() %&gt;% # remove grouping for later calculations filter(filing_order == 1) %&gt;% # filter to the earliest priority at rank 1 mutate(duplicate_priority = duplicated(.$priority_number)) %&gt;% # identify duplicate priority numbers filter(duplicate_priority == &quot;FALSE&quot;) %&gt;% # remove duplicate priority numbers select(-priority) # drop unnecessary field While the above may appear initially appear complex it follows step 1 to 7 above. Because the data lacks a priority country and year field it extracts these fields from the data to use in graphing as the next step. The key steps in arriving at accurate counts in the above process is trimming the separated fields to remove any leading and trailing white space. Note that the amount of cleaning required is likely to vary when working with data from different databases. The steps above reduce the dataset from 15,776 applications containing 23,382 priority numbers (after the exclusion of Not Available results) to 9,358 earliest priority numbers. We are now in a position to graph the data. For convenience we will continue to use R and the popular ggplot2 package. The results of a raw count are presented in Figure 4.1 Figure 4.1: Raw Graph of Trends in Priority Filings This is not generally what we will be expecting because it transpires that we have a long tail of low frequency records where the priority year is invalid such as 0001-01-01, or the date was not in the expected YYYY-MM-DD format. In addition this test dataset includes historic records from the 19th Century that we will not in this case be interested in seeing. It is therefore sensible to filter the results to a more recent period. Figure 4.2 shows the effect of filtering the priority year to 1990 to 2017. Figure 4.2: Graph of Trends in Priority Filings showing the Priority Data Cliff This version of the graph brings us closer but note that the data falls off a cliff between 2015 and 2017. This is a characteristic of counts of patents by priority filings and you should always expect to see it. The reason it occurs is not because of a collapse in patent activity but because of the lag time between the filing of applications and their availability as publications in patent databases. The gap between the filing of an application and its publication is generally 18 months but it may be at least two years. This problem is referred to as timeliness in patent statistics. The safest option is to pull back to the date range between 2 - 3 years to avoid giving the impression of a collapse in activity. Figure 4.3 shows the effect of this approach. When using this measure it is good practice to always add a note in the account to explain why this action is taken. Equally if choosing to present the data with the data cliff present it is good practice to explain that the data cliff arises from a lack of recent data and does not reflect a change in trends in activity. Figure 4.3: Graph of Trends in Priority Filings excluding the Data Cliff More advanced techniques for addressing the problem of timeliness were developed by Helene Dernis at the OECD and subsequently taken forward by Eurostat (Dernis 2007, de_Rassenfosse_2013). The key advantage of counts of the earliest priority filings is that they remove the duplication that is inherent in patent data and allows us to focus in on the underlying filing rate: that is to examine trends in filings using the dates that are closest to the underlying investment in Research and Development. However, trends in the first filings of patent applications are not the whole story. Other types of patent counts focusing on applications, patent families and patent family members focus on the nature and geography of demand for patent rights. This will allow us to compare the implications of different types of counts. 4.5 Counting Applications One challenge for the patent analyst is that data on priority applications may not be readily available or accessible when preparing a report. Patent databases vary in the quality of priority data and in these circumstances the use of simple counts of patent applications, accompanied by an explanatory note are likely to be an appropriate alternative. Here we need to bear in mind: That we need to use the application year; That we will be counting distinct applications that may arise from the same underlying filing. Figure 4.4 shows trends in patent applications by application year. Figure 4.4: Graph of Trends in Applications by Application Year In this case the data cliff (not shown) occurs from 2016 onwards. This reflects the fact that when compared with counts by the earliest priority year, counts by application year lean forward. Because these counts also capture the applications stemming from a priority, such as applications in more than one country, they will also be at a higher level. Figure 4.5 compares the two figures. Figure 4.6 zooms in to the figure for the period 2005 to 2015. Figure 4.5: Trends in First Filings and Patent Applications Figure 4.6 focuses in on the period 2007 to 2015 to more clearly see the divergences between the data. Figure 4.6: Trends in First Filings and Patent Applications As we might expect Figure 4.5 makes clear that counts of applications run parallel to trends in first filings. However, this is not the entire picture. Note the speed bump that appears between 2007 and 2010 starting with a downward inflection in 2008 followed by an upward inflection and downward inflection in 2010 reflecting a decrease then rapid increase and decrease in first filings. While the initial downward inflexible is displayed in the trend for applications the speed bump is replaced by a steadily increasing slope until 2013. What this reflects is the follow on multiplier effect of applications based on the underlying filings being published in multiple jurisdictions that disguises the temporary bottoming out of first filings over the same period before growth in filings accelerates dramatically. We can also see that in 2013 an inflexible occurs in filings and applications with the inflection occurring at a higher document count for applications. In summary, the application rate runs broadly in parallel with the priority rate but it smooths out and disguises changes in the priority filing rate that are likely to more closely reflect underlying investments in research and development. 4.5.1 Mapping Publications (Family Members) So far we have focused on mapping trends using the earliest priority documents and application numbers. We will now examine what happens if we map patent publications using the data contained in the INPADOC family member field consisting of 49,508 publications linked to the applications and their priorities. Figure 4.7 shows the trend for counts of publications in the INPADOC family members field. Figure 4.7: Trends in Publications (INPADOC Family Members) We can now place this data which simply counts the number of publications within the INPADOC family members field into a graph with the counts by priority and applications as in Figure 4.8. Figure 4.8: Trends in Publications (INPADOC Family Members) We can see in Figure 4.8 that the difference between the patent publications linked to the applications and the first filings is dramatic. We will consider different definitions of patent families below. For the moment, the important observation here is that trends in the publication of INPADOC family members linked to the applications and priority filings do not closely match the pattern displayed by the applications and priority filings. What we are observing here is the multiplier effect of demand for patent rights in multiple countries around the world. Demand for patent rights, as manifest in the filing, pursuit and maintenance of patent rights in multiple countries is an expression of the importance of the underlying invention to the applicants expressed in their willingness to pay for the pursuit of patent protection in different jurisdictions. Whereas first filings of patent applications are a proxy indicator for investments in research and development, trends in patent publications are an indicator of the commercial importance of those inventions to the applicants expressed in willingness to pay the relevant fees for examination of applications and the maintenance of patent grants. As we will see below the size of a patent family is therefore an indicator of the importance of an underlying invention to the applicant. However, as we will now see, while the majority of attention in patent statistics focuses on counts of priority filings and applications, publication data provides a route to identifying trends in patent applications and grants on the country and instrument level. 4.6 Trends by Country using Publication Data In this section we illustrate the process for developing patent trends analysis at the country level using the drones data. However, it is important to emphasise that the drones dataset is not complete. A complete analysis would require the construction of a patent dataset that used searches for relevant jurisdictions in the appropriate languages using tools such as Patentscope CLIR (Cross Lingual Information Retrieval) which facilitates the translation of search terms into other languages. As such the drones data we will be working with is incomplete for national level analysis. However, bearing this limitation in mind, it is nevertheless useful for illustrating methods, considering the issues that are likely to be encountered and how to deal with them. When seeking to develop analysis on the country level it is important to note that some countries and instruments will display high levels of activity (WO, EP, US, JP, CN and others) while others will display very low levels of activity. As a consequence, attempts to graph the data will results in a large number of countries appearing at the bottom of the graph. One approach to this is to simply focus on graphing the countries/instruments with the highest number of results. We will focus on mapping trends for the top countries and instruments. Figure 4.9 displays the trends in publications, as an indicator of demand, for the top countries and instruments. Figure 4.9: Trends in Publications using INPADOC Family data Figure 4.9 reveals that in terms of patent publication counts the United States is the lead country by a considerable margin, followed some distance behind by the international Patent Cooperation Treaty (PCT with code WO) and the European Patent Convention (EP) administered by the European Patent Office (EPO) and in turn by Japan (JP) and China (CN). Note however, that this is not comparing like with like as the US data is confined to a single country whereas the EP and PCT are vehicles for the pursuit of protection in multiple countries. We can also see that the dominance of the US data in terms of counts of publications is compressing the data for the other main countries and instruments to the bottom of the graph. In practice, the difference is so marked that we would probably seek to separate out the data. We will address this in greater detail below. Patent publication data, in this case derived from INPADOC Family Member numbers, is important because it allows for the analysis of trends in patent applications and patent grants using patent kind codes. As discussed above, one challenge with patent kind codes that describe publication levels is that their use has varied over time. Thus, in the United States prior to 2001 patent documents were only published when granted and received kind code A (first publication level). After 2001 the United States began publishing patent applications as the first publication level and they received kind code A while kind code B is used for the second publication level (granted patents). This has two impacts on counts of patent data. First, as we see in Figure 4.9 the data for the United States appears to leap between 2000 and 2001. This does not reflect a leap in patent activity but is instead a reporting effect arising from the publication of both applicants and grants. Second, we cannot simply use the A and B kind codes to separate out trends in patent applications and grants for the United States because they refer to different types of publications over time. To address this we have created a field in the families table called kind-adjusted that has converted US kind code A documents to kind code B in the period before 2001. The original kind code is maintained in a field called kind_original as good practice when transforming the original data. The effect of this adjustment is shown Table 4.8 where based on the date of the publication (family date), the kind code is adjusted. Table 4.8: Adjusting historic kind codes for US patent data family_members family_date kind_original kind_adjusted US5551521A 1996-09-03 A B US5894897A 1999-04-20 A B US6158531A 2000-12-12 A B US5920995A 1999-07-13 A B US6032374A 2000-03-07 A B US5155307A 1992-10-13 A B Figure 4.10 displays trends in patent applications and grants based on this adjustment.10 Figure 4.10: Trends in US Patent Applications and Grants These raw counts of publications with kind code A for applications and kind code B for patent grants reveal two points. In the case of this data there appears to have been a spike in patent grants between 2000 and 2001 from 67 to 184 grants which may, or may not, be associated with the shift to the publication of patent applications from 2001 onwards. The second point is that we observe the start of the publication of patent grants from 2001 onwards with notable peaks and troughs suggesting that this graph would benefit from smoothing. However, for our purposes we can clearly see the point at which US data is transformed in scale by the publication of applications. The important point to bear in mind here is that in the period prior to 2001, US patent activity was under-reported relative to activity elsewhere because only data on grants was published. From 2001 the US harmonises with the rest of the world and we see an apparent jump in activity that is in fact a reporting effect. In considering the use of patent publication data (in this case arising from INPADOC family members) it is important to remember that data by publications also leans forward. Peaks and troughs in this data will in fact reflect changes in underlying filing activity from at least two years before. In addition, peaks and troughs will be affected by strategic behaviour by applicants such as the filing of continuation, continuation in part and divisional patent applications (see Hegde, Mowery, and Graham 2007 for discussion) along with administrative issues within patent offices that may affect the publication of patent documents. As this example suggests, significant caution is required when seeking to use publication data to map patent applications and grants in any given country. That is, it is important to investigate the use of kind codes over time for each country that is included in an analysis. For example, in a number of European Patent Convention (EPC) member states, a regional European patent grant only becomes a national patent grant when it is translated. These documents are awarded kind code T. In EPC member states kind code T is therefore equivalent to a patent grant. We will now look at trends across the top 10 countries. In doing so we need to recall the major caveat that this data will be incomplete for drone technology in each country. However, to get a feel for this we will start simply by mapping trends in publications and then break the data down using kind codes. Because some of the countries in the top ten have relatively limited activity we will include a Loess smoothing trend line rather thank linking the data points. The results are presented in Figure 4.11 and limited to the period 2000-2017 for ease of visibility. Figure 4.11: Publication Trends for Top Ten Countries (INPADOC Family Members) In considering Figure 4.11 note that the scale of activity (shown by the y axis for each country) reveals quite dramatic differences in activity for each country. We can also see that Australia (AU) and Germany (DE) display unusual patterns. This type of pattern typically reflects a relative lack of pattern in low frequency data. As such, this type of graph can assist with decision making in identifying the most significant sources of data to present to readers for analysis. In work on the development of the scientific and patent landscape for marine genetic resources for South East Asian countries a problem emerged where major peaks were encountered followed by zero or very low activity. Radical peaks and troughs in patent data typically suggest missing data. In the case of South East Asia it transpired that the EPO Worldwide Patent Statistical Database (PATSTAT) had very limited coverage of ASEAN national collections and thus presented a very partial view when compared with Derwent Innovation which includes the ASEAN national collections. As such, graphs of the type displayed above should be initially used for exploratory data analysis with a focus on the assessment of the completeness of the data (in this case we know that the data will be incomplete). Figure 4.11 also provides us with important clues on what we might expect when we seek to visualise patent trends by kind codes for these countries. That is we should expect to see erratic data or no real pattern for countries with lower counts. For illustration Figure 4.12 presents the breakout of the data for each country above using kind code A and B and excluding other types of document (e.g, Utility models and designs). Note that for the purpose of discussion we are ignoring the variations in the uses of kind codes in the countries. Figure 4.12: Publication Trends for Top Ten Countries with Kind Code A and B A number of features emerge in Figure 4.12, In the case of the Patent Cooperation Treaty (PCT and coded WO) we can see a small number of patent documents with Kind Code B. In practice there are two PCT documents in the dataset with Kind Code B. This is presumably a data entry mistake because WIPO does not issue documents with Kind Code B as the PCT has no second publication level. A second feature emerges in the case of Japan (JP) where the number of patent applications dips dramatically below the level of B documents before increasing dramatically. We would reasonably expect that the application rate would be higher than the grant rate. In practice, this issue will reflect the incomplete nature of the dataset we are working with. A third issue arises with Australia (AU) which exhibits a peak in activity around 2000/2001 and then collapses. One known issue with data from Australia is that around this period Australia recorded PCT designations as if they were actual applications leading to distortion of the statistics between approximately 2000 an 2003.11 In the case of Canada (CA) we observe activity for Kind code A but no activity for Kind code B. This illustrates the importance, as emphasised above, of examining the use of kind codes in individual countries that will be the focus for analysis. Canada uses Kind code C for patent grants while Kind Code B is used for reissue patents. The case of China (CN) which displays data for kind code B from 2010 onwards may suggest that there are limitations in the availability of patent grant data that require investigation. In the case of Germany (DE) this exposes the limitations of our dataset (which did not involve searching the German collection). While bearing this in mind, note that the data on patent grants (which exceeds those for applications) is reflecting the translation of EPC patent grants (with kind code T for Translation). In practice the landscape of Kind Codes for Germany is quite complicated and once again reveals the need to review Kind Codes with considerable care when developing this type of analysis. 4.7 Patent Families In the preceding sections we have moved from counting patent data by priority or first filings, to counting applications and then using patent publications to explore the issues involved in mapping trends in patent applications and grants on the country level. As we have seen when we move to the country level issues of data capture from the language of the search strategy and the interpretation of kind codes become major issues. As part of this discussion we used patent publication data from INPADOC Family Members that are linked to the underlying first filings. We will now look at patent families and their uses in more detail. At its simplest a patent family is a grouping of patent documents based on a relationship or set of relationships. As we will see below that relationship can vary. However, for everyday purposes the following working definition has the benefit of being simple and easy to remember. At its simplest a patent family can be understood as a stack of documents published in any language anywhere in the world that share a common parent in the form of a priority number. As we will see, this simple working definition describes around 75% of patent families in the EPO Worldwide Patent Statistical Database (PATSTAT) database. In practice, there are a number of different definitions of patent families. Simple first filing based families. These are families where members must share a priority number (Martinez 2010a). DOCDB families. Similar to the above except that based on expert review at the EPO documents with the same technical content are added to the family. DOCDB refers to the EPO central documentation database (Martinez 2010a) INPADOC extended families. These families share the DOCDB definition but the definition is expanded so if document A shares a priority number with document B they are in the same family. However, if document C shares a priority with document B but not document A then document C will still be grouped in the family of document A. In addition, examiners may identify other technically related documents that are added to the family. INPADOC patent families are therefore larger than DOCDB simple families (see below). Triadic patent families (OECD definition for filings shared between the US, EP and JP). This is used by the OECD in patent analysis to refer to patent filings that are made in the United States, at the European Patent Office and in Japan. The aim of these families is to allow for analysis of the internationalisation of technology by neutralising the home bias created by the fact that most applications are made in national offices. This is achieved by focusing on those made at the three major offices (Dernis and Khan 2004; Criscuolo 2006; Sternitzke 2009). Derwent Patent families. A type of patent family used in the Derwent World Patent Index from Clarivate Analytics. This type of patent family relies on the identification of new priority filings that become the Basic patent for a patent family. Documents sharing that priority are classified as equivalents and become part of the patent family and includes continuation and continuations in part. In addition, what are called non-convention equivalents that do not share a priority but with the same technical content are added to the family and marked. This allows users to identify documents for the same invention that do not share a priority. PatBase families. PatBase defines its families as follows: “A PatBase family contains all publications that share one or more common priority number(s). This includes continuations-in-part. If PatBase families become very large (100+ members) where possible these are broken down into sub-groups of simple families. A simple family is one where all priority numbers are shared.” It is likely that this list of definitions of patent families is incomplete but it indicates the range of possible groupings. In practice, the most commonly encountered definitions of patent families are simple families, DOCDB families and INPADOC extended families.However, as we can see from the definitions above one challenge with patent families is that there appears to be an element of subjective judgement whereby examiners or database providers take decisions on the members of patent families. In addition patent database providers do not always clearly describe the process for determining patent families. This can make patent families confusing and indeed obscure. The simple working definition provided above is designed to help maintain clarity of focus. Research on patent families has been greatly enhanced by the creation in 2006 of the EPO Worldwide Patent Statistical Database (PATSTAT). In 2010 Catalina Martinez published an important OECD Working Paper entitled Insight into Different Types of Patent Families” on the structure of patent families using PATSTAT data for the period between 1991 and 2009 (Martinez 2010a, 2010b). This study made a major contribution to clarifying the impact of different definitions of patent families using PATSTAT as an international baseline and also explored the structure of patent families. We will now briefly summarise and explore the main findings from this study. Martinez focuses on the important question of how to build patent families and identifies four types of linkages that can be used to build patent families: Paris Convention priorities Technical similarities (also called non-convention priorities, intellectual priorities or technical relations) Domestic priorities (e.g.continuations, continuations in part, provisionals, divisionals) PCT regional/national phase entries (Martinez 2010: 23) Each of these types of linkages is accompanied by a definition and whether the information is provided by the applicant as in Table 4.9 reproduced from Martinez below (Martinez, 2010. Table 7, 23) Table 4.9: Sources of the Building Blocks for Patent Families Type Definition Claimed by applicant in patent document Paris Convention priorities Allow a one year delay between first original filing and subsequent foreign filings by same applicant claiming the priority right (1883 Paris Convention). YES Technical similarities Relations among patent documents with similar scope, inventor and applicant names, that nevertheless lack common priority. An artificial priority link is assigned manually by the database producers. NO Domestic priorities Filed at the same office. They are mainly continuations, continuations in part and provisionals (the three of them only available at USPTO), and divisionals, which are available at most patent offices (1883 Paris Convention). YES National phase entries of PCT filings Entry into regional/national phases of PCT filings. YES As we can see in this table, the source of information for building patent families predominantly comes from the applicant except for the technical similarities. Technical similarities are identified by examiners based on their assessment of the scope, inventor and applicant names and lead to the creation of an artificial priority link. As such, the identification of technical similarities is not purely subjective. Martinez then uses the EPO Worldwide Patent Statistical Database (PATSTAT) to quantify the impact of the use of the different linkages on the size of patent families for the period 1991 to 1999. Table 4.10 reproduces Table 9 from Martinez’s research. Table 4.10: Counts of families and applications in them, by source of family relations 1991-1999 (Martinez 2010a) Source 1 Families Source 2 Families Source 3 Families Source 4 Families Paris Convention Paris Convention + Domestic Continuations Paris Convention+ Domestic continuations + Technical similarities) Paris Convention + Domestic continuations + Technical similarities + PCT national phase entries year families_1 members_1 families_2 members_2 families_3 members_3 families_4 members_4 1991 106850 567024 110371 610367 110856 614631 110745 614888 1992 107873 571499 113659 624205 114394 629235 114276 629538 1993 112351 602058 119014 655380 119656 659896 119533 660106 1994 116602 639485 123818 693431 124606 698404 124362 699194 1995 129535 722318 135946 761371 136702 766020 136405 766939 1996 146281 805300 152088 849307 152994 854422 152681 855240 1997 161060 874480 166277 914281 167220 919518 166857 920284 1998 173243 939352 179345 985989 180095 990508 179618 990914 1999 196972 1057368 204330 1102833 205222 1106929 204659 1107482 1991-1999 1250767 6778884 1304848 7197164 1311745 7239563 1309136 7244585 Note that Table 4.10 focuses on the earliest priorities and excludes singletons.b A number of important points emerge in Martinez’s analysis. The first of these is that “Paris Convention priorities alone make up more than 95%, which make them the most relevant patent linkage by far in the construction of extended patent families.” As such Paris Convention priorities are the foundation of patent families. The second major observation is that the number and size of families increases as the definition is expanded in the first three cases. The third case exactly matches with the widely used INPADOC extended patent family definition (Paris, domestic continuations and technical similarities). However, observe that the number of families in the final case in the table is lower in all cases than for the source 3 (INPADOC) even while the number of family members increases. The reason for this is that first three types create new independent families. In contrast, the final type consolidates families by revealing shared links through the Patent Cooperation Treaty. That is, the number of families falls because otherwise hidden linkages with the PCT become obvious (see Martinez 2010: 25). Using this data we can visualise the impact of different types of counts of families and family members. We will focus here on displaying the difference between the simple family definition and the INPADOC definition (source 3 above). Figure 4.13 trends in the number of patent families using the simple and INPADOC definitions (Martinez 2010a). Figure 4.13: PATSTAT Trends in DOCDB Simple and INPADOC Extended Patent Families We can see here that the DOCDB simple families and the INPADOC family counts follow the same pattern except that the number of INPADOC families are consistently larger than the DOCDB simple families. Figure 4.14 places counts of the number of families and the number of family members onto the same graph. This brings the difference between counting the number of families rather than the number of family members into focus. Figure 4.14: Trends in Patent Families (priority filings) and Family Members compared In considering this graph note that while the families count range is in the hundreds of thousands, with 205,222 INPADOC families in 1999, the equivalent count for INPADOC family members was 1,106,929. Under both the DOCDB families and the INPADOC families definitions this brings home the scale of the multiplier effects arising from the publication and republication of patent documents. We have seen a similar type of pattern in Figure 4.8 for the drones data where the number of patent family members accelerates away from the priority filings as publications multiply. Martinez also explores and quantifies the different structures of patent families in the period between 1991 and 1999 (see in particular Martinez 2010a). Martinez developed an algorithm to assess the structure of the INPADOC patent families discussed above and found that: “Applying the algorithm just described to the 1 311 745 INPADOC extended patent families with earliest priorities in the 1990s, as reported in PATSTAT September 2008, and excluding families formed by one patent document only (singletons), a total of 47 437 different family structures are identified. Among them, however, only a few structures are really popular: 86% of the structures represent just one family each, whereas only 10 structures represent 73% of all families and 25 represent 81%. In addition, more than half of the top 25 structures are made up of one earliest priority followed by several direct subsequent filings, what we will call “simple structures” from now onwards.” 12 The important point here is that while initially the structure of patent families may appear to be extremely diverse, with 47,437 different family structures, in practice 10 structures described 73% of the families and 25 structures accounted for 81%. This is a very significant finding because it informs us that the majority of the time the structure of patent families falls within a limited set. Furthermore, of the top 25 structures, over half are made up of a simple structure consisting of a single earliest priority followed by several direct filings. Martinez goes on to conclude that “…we have found that 75% of all INPADOC extended priority patent families with earliest priorities between 1991 and 1999 have a simple structure, consisting of one single first filing and its direct subsequent filings” (Martinez 2010a). The broader significance of this is because INPADOC extended families are the broadest category of common family types that 75% of other patent families will be of the single first filing and direct subsequent (child) filing type. From this we can reasonably conclude that the majority of patent families that we will ever encounter will be of the simple type (single priority followed by several direct filings). However as Martinez also notes “Complex families may favour specific technologies, countries or more valuable patents” (Martinez 2010: 17) so it is important to bear in mind that complex families while representing a much smaller proportion of the data may also be important when dealing with particular fields or valuable patent families. In considering the use of different types of family counts the most commonly encountered forms will be the DOCDB simple family types or the INPADOC family types. In the sections above we have graphed families (priority filings) and used raw family members to explore trends in different countries. However, as mentioned above the size of patent families is an indication of the importance of an invention to the applicants based on their willingness to pay fees for the stages of the procedure and for the maintenance of any granted patents in one or more jurisdiction. Table 4.11 displays the raw count of INPADOC family members across the drones dataset linked to the earliest priority number. Table 4.11: Top INPADOC Families earliest_priority application_number family_count AU19977991A 1997-07-15 JP200944799A 2009-02-26 2964 US13420236A 2012-03-14 US14253376A 2014-04-15 1819 US2013811981P 2013-04-15 US14253099A 2014-04-15 1819 US2001270625P 2001-02-23 EP2002714961A 2002-02-21 1440 US2011452418P 2011-03-14 US14024204A 2013-09-11 808 US2002387792P 2002-06-11 US13602510A 2012-09-04 808 US14064189A 2013-10-27 US14526503A 2014-10-28 371 US13158372A 2011-06-10 US14065419A 2013-10-29 371 US201364189A 2013-10-27 EP2014857043A 2014-10-28 371 US14065415A 2013-10-28 WO2014US62477A 2014-10-27 371 The top result in the drones dataset lists an earliest priority filing in 1997 and concerns Methods for Manufacturing Inkjet Print Head using Multilayer Material Layer by Silverbrook Research in Australia involving Kia Silverbrook who has been described on Wikipedia as having been the worlds most prolific inventor.13 According to news reports Silverbrook Research went into administration in 2014.14. This is an example of noise in our drones dataset and a historic example, however the size of the INPADOC patent family indicates that the claimed invention was of great importance to the applicants. Table 4.12 displays the countries that have been the focus for the development of this family. Note that each document in the family is counted. Table 4.12: Top Family Countries for priority AU19977991A family_country raw consolidated US 2116 2116 AU 334 292 SG 52 52 EP 119 49 JP 44 44 WO 40 38 CN 35 35 DE 38 31 IL 55 30 KR 30 30 AT 28 28 ZA 27 27 CA 43 26 NZ 2 2 ES 1 1 Table 4.12 shows the raw counts of family member documents for each country and the consolidated counts (with the kind codes removed to group by document identifier). The documents are ranked on the count of the consolidated family members. We can immediately see that while the applicant company was Australian the key target market was the United States followed by Australia, Singapore, Europe (through the European Patent Office) and Japan. By far the greatest intensity of family members is found in the United States and it transpires that these family members are clustered by date. Figure 4.15 shows US family member publications as a simple frequency plot over time. Figure 4.15: Frequency Plot for the Publication of US Family Members for priority number AU19977991A 1997-07-15 Figure 4.15 shows a strong clustering effect. In order to more clearly understand what is happening inside this plot Figure 4.16 displays the frequencies for US granted patents within the family over time (kind code B). Figure 4.16: Frequency Plot for Patent Grants among US Family Members for priority number AU19977991A 1997-07-15 What emerges here is that there are 1,086 US patent grants that can be traced to the single Australian priority filing.15 The earliest US patent grant in the set was published in March 2000 as US6041600A for the Utilization of quantum wires in MEMS actuators that claims priority to the Australian document. The latest patent grant in 2017 provides the explanation for what has been happening with the family members. The text of the most recent patent grant US9584681B2 for a Handheld Imaging Device Incorporating Multi-Core Image Processor in the description on related applications says: This application is a continuation of U.S. application Ser. No. 13/101,131 filed May 4, 2011, which is a continuation of U.S. application Ser. No. 10/656,791 filed Sep. 8, 2003, issued Jun. 7, 2011, as U.S. Pat. No. 7,957,009, which is a continuation application of U.S. application Ser. No. 09/922,274 filed Aug. 6, 2001, issued Sep. 9, 2003, as U.S. Pat. No. 6,618,117, which is a continuation-in-part application of U.S. application Ser. No. 09/113,053, filed Jul. 10, 1998, issued Mar. 26, 2002, as U.S. Pat. No. 6,362,868. Each of the above identified patents and applications and U.S. Pat. No. 6,238,044 are hereby incorporated herein by reference in their entirety. While we have not been able to review the entire portfolio of patent grants, a review of the five most recent documents suggests that the US members of this family have been constructed from a long series of continuation and continuation in part applications. Further analysis of this specific case is beyond the scope of the Handbook. However, as discussed in the recent in depth review of patent family research by Dechezleprêtre et. al. 2017 it highlights the strategic uses of the patent system with respect to continuation filings (Dechezleprêtre, Ménière, and Mohnen 2017). Lemley and Moore provide a critique of the use of continuations in the US system and focus on the extreme example of patent activity by an individual inventor (Jerome Lemelson) in connection with bar code readers (Lemley and Moore 2003). They highlight that by using continuations and continuations in part: “Inventors can keep an application pending in the PTO for years, all the while monitoring developments in the marketplace. They can then draft claims that will cover those developments. In the most extreme cases, patent applicants add claims during the continuation process to cover ideas they never thought of themselves, but instead learned from a competitor. The most egregious example is Jerome Lemelson, who regularly rewrote claims over the decades his patents were in prosecution in order to cover technologies developed long after he first filed his applications. Lemelson filed eight of the ten continuation patents with the longest delays in prosecution in our study. Those Lemelson patents spent anywhere from thirty-eight to more than forty-four years in the PTO.” (Lemley and Moore 2003) As Dechezleprêtre et. al. point out the aspects of US patent law that facilitated the specific strategic behaviour by this applicant have been abolished. However, continuations and continuations in part continue to feature as part of the US patent system. From the perspective of patent analytics, this example demonstrates that we are able to move from the use of patent family data to explore patent activity linked to a priority filing or set of priorities anywhere in the world. We are also able to identify the top ranking patent families and to explore the details of those families in individual countries. Simple techniques such as the use of a frequency plot over time help us to identify patterns in activity. In the case of Silverbrook we have seen that while the company reportedly went into administration in 2014, with a collapse in US family activity observable in the frequency plots above for that period, we then observe the issuance of patent grants from 2015 into 2017 (the end of our data). This suggests that this portfolio has been taken over (perhaps through the sale of the IP to a third party). To proceed further in the analysis of this case we would want to look at the legal status for the documents in the portfolio. Thus, a review of the legal status for the most recent patent grant US9584681B2 reveals that it is now owned by Google.16 In addition, as discussed by (B. H. Hall and Harhoff 2012) the analysis of patent renewal fees has an important role to play in analysis of the economic value of patent grants within a family portfolio. The discussion of patent families presented above points to the richness of research that is possible using patent family data. It is important to note that different definitions of patent families are important in patent analytics and may yield different insights. Thus, one criticism of INPADOC patent families is that they are too broad and a simpler more focused definition may be preferred. In other cases as in the work by the OECD the use of trilateral patent families is intended to promote international comparability by removing the home bias in patent data. The use of this type of definition is important for both comparability and identification of more important patents. Thus, in the case of countries such as China, where filings are heavily focused on the national level, the selection of patent families with international members can facilitate the analysis of cases where Chinese inventors and applicants may be seeking to invest in external markets. In closing this discussion it is important to emphasise that patent families continue to be a very active focus of research in economics and innovation studies. As the most recent review of the literature by Dechezleprêtre et. al. 2017 has highlighted to date the main focus in this literature has been on using family size as the indicator of value (focusing on the number of countries represented in a family) the number of filings in the priority country along with the time span between the first and last filings within a family also offer insights into patent value (Dechezleprêtre, Ménière, and Mohnen 2017). As such, the discussion presented above presents a starting point for engaging with this increasingly rich literature. 4.8 Modelling Data So far in this chapter we have focused on practical issues that need to be considered in counting patent data. We now turn to a basic introduction to modelling data as a basis for introducing forecasting techniques. Our aim is not to provide an in depth consideration of modelling, that would require more in depth engagement with statistical concepts and techniques. Rather, we wish to encourage you to engage with modelling. In approaching there topics we should bear in mind that most patent analysts and data scientists are not statisticians or economists. However, many within the data science community are aware of this. As a result there are many resources out there to make these methods and techniques accessible for people who may have very different levels of knowledge and experience with statistics. If you are already familiar with statistical methods you may want to skip to the next section on forecasting patent data. In this section we focus on two widely used models for linear regression: simple linear regression and loess smoothing (Local Polynomial Regression Fitting). These models are methods for finding the ‘line of best fit’ to represent trends in a set of data points and they are widely used in software tools such as Excel and Tableau among others. We will use these models as an easy way in to modelling data and means for assessing data. There are many textbooks that address modelling and linear regression for readers of all levels of experience. For R and Python users there is the accessible Practical Statistics for Data Scientists by Peter Bruce et al. For R users seeking a more in depth but accessible introduction to the maths with worked examples there is Discovering Statistics Using R by Andy Field, Jeremy Miles and Zoe Field. More advanced users are likely to benefit from An Introduction to Statistical Learning with Applications in R by Gareth James et al. which builds on the now classic, and maths intensive, The Elements of Statistical Learning: Data Mining, Inference and Prediction by Trevor Hastie et al. The tidymodels framework in R brings together many popular modelling packages in R for intermediate and advanced users. 4.8.1 The Data WIPO makes its patent statistics available as part of the intellectual property statistics section of its website through the WIPO IP Statistics Data Center. Under the PCT section of the Data Center we are able to select a range of indicators. For our purposes we select PCT applications by filing date and Yearly Statistics as the Year Type (note that monthly is also available). A range of different offices around the world can receive Patent Cooperation Treaty (PCT) applications because it is the international patent instrument for submitting applications outside ones home country. We want the number of applications submitted directly to WIPO as the International Bureau and so we choose the International Bureau as the Receiving Office. For simplicity we just choose the Total as the Origin Office. Figure 4.17: Selecting PCT Applications to the International Bureau When we press Search we see the data that we are after. Figure 4.18: WIPO Patent Statistics Data format Seeing the data immediately tells us something about an initial task before we can get to modelling. The data is in wide format (the easiest for a human to simply print out and read). However, we will want this data in long format with year as the column heading. Having downloaded the data we start to think about importing into R or Python. However, it turns out that there are some extra headings at the top (which we can normally skip), but more importantly that the heading for the Receiving Office and the Origin columns are on different rows to the year. That requires a frustrating manual fix. When we import the data we discover that the year columns are preceded by an x for reasons that are completely unclear. We make the adjustments and pivot the table to long format. For 2022 the value is not available (NA) and so we simply drop it. This is the reality of data science. Following the manual step we can perform the rest of the transformations with R or Python as we see in the code below. library(tidyverse) library(tsibble) library(slider) library(fable) # manually prune uneven rows at the top # make column names consistent with janitor pct_ib &lt;- read_csv(&quot;data/wipo2022/pct_ib.csv&quot;) %&gt;% janitor::clean_names() # replace junk on import pct_names &lt;- names(pct_ib) %&gt;% str_replace(., &quot;x&quot;, &quot;&quot;) names(pct_ib) &lt;- pct_names # convert from wide to long format pct_ib &lt;- pct_ib %&gt;% pivot_longer(., cols = 5:32, names_to = &quot;filing_year&quot;, values_to = &quot;n&quot;) %&gt;% drop_na(n) load(&quot;data/patent_counts/pct_ib.rda&quot;) We are now in a position to start creating basic models for our data. We will start with a technique that we have already seen in Figure 4.11 above, with the application of a linear model to find the line of best fit between a set of data points using linear regression. Linear regression involves estimating the relationship between what is called a dependent or response variable and one or more independent or predictor variables wikipedia. In R and other software packages, the ability to create simple linear models is commonly built in because their use is so common. In R we can use lm() to create linear models of various types. We can get a glimpse of our PCT filings to the International Bureau data in Table 4.13. Table 4.13: PCT applications with the International Bureau filing_year n 1995 1154 1996 1502 1997 1638 1998 2142 1999 2127 2000 2069 We then create a set of plots for this data and create two linear models, one with simple lm() and one with loess smoothing as in the code below. The popular R graphing package, ggplot includes methods for displaying linear models as part of graphics. We will create the models directly below. To move through this process we show four panels in Figure 4.19. In the first panel we see a standard scatter plot. In the second panel we show a line graph that joins the dots. In the third panel we apply a linear model that projects a straight line through the data as ‘the line of best fit’. In the fourth panel we use a form of linear model called loess for what is variously called “locally estimated scatter plot smoothing” and Local Polynomial Regression Fitting. Figure 4.19: Plotting PCT Applications to the International Bureau In Figure 4.19 we can see that the line plot in panel 2 simply traces a straight line between the data points found in Panel 1. This is the type of graph we commonly see in patent analytics. Panel 3 models a straight line of best fit that passes through the data points. Note how this line reveals positive and negative variation in the data points exposing dips and crests. In Panel 3 the line of best fit is calculated using the least square method that seeks to mininimse the square of errors between the data points in order to find the best fit. In practice, this focuses on minimising what are called the resiudals between the observed values and the value predicted by the model. Residuals are another name for the standard deviation from the mean. We can create our linear model directly in R using the lm() function. Here for convenience we use the modelr (Wickham 2022) package to create the linear model and loess model. We start with the lm() model which uses Ordinary Least Squares (OLS) to fit the data by modelling the relationship between the predictor variable (in this case the filing year) and the response value (in this case n for the number of application years to the International Bureau). library(modelr) # filing year as numeric pct_ib &lt;- pct_ib %&gt;% select(filing_year, n) %&gt;% mutate(filing_year = as.integer(filing_year)) # create a model pct_model &lt;- lm(n ~ filing_year, data = pct_ib) pct_fitted &lt;- pct_ib %&gt;% add_predictions(pct_model) %&gt;% add_residuals(pct_model) pct_plot &lt;- pct_fitted %&gt;% ggplot(aes(x=filing_year, y=pred)) + geom_line() + geom_point(aes(y=n)) + labs(title=&quot;1. Prediction&quot;, y=&quot;Applications&quot;, x = &quot;Filing Year&quot;) + scale_x_discrete(breaks=seq(1995, 2021, 5)) pct_resid &lt;- pct_fitted %&gt;% ggplot(aes(x = filing_year, y = resid)) + geom_point() + labs(title=&quot;2. Residuals&quot;, x = &quot;Filing Year&quot;) pct_compare &lt;- ggplot(pct_fitted, aes(x = filing_year, y = n)) + geom_segment(aes(xend = filing_year, yend = pred)) + geom_point() + geom_point(aes(y = pred), shape = 1) + labs(title=&quot;3. Predicted and Actual&quot;, y = &quot;Applications&quot;, x = &quot;Filing Year&quot;) (pct_plot + pct_resid) / pct_compare An important feature of regression models and the description of these models is that the data should be normally distributed (that is evenly distributed) around the mean. In reality, a lot of real world data is not normally distributed and displays considerable variation such as outliers. In other words, real world data is often skewed in a variety of ways. In many cases this may not actually matter. Where it does matter is if a specific regression test assumes or requires normality. Where plotting in this way, notably of the residuals, really helps us is in appreciating the scale of variation. Thus we can observe a trough in 2000 and a peak of filings in 2005-2007, prior to a smaller trough followed by a larger trough in 2016 to 2017. The reader will perhaps already have a sense of the reason these troughs might exist. Other popular ways of visualising this data is in a histogram (see below). For more detailed analysis than is offered here the summary of the model data is an important starting point for the application of a range of different tests and methods that can be used to both evaluate different models. As a general rule of thumb the a better model is one with lower residuals and error scores. summary(pct_model) ## ## Call: ## lm(formula = n ~ filing_year, data = pct_ib) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1811.45 -730.79 0.41 767.45 1982.34 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -945607.95 55754.27 -16.96 3.17e-15 *** ## filing_year 474.74 27.77 17.10 2.63e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1124 on 25 degrees of freedom ## Multiple R-squared: 0.9212, Adjusted R-squared: 0.9181 ## F-statistic: 292.3 on 1 and 25 DF, p-value: 2.631e-15 4.8.2 Loess Smoothing We can appreciate from this that the standard linear model is not doing a great job because of the distribution of the residuals. Our second model, Loess smoothing or Local Polynomial Regression Fitting, uses weighted least squares with subsets of the data from local neighbours to build up the model. Historically, this was more computationally intensive than the standard lm model but is now trivial to calculate. In the code below we follow the same process that was used above to create and then plot a model using the loess method. In Figure 4.20 we show the prediction from using Loess smoothing along with the residuals and the actual vs. predicted values from the model. pct_ib &lt;- pct_ib %&gt;% select(filing_year, n) %&gt;% mutate(filing_year = as.integer(filing_year)) # create a model pct_loess &lt;- loess(n ~ filing_year, data = pct_ib) pct_l_fitted &lt;- pct_ib %&gt;% add_predictions(pct_loess) %&gt;% add_residuals(pct_loess) pct_l_plot &lt;- pct_l_fitted %&gt;% ggplot(aes(x=filing_year, y=pred)) + geom_line() + geom_point(aes(y=n)) pct_l_resid &lt;- pct_l_fitted %&gt;% ggplot(aes(x = filing_year, y = resid)) + geom_point() pct_l_compare &lt;- ggplot(pct_l_fitted, aes(x = filing_year, y = n)) + geom_segment(aes(xend = filing_year, yend = pred)) + geom_point() + geom_point(aes(y = pred), shape = 1) (pct_l_plot + pct_l_resid) / pct_l_compare Figure 4.20: Loess Smoothing Results summary(pct_loess) ## Call: ## loess(formula = n ~ filing_year, data = pct_ib) ## ## Number of Observations: 27 ## Equivalent Number of Parameters: 4.39 ## Residual Standard Error: 748.4 ## Trace of smoother matrix: 4.81 (exact) ## ## Control settings: ## span : 0.75 ## degree : 2 ## family : gaussian ## surface : interpolate cell = 0.2 ## normalize: TRUE ## parametric: FALSE ## drop.square: FALSE On balance in comparing the results of the two models we can see that the Loess smoothing does a better job of fitting to the data because the pattern of the residuals exhibits a narrower range and is closer to zero across the range with the exception of break points representing troughs at 2000, 2008-2011 (to a lesser degree) and 2016-2017. In addition, as the general rule of thumb is to minimise the error we would in this case prefer the lower Residual Standard Error (RSE) of 748.4 from Loess over 1124 from the standard lm model. In practice, these are only two of the linear models that can be applied to data using R’s lm function (which uses QR decomposition as the method). Other available methods include: robust linear model (rlm), general linear model (glm) and generalized additive model (gam). Some of these, notably gam, come in a variety of flavours and allow you to test and create models for the best fit for your particular data. Our purpose here has been to introduce these possibilities, and the basic means available for understanding their outputs. As we can see, key features of the use of these models are the analysis of residuals and a range of statistical tests to facilitate comparison between the results produced by different models. We have just scratched below the surface of these models. However, the use of simple linear models is enough of an introduction to grasp some of the key issues involved in one of the most important uses of statistical models: forecasting. 4.9 Forecasting To conclude this chapter we will briefly introduce forecasting techniques using a fictional situation. Forecasting the future involves a combination of values for time (year, quarter, month, day etc.) and observations for a variable where you want to predict future values for that variable over a time horizon. Forecasting using time series data matters in many different areas of life because predictions about the future can be incorporated into planning. A great deal may be at stake, for example when trying to predict the stock market or model patient admissions to hospitals during a pandemic. As we will see below, forecasting is normally based on past observations of a particular variable and the application of statistical methods, notably smoothing, rolling averages and tests for various forms of error as the basis for modelling potential future values of a variable. The general objective of these methods is to make the model fit existing data while minimising errors as the basis for predicting potential future values. These potential values are commonly predicted across a range based on values falling within the 80% to 95% probability. A range of methods and models are available for forecasting. We will use three in our simple fictional case study below: naive, ETS and ARIMA. We will use the freely available Forecasting: Principles and Practice (3rd ed) by Rob J Hyndman and George Athanasopoulos from Monash University as our guide throughout the discussion below. The introductory example we present below is adapted from examples in their book. We strongly encourage anyone interested in learning about forecasting to read this excellent book. Forecasting: Principles and Practice produces forecasts in R using the tsibble (a tibble data frame for time series data) and fable packages in R. A Data Camp course on Forecasting in R is available led by Rob Hyndman using the predecessor package to fable known as forecast. For readers who are more comfortable using Python, the models used for forecasting, such as ARIMA, are also available in Python. A number of books are also available for time series forecasting in Python that you may wish to explore. However, the author recommends Forecasting: Principles and Practice because of its clarity of exposition for readers who may not initially feel comfortable approaching the maths involved. Our purpose in this section is to encourage you to engage with the literature on forecasting in either R or Python. We will do this using a fictional example of a crisis at WIPO. 4.9.1 Day of Days You stroll casually into your office at WIPO headquarters in Geneva with your morning coffee to be greeted by your boss pacing the floor: he is looking anxious. The Director General (universally known as “the DG”) is demanding to see projections for filings of Patent Cooperation Treaty patent applications with the International Bureau (WIPO) for the next five years. Whenever a Patent Cooperation Treaty application is filed with WIPO it attracts an international filing fee. In 2022 that filing fee was 1330 Swiss Francs (CHF), or around 1387 US dollars. As the application moves through the procedure a range of other fees may be charged for other services (such as search reports). Patent fees are a major source of revenue for WIPO and so the ability to predict potential revenue for future years has big implications for the budget. Your boss looks you in the eye and explains: “There is a crisis, the DG needs the figures on future filings with the International Bureau over the next five years. Can you do it?” You take a sip of coffee and respond with a question: “I thought we had a Chief Economist and a bunch of economists for that?”. Your boss looks impatient. “Yes, yes, yes… but the Chief Economist and all of his staff are all away at a conference on a tiny island in the South Pacific where there is no internet.” Sensing an important opportunity lurking in the vicinity you say: “I’ll go! I volunteer to take the message and a satellite phone and will ring you with the answer when I get there! I’ll book my ticket right now”. Your boss looks at you with a knowing smile and a shake of the head and says: “There is no time, the DG needs the answer in three hours”. Crestfallen at missing the chance to sip a cocktail on a beach in the South Pacific with no internet you sit at your desk and contemplate your future employment prospects. Where to start? 4.9.2 The Data WIPO makes its patent statistics available as part of the intellectual property statistics section of its website through the WIPO IP Statistics Data Center. Under the PCT section of the data center we are able to select a range of indicators. We select PCT applications by filing date and Yearly Statistics as the Year Type (note that monthly is also available and we may want to grab those as well). A range of different offices around the world can receive Patent Cooperation Treaty (PCT) applications because it is the international patent instrument for submitting applications outside ones home country. We want the number of applications submitted directly to WIPO as the International Bureau (IB) and so we choose the International Bureau as the Receiving Office. As we have very little time we decided to choose just the Total as the Origin Office. We can see how to obtain this data in Figure 4.21 below. Figure 4.21: Selecting PCT Applications to the International Bureau When we press Search we see the data that we are after in Figure 4.22. Figure 4.22: WIPO Patent Statistics Data format Seeing the data immediately tells us something about the task ahead. The data is in wide format (the easiest for a human to simply print out and read): we will want this data in long format with year as the column heading. Having downloaded the data we start to think about importing into R or Python. However, it turns out that there are some extra headings at the top of the file (which we can normally skip), but more importantly that the heading for the Receiving Office and the Origin columns are on a different rows to the year. That requires a frustrating manual fix. When we import the data we discover that the year columns are preceded by an x for reasons that are completely unclear. We make the adjustments and pivot the table to long format. For 2022 the value is not available (NA) and so we simply drop it. We can see the cleaned up data in Table 4.14 Table 4.14: Tidied WIPO data in long form filing_year n 1995 1154 1996 1502 1997 1638 1998 2142 1999 2127 2000 2069 Forecasting in R relies on the use of time series objects that can be created in various ways such as the inbuilt (base) ts() function. However, we will be using the recent time series tsibble and fable packages for our data. This requires that we enter our year as a date. At first sight, that is not a problem because a year is a date, right? No, the four digits for a year are not a date in R (and cannot be), a date is something like YYYY-MM-DD sometimes with hours minutes and seconds. It is possible to try various ways of making four digits into a year but they will not work without making the string conform to what R expects a date to look like. Time is short and so we exploit the tsibble package to simply write in our years directly. We then pass in the counts of the number of filings (pct_id$n) and the year as the time series index. Note that if using this kind of approach it is very important to check that your year range is complete or this will go badly wrong. The life of a data scientist is often not an easy one when wrangling data, however, we are now in a position to plot the data using the autoplot function as in Figure 4.23. library(tsibble) library(fable) pct_ts &lt;- tsibble( year = 1995:2021, filings = pct_ib$n, index = year ) pct_ts %&gt;% autoplot(filings) Figure 4.23: PCT Filings with WIPO Forecasting involves a standard procedure that is illustrated in Figure 4.24 and reproduced from Forecasting: Principles and Practice (3rd ed). Figure 4.24: Forecasting Procedure We are presently at step 1: evaluating our data. The key issues to be considered in understanding our time series data are is the data trending (yes) and does it involve seasonality or cyclicity. Seasonality is common in time series data, such as purchasing products at particular times of year. Because our data is yearly it cannot be seasonal. Cyclicity refers to whether our data is affected by business cycles. This is harder to assess. While the submission of patent applications to the International Bureau is clearly trending upwards, there are some very notable peaks, flat spots and troughs. In the author’s view these are likely to reflect global economic conditions, such as the bursting of the ‘dot com’ bubble in the late 1990s, the financial crisis in 2007-2008 and the stock market crash of 2015-2016. We may be tempted to think of a cycle as a regular predictable series of events. However, in reality cyclicity is represented by irregular events of varying duration. What we discern from our quick time series plot is that our data is clearly trending and is not seasonal. However, it does display cyclicity and this will be reflected in a skew in the distribution of the data. It is possible to apply various transformations to the data to address this such as the Box-Cox Transformation or differencing to stabilise the data. However, this can rapidly become quite involved and the clock is ticking (see below). We make a mental note to highlight this issue in bold in the presentation of the data. Using the fable package it is remarkably easy to fit different forecasting models and compare the results with straightforward code. As we are in a rush we will use three models: Naive. The naive model simply takes the last data point and projects it into the future. The naive model therefore assumes that the last observation will continue into the future. ETS is Exponential Smoothing. As Hyndman and Athanasopoulos explain: “Forecasts produced using exponential smoothing methods are weighted averages of past observations, with the weights decaying exponentially as the observations get older.” You can read more about exponential smoothing methods in Chapter 8 of the Forecasting: Principles and Practice (3rd ed) ARIMA stands for Autoregressive Integrated Moving Average and, along with ETS, is one of the most popular sets of forecasting models. The model is autoregressive because it uses past observations to predict the future, and in this case as the name implies, it does this using a lagged moving average. We start by fitting the models to our data. This is extremely easy to do. library(tidyverse) library(fable) fit &lt;- pct_ts %&gt;% model( naive = NAIVE(filings), ets = ETS(filings), arima = ARIMA(filings) ) fit ## # A mable: 1 x 3 ## naive ets arima ## &lt;model&gt; &lt;model&gt; &lt;model&gt; ## 1 &lt;NAIVE&gt; &lt;ETS(A,A,N)&gt; &lt;ARIMA(0,1,0) w/ drift&gt; After fitting the models we can access a range of statistics with the general aim of identifying the model with the lowest error scores across a range of measures. In particular, when creating several models there are standard measures for comparison that focus on calculation of types error. Of the available measures Hyndman and Athanasopoulos generally recommend the second order AICc (a variant of the Akaike Information Criterion or AIC). Other options include the Bayesian Information Criterion (BIC). We can see below that across three types of information criteria the ARIMA model comes out best (note that some scores are not available for the different models). We can see these scores in Table 4.15. fit_glance &lt;- fit %&gt;% glance() fit_glance %&gt;% knitr::kable(caption = &quot;Comparative Model Scores&quot;) Table 4.15: Comparative Model Scores .model sigma2 log_lik AIC AICc BIC MSE AMSE MAE ar_roots ma_roots naive 509955.5 NA NA NA NA NA NA NA NULL NULL ets 609713.7 -222.1592 454.3185 457.1756 460.7976 519385.7 1240281 516.8461 NULL NULL arima 509955.5 -207.2296 418.4591 418.9809 420.9753 NA NA NA Based on these measures we cad reasonably choose the ARIMA model to present to the DG. However, before going ahead we realise that we are experiencing an uncomfortable itch from the variance in the data resulting from the financial crises. We can gain an understanding of this itch by inspecting the residuals. Residuals are simply the difference between the observed and predicted values. The residuals for the ETS model are displayed in Figure 4.25 and for the ARIMA model in Figure 4.26. library(feasts) fit %&gt;% select(ets) %&gt;% gg_tsresiduals() Figure 4.25: Residuals for the ETS fitted model library(feasts) fit %&gt;% select(arima) %&gt;% gg_tsresiduals() Figure 4.26: Residuals for the ARIMA fitted model In considering these plots, the residuals are what is left over after a model has been fitted and is made up of the difference between the observed and the fitted values. In assessing the residuals the innovation residuals should not be displaying evidence of a trend. For both models we do not observe evidence of trend although there is clear evidence of significant variance from the mean. The ACF (autocorrelation plot) displays the coefficient of correlation between two values in a time series with a lag of 1. The residuals fall within the boundaries represented by the dotted lines and we are happy enough with the lack of marked outliers in the residuals. So far, so good. However, the third plot in the residuals report shows the distribution of the data. This should be normally distributed. We can see that our data skews to the left with outliers to the right. The ARIMA model does a better job of distinguishing these outliers. There are a number of transformations that could be applied to this data to normalise it such as the use of differencing or a Box-Cox Transformation. These can be readily calculated in R using the fable and fabletools packages. However, note that transformations of data can be a subject of controversy. What are we going to tell the DG? Having inspected our data and being aware that it is rather skewed by cyclic events in the world economy we use our fitted models to produce a forecast. fc &lt;- fit %&gt;% forecast(h = &quot;5 years&quot;) fc %&gt;% head(20) %&gt;% knitr::kable() .model year filings .mean naive 2022 N(13508, 716113) 13508.00 naive 2023 N(13508, 1432226) 13508.00 naive 2024 N(13508, 2148339) 13508.00 naive 2025 N(13508, 2864452) 13508.00 naive 2026 N(13508, 3580565) 13508.00 ets 2022 N(14060, 609714) 14059.88 ets 2023 N(14612, 1219427) 14611.72 ets 2024 N(15164, 1829263) 15163.55 ets 2025 N(15715, 2439220) 15715.39 ets 2026 N(16267, 3e+06) 16267.22 arima 2022 N(13983, 509956) 13983.15 arima 2023 N(14458, 1e+06) 14458.31 arima 2024 N(14933, 1529867) 14933.46 arima 2025 N(15409, 2e+06) 15408.62 arima 2026 N(15884, 2549778) 15883.77 Having fitted the model we can plot the three forecasts as in Figure 4.27 fc %&gt;% autoplot(pct_ts) Figure 4.27: Three Forecasts In interpreting the forecast the three straight lines represent the mean for the forecast produced by each model with the 80% to 95% confidence intervals represented by the corresponding colours. We can clearly see that the ETS model produces the steepest projected forecast with the naive model projecting a straight line from the last data point. Based on the AICc and other scores we will choose the ARIMA model forecast and use the naive model as the as is example. We can easily drop the ETS model from the figure that we will include in the brief for the DG. fc %&gt;% filter(.model != &quot;ets&quot;) %&gt;% autoplot(pct_ts) Figure 4.28: As Is and ARIMA Forecasts The forecasts include the data on the number of projected filings and we can therefore project likely revenue from this data using the rough US$1387 fee value as our assumption. revenue &lt;- fc %&gt;% filter(.model != &quot;ets&quot;) %&gt;% tsibble::tibble() %&gt;% select(.model, year, .mean) revenue_asis &lt;- revenue %&gt;% filter(.model == &quot;naive&quot;) %&gt;% tsibble::tibble() %&gt;% select(.model, year, .mean) %&gt;% mutate(.mean = round(.mean)) %&gt;% mutate(forecast_revenue_us = .mean * 1387) %&gt;% rename(forecast_filings = .mean) %&gt;% janitor::adorn_totals() revenue_forecast &lt;- revenue %&gt;% filter(.model == &quot;arima&quot;) %&gt;% tsibble::tibble() %&gt;% select(.model, year, .mean) %&gt;% mutate(.mean = round(.mean)) %&gt;% mutate(forecast_revenue_us = .mean * 1387) %&gt;% rename(forecast_filings = .mean) %&gt;% janitor::adorn_totals() We can see the results in Table 4.16 and Table 4.17. Table 4.16: As is Revenue Forecast .model year forecast_filings forecast_revenue_us naive 2022 13508 18735596 naive 2023 13508 18735596 naive 2024 13508 18735596 naive 2025 13508 18735596 naive 2026 13508 18735596 Total 10120 67540 93677980 Table 4.17: ARIMA Revenue Forecast .model year forecast_filings forecast_revenue_us arima 2022 13983 19394421 arima 2023 14458 20053246 arima 2024 14933 20712071 arima 2025 15409 21372283 arima 2026 15884 22031108 Total 10120 74667 103563129 We will want to write a quick explanation of the models that we have produced and present the forecast numbers. The DG doesn’t like long documents, a page or two at most. We will present the naive model as a forecast for filings and revenue assuming that last years filings continue into the future with no change. This can serve as a useful baseline of what might reasonably be expected. We use the ARIMA model as our actual forecast of filings and revenue which shows a steady upward trend. The difference between total revenue under the ‘as is’ and the ARIMA forecast over the five year period is US$9,885,149. In the note we ensure that we are transparent about the uncertainty created by the skew in the data and recommend that the economists are set to work on it. There is a knock at the door. It will be the boss. Our two page print out is on the printer. You gulp… pick up the forecast and head off to meet your fate. Global stock markets crash the next day. 4.10 Conclusion This chapter has provided a wide ranging practical introduction to patent counts and descriptive patent statistics that concluded with a brief introduction to forecasting. Of all of the chapters in this handbook this is the most important. Arriving at accurate counts of patent data is not straightforward. This chapter has taken you through those issues. We concluded with a brief introduction to linear regression by exploring commonly used linear models and their application to patent data before moving on to exploring ways to generate forecasts and the issues they involve. In closing it is important to emphasise that there is frequently more than one way to count patent data and the type of count that you use will depend on the question you are seeking to address, the data at hand and what you want to communicate to the audience. References "],["classification.html", "Chapter 5 Patent Classfication 5.1 Exploring the International Patent Classification 5.2 The US IPC Table 5.3 Assessing Relationships Between Technology Areas 5.4 Visualising Relationships with Chord Diagrams 5.5 The Short IPC 5.6 Classification and Patent Overlay Mapping 5.7 The Structure of Patent Activity 5.8 Conclusion", " Chapter 5 Patent Classfication This chapter focuses on the use of patent classification codes in patent analytics. Patent documents are normally awarded classification codes or symbols that describe the technical content of the document. For those unfamiliar with patent classification schemes the closest systems are library classification codes such as those used to order the items on the shelves of a university or local library. Classification systems are important because once we are familiar with them we can navigate to the right part of the library with ease to retrieve the relevant item. For example, in the international patent system documents relating to genetic engineering are commonly found under Subclass C12N while those that relate to plants in agriculture are under A01H and animals are found under A01K. In contrast we would find kitchen equipment under Class A47. As we will see in this chapter, in contrast with simple library classification codes, patent documents are normally assigned more than one code to more fully describe the technical content of a document. So, if we wanted to identify those patent documents that involve biochemistry and plant agriculture we could search for those documents that share Subclass C12N (genetic engineering) and plant agriculture (A01H) to identify those documents that are about both subjects. Patent classification systems are also hierarchical meaning that we can navigate to greater and greater levels of detail. The original purpose of patent classification systems was to facilitate the retrieval of patent documents as prior art through the physical ordering of patent documents in patent office libraries. As patent offices expanded classification was also used to assign work to relevant sections of examining teams (e.g. Chemistry, Electrical Engineering etc.). As such, patent classification is important to the organisation of the work of patent offices. Increasingly, in the context of the rise of machine learning and artificial intelligence, patent offices are exploring the possibility of automating at least part of the time consuming task of patent classification as part of the organisation of their work. For patent analytics it is important to recognise that the use of classification codes has changed over time and continues to be dynamic. Thus, initially patent documents were awarded one classification code focusing on description of the contents of the claims. The number of codes assigned to a documents was gradually expanded with the first in the list generally being the most important with respect to the contents of the claims. In the early 2000s this began to change with the focus shifting towards the use of multiple classification codes to describe the technical content of patent documents. Patent classification is also dynamic as specialist bodies responsible for classification respond to changes in technology and issue revised editions of classification schedules and take advantage of databases to retrospectively reclassify entire collections. This means that old assumptions that the first classification code on a document was the most important is no longer true and that analysts must pay attention to revisions to classification schemes. For patent analysts understanding patent classification systems is fundamentally important for four main reasons: When combined with keywords it provides powerful tools for narrowing patent searches and dataset generation; It allows for trends in particular areas of science and technology to be identified for statistical purposes; It allows for networks of co-occurrences of classification codes to be mapped to allow the components of landscapes to be identified and explored; combined with citation analysis patent classification may allow the identification of the main paths or trajectories of emerging areas of science and technology. There are a number of patent classification systems that are in use. In some cases, such as the patent office of Japan, a national classification system will be used. These national systems can help analysts to navigate the particularities of individual systems. However, for international patent research the two most important classification systems are: The International Patent Classification (IPC) administered by WIPO and established by the 1971 Strasbourg Agreement.17. The IPC consists of over 74,000 classification codes.18 The Cooperative Patent Classification (CPC) operated by the European Patent Office and USPTO. The CPC is based on the earlier ECLA system and can be considered to be a more detailed version of the IPC using just over 260,000 classification codes 19. The CPC is also used by a number of EPO national offices, the Chinese State Intellectual Property Office (SIPO), the Korean Intellectual Property Office (KIPO), the Russian Federal Service for Intellectual Property (Rospatent) and the Mexican Institute of Industrial Property (IMPI).20 One challenge for patent analysts who have not received training in patent examination is that there are limited resources available for training in the use of the patent classification schemes. Essential resources for the IPC include the WIPO Guide to the International Patent Classification (2022) available in English, French and Spanish and regularly updated. For the CPC it is important to familiarise oneself with the CPC website and its section with free training resources. esp@cenet also provides a very valuable Cooperative Patent Classification search tool. The purpose of this chapter is to introduce the reader to the most important features of patent classification for the purpose of patent analytics. We will use the US PatentsView ipcr table that is available free of charge from the following link http://data.patentsview.org/20190820/download/ipcr.tsv.zip to explore the uses of the classification. We will then examine the use of co-occurrence network analysis using the WIPO Patent Landscape Report on Animal Genetic Resources to illustrate how an understanding of the relationships between classification codes can be used to make sense of very noisy data. Finally we will consider recent developments focusing on the use of vector space models to assist with patent classification based on a dynamic Lens public collection of articles on patent classification. 5.1 Exploring the International Patent Classification The International Patent Classification in 2019 consisted of over 74,000 classification codes consisting of alphanumeric codes. The IPC is hierarchical in nature moving from the most general level (Section) to the most detailed level (Subgroup). The IPC is ordered in the sections displayed in Table 5.1. Table 5.1: The Sections of the IPC Section Description A HUMAN NECESSITIES B PERFORMING OPERATIONS; TRANSPORTING C CHEMISTRY; METALLURGY D TEXTILES; PAPER E FIXED CONSTRUCTIONS F MECHANICAL ENGINEERING; LIGHTING; HEATING; WEAPONS; BLASTING G PHYSICS H ELECTRICITY Each Section of the IPC contains a set of codes on the Class, Subclass, Group and (where relevant) the Subgroup level. An illustration of a full classification code is A01H5/10 which is built from the following hierarchy. A - Section = Human Necessities 01 - Class = AGRICULTURE; FORESTRY; ANIMAL HUSBANDRY; HUNTING; TRAPPING; FISHING H - Subclass = NEW PLANTS OR PROCESSES FOR OBTAINING THEM; PLANT REPRODUCTION BY TISSUE CULTURE TECHNIQUES 5/00 - Group = Angiosperms, i.e. flowering plants, characterised by their plant parts; Angiosperms characterised otherwise than by their botanic taxonomy 5/10 = Subgroup = Seeds In the example above we can appreciate the basics of the hierarchy in constructing the classification code A01H5/10. The first point to note is that the classification can be difficult to read in a normal human way. In approaching this task it is generally best to start at the lowest possible level to construct a human understandable summary description such as the use of seeds from angiosperms to create new plants. The second point to note is that it is important when seeking to interpret these codes to review the relevant entries in the online classification tool to understand what is nearby in the classification and the relevant notes. Notes in the IPC provide guidance to examiners on the appropriate place to classify a document. By way of illustration consider the following under A01H in Table 5.2. Table 5.2: New Plants in the IPC Code Description A01H NEW PLANTS OR PROCESSES FOR OBTAINING THEM; PLANT REPRODUCTION BY TISSUE CULTURE TECHNIQUES [5] A01H 1/00 Processes for modifying genotypes (A01H 4/00 takes precedence) [2006.01] A01H 3/00 Processes for modifying phenotypes (A01H 4/00 takes precedence) [2006.01] A01H 4/00 Plant reproduction by tissue culture techniques [2006.01] A01H 5/00 Angiosperms, i.e. flowering plants, characterised by their plant parts; Angiosperms characterised otherwise than by their botanic taxonomy [2018.01] A01H 5/02 • Flowers [2018.01] A01H 5/04 • Stems [2018.01] A01H 5/06 • Roots [2018.01] A01H 5/08 • Fruits [2018.01] A01H 5/10 • Seeds [2018.01] A01H 5/12 • Leaves [2018.01] When viewed from the fuller IPC we can make a number of observations. The first of these is that A01H5/10 describes the part of a plant used in the claimed invention. Second, we can see instructions to examiners to the effect that code A01H4/00 takes precedence (priority) over the use of either A01H1/00 or A01H3/00 with respect to modifications of genotypes and phenotypes respectively. In our human readable summary of this data we can describe this as the use of seeds from angiosperms to create new plants because A01H4 refers specifically to plant reproduction by tissue culture techniques and should be present on documents where tissue culture techniques are used. As this suggests, when approaching analysis of a particular area of the patent system it is useful to begin by consulting the IPC and the codes around a potential area of interest to work out which codes might need to be included in the construction of a search to capture the universe of activity. Three additional observations are important here. As we can see above the year and IPC edition appear in brackets after the code entry. For items under A01H5/00 note we can see that this is a 2018 addition to the classification and, depending on the year the updated IPC was released (e.g. 2022), may not yet be fully reflected in patent databases as a result. Increasingly, patent documents in major databases are reclassified shortly after the adoption of new editions of the IPC. However, it is worth bearing a potential lag time in mind. Some classification codes are essentially descriptive rather than being about a technology. Thus, A01H5/00 describes parts of plants, C12R describes microorganisms and A61P describes diseases/medical conditions. In contrast, A01H4 and A01H3 describe technologies or methods. When constructing patent searches with classification codes care is required with respect to the use of five character (e.g. A01H5) or eight character strings (e.g. A01H5/00) because different results may be retrieved depending on the database used. Thus, the use of A01H5 in conducting a search should capture everything that appears under A01H5. However, a search for A01H5/00 will capture only documents literally coded as “A01H5/00”. As noted above, patent databases may also vary in terms of which edition of the classification they are using at a given time. 5.2 The US IPC Table The USPTO makes its patent data available free of charge in table format for download from the Patents View website https://www.patentsview.org/download/. This is an excellent service and it is recommended that anyone interested in patent analytics should engage with this fundamental resource. In this section we will explore the 2019 IPC table for granted patents and you should expect that numbers will change if you use the latest edition. The first observation to make is that the 2019 IPC table consists of 15,190,916 rows and 6,625,659 unique patent documents.21 As these numbers suggest on average each document receives 2.2 classification codes. A single patent document may also appear in more than one area of the classification (e.g. Agriculture and Biochemistry). We will explore this in more detail below. Table 5.3 displays the number of occurrences of codes in each section (the same code may appear multiple times) and a count of the number of documents in each section. Note here that a single document may appear in more than one section (e.g. A and C) but is counted only once in each section. We can readily see that the most frequently occurring classification codes across all years of the US data are in Electricity (H) and Physics (G) followed by Performing Operations and Transporting (B). As we will see below some areas of the classification (notably Physics and B for Performing Operations and Transporting) may commonly co-occur with documents in other areas of the classification. Table 5.3: The International Patent Classification Sections section Description document count ipc count A HUMAN NECESSITIES 1034299 2049387 B PERFORMING OPERATIONS; TRANSPORTING 1219010 2168833 C CHEMISTRY; METALLURGY 833582 1852281 D TEXTILES; PAPER 221211 268316 E FIXED CONSTRUCTIONS 196842 327027 F MECHANICAL ENGINEERING; LIGHTING; HEATING; WEAPONS; BLASTING 587207 1072005 G PHYSICS 1955738 3563913 H ELECTRICITY 1727199 3887862 This data helps to demonstrate that multiple classification codes will be applied to a more limited set of documents. One way to think about this is as a cloud of classification codes that surround documents in order to describe their contents. Those codes will form create clusters depending on the number of documents that share a code. One challenge with the IPC is how to visualise it at scale. Here we can use more recent visualisation techniques that will allow us to understand structure and to visualise relationships between elements within the structure. Figure 5.1 displays the USPTO IPC data in a static sunburst diagram. The purpose of this static image is to familiarise you with what you are looking at before going dynamic. To read this diagram, start in the centre which contains the entire IPC and its occurrence count for US granted patents. The next circle from the centre is the section level followed by the class level. While this image is static, we will switch to interactive versions below. The code used to generate this graph is drawn from the sunburstR package developed by Mike Bostock and collaborators. The sunburstR package implements the original D3 code developed by Mike Bostock and uses the sunburst code example developed by Kerry Roden https://bl.ocks.org/kerryrodden/7090426. Figure 5.1: Visualising the IPC Hierarchy in a Sunburst Diagram The sunburst graph displays the IPC in the central blue circle with 14,744,152 occurrences of classification codes from the US collection. The next circle is the section level beginning with A at 12 o’clock and proceeding clockwise to H (electricity). The third circle is the class level with the size of each class reflecting the number of occurrences. The fourth circle moves to the subclass level (e.g. A61) and the outer circle is the group level (e.g. A61K31). This type of visualization is useful for understanding the proportion of patent activity in terms of the use of classification codes. Thus, moving clockwise we can see that A, B and C are roughly proportional at the Section level. However, D (textiles and paper) and E (fixed constructions) are striking in how small they are relative to the rest of the classification. In contrast Sections G (Physics) and Section H (Electricity) are roughly proportionate and larger than sections A to C. Figure 5.2: Navigating Section A to the Group Level Figure 5.2 reveals that we can navigate across the hierarchy down to the group level. In this case from Figure 5.1 we observe a high proportion of activity in section A just after 12 o’clock that falls under group A61K31. We have now oriented ourselves in using this type of visual. The growing power of interactive visualisation tools such as the D3 JavaScript library also allows us to navigate into individual sections to explore activity. Figure 5.3 zooms in to subclass A01H down to the subgroup level discussed above. This version is interactive and allows the main subclass for plant agriculture (A01H) to be explored down to the subgroup level. Thus, we can readily observe that A01H5 for angiosperms (flowering plants) dominates activity in this subclass. The dominant subgroup is A01H5/00 which in reality is a reference to the group with the main subgroup being A01H5/10 for Seeds followed by the group A01H1 referring to processes for modifying genotypes. Observe here that the use of A01H1 and A01H1/00 both refer to the group level which is confusing. As noted above, for this reason it makes sense when conducting IPC searches to use A01H1 to ensure that all relevant results under this group are captured. Figure 5.3: Exploring Agriculture in the Patent Classification This type of 30,000 foot overview provides a better understanding of the proportion of activity in different areas of the United States Patent system than can be gained from a simple stacked bar chart. This also usefully illustrates the fundamental point that a great deal of time can be saved in patent analytics by engaging with the classification. That is, it makes sense when working on established technologies to start by using knowledge of the classification to navigate to relevant documents rather than conducting unrestricted searches. As we will discuss in further detail below, emerging technologies can prove more challenging. However, a great deal of effort can be saved by using the classification as a starting point. Thus, taking the example that we have used so far from plant agriculture and sequencing technology we might want to use this knowledge to conduct a classification based search that combines the two classifiers A01H and C12Q and as necessary to move to more detailed levels using A01H5 and A61Q1 or into relevant subgroups such as C12Q1/68 for nucleic acids (sequences). One factor to take into account when using the classification in a search strategy or the development of indicators, particularly where working with international patent activity, is that patent offices may classify to different levels of detail. For this reason it is often necessary to move up one level (or more) from the target and to test and refine the data as needed. As this suggests, rather than focusing on the IPC hierarchy in practice we will often want to focus on the linkages between areas of the classification. The increasing use of more than one classification code to describe the content of patent documents introduced under IPC8 represented a major step forward in this regard. 5.3 Assessing Relationships Between Technology Areas To assess relationships or linkages between different areas of the classification we need to cast the IPC data into a matrix that shows co-occurrences between areas of the classification. Co-occurrence is based on counts of the number of documents where one or more classification code or symbol occur together. It is a straightforward matter to create a co-occurrence matrix in Excel using a pivot table. VantagePoint makes it very easy to create matrices with the touch of a button in order to refine analysis while R and Python offer easy means to create such matrices with a small amount of programming knowledge. Table 5.4 shows a matrix of the cooccurrences of IPC sections (A to H) for USPTO patent grants across all years until 2021. In preparing this matrix, the data is first filtered so that only one IPC code is represented per section on each document. So, if an individual document has three codes from Section A and two Codes from Section B, only one section A and one section B code will be counted. We adopt this approach because our interest is in examining the relationships between sections rather than within them. In approaching Table 5.4 note that in creating the cooccurrence matrix we also remove self-references (that is, where section A occurs with section A) by converting the diagonal values to 0. For most purposes when constructing and visualizing a co-occurrence matrix or network you will want to remove these self references (known as removing the diagonal) for a clearer view. For example, in R this can be achieved with the following simple code diag(mymatrix) &lt;- 0. Table 5.4: IPC Cooccurrence Matrix on the Section Level, USPTO Patent Grants A B C D E F G H A 0 74793 184823 5205 10123 26051 86200 30268 B 74793 0 112054 12702 26170 90468 118864 87322 C 184823 112054 0 9845 8776 15670 55678 49571 D 5205 12702 9845 0 769 2285 2050 1689 E 10123 26170 8776 769 0 18668 15254 6863 F 26051 90468 15670 2285 18668 0 48185 44039 G 86200 118864 55678 2050 15254 48185 0 401008 H 30268 87322 49571 1689 6863 44039 401008 0 Table 5.4 reveals that if we read the values across the rows on the left, the top section shared with Section A is Section C for Chemistry and Biochemistry. Moving down the rows, one of the smaller areas of patent activity is Section F for Mechanical Engineering, Lighting, Heating and so on. The strongest connection from Section F is Section B for Performing Operations and Transporting. Rows and columns and numbers can be difficult to read and interpret. For this reason a common approach in data science for visualising a matrix is to use a heatmap. A heatmap transforms the values in the matrix into colours, where lighter colours normally represent lower values and darker colours higher values, or intensity, of connections or activity. Figure 5.4 displays a heatmap view of the data presented in Table 5.4. Figure 5.4: Co-Occurrence Heatmap for USPTO patent grants by IPC section Reading this heatmap column wise from A to H we can almost immediately see that the highest area of activity for Section A is Section C (for Chemistry and Biochemistry), for Section B it is split between Section D and Section F and for C it is section A followed by D (for Textiles and Paper). There are multiple ways in which heat maps can be constructed for different purposes to display relationships with Figure 5.4 is one of the simplest. For patent analysts heat maps can be particularly useful as exploratory devices for identifying the intensity of activity rather than as explanatory devices when communicating with a general audience. Network visualisations are typically a more effective device for visual communication with an audience as we will now see. 5.4 Visualising Relationships with Chord Diagrams We can visualize this co-occurrence network using a Chord Diagram or a network. Figure 5.5 displays the raw connections between US patent documents classified in more than one section of the IPC. The chord diagram was generated using the easy to use circlize package in R. Similar options available in the bokeh and plotly packages in Python and R. The circilize package and software programmes such as circos are widely used in bioinformatics and genomics for visualising relationships.22 Figure 5.5: Visualising an IPC Co-Occurrence Network in A Chord Diagram In considering this raw chord diagram we will begin at 12 noon with Section H and proceed in a clockwise direction. The most immediately striking aspect of this visualisation is that Section H (Electricity) is very strongly connected with Physics (Section G) but more weakly with other areas of activity. Section A (Human necessities) covers a broad spectrum from Agriculture to Baking and Foodstuffs, Clothing, Medicines and Games. Section A displays multiple but relatively weak links with Physics but strong links with Chemistry (a section including biochemistry and genetic engineering). Section B (Performing Operations; Transporting) encompasses a wide variety of technologies concerned with the separation and processing of materials (such as plastics), shaping of materials, printing and vehicles. This section also covers microstructural and nanotechnology. Perhaps as a reflection of this diversity and the importance of separation processes in many fields, Section B appears to display a more even distribution of linkages with other areas of patent activity. As we move past Section C (chemistry) we observe that two smaller areas of activity in terms of links with other areas of the system. These are Section E for Fixed Constructions covering Building, Road construction and Mining followed by Section F covering Engines, general engineering, weapons and blasting. When we compare the chord diagram in Figure 5.5 with our simple heatmap in Figure 5.4 we can observe that the chord diagram is more informative because it more clearly displays the proportions of the relationships between the different sections of the classification. However, note that the reader will need to be guided in interpreting each of these visualisations. When presenting these kinds of data visualisations the data can be presented as is or calculations can be applied. Figure 5.6 scales the data to show the fraction of activity in a section that is shared with other sections. Figure 5.6: IPC Co-Occurrence Network with Fractionated Counts We will read Figure 5.6 from 12 noon in a clockwise direction. The first point we observe at 12 o’clock is the inverted mountain of links between G (Physics) and H (Electricity) as we proceed along the axis of H a second string of linkage emerges to section G. Note however, that only limited connections are visible between H and other areas of the classification. Moving into Section A we initially observe a link to Section H before major links begin to appear to D (Textiles and Paper) and a separate major stream of links to C for Chemistry. It is possible to draw chord diagrams of this type at variety of different levels from classes down to sub-groups. Figure 5.7 shows a chord diagram on the subclass levels in order to illustrate two points. Figure 5.7: IPC Co-Occurrence Network on the Sub Class Level The first point is that when dealing with the relationships between tens of thousands of entities it will always be necessary to filter the data. In this case to codes with 10,000 or more occurrences on the class or the subclass level respectively. The second point follows from the first. That is, visualisation is an exercise in communication with an audience. Interpretation of the codes used as labels requires an intense interest in the symbols in the classification. While it is reasonable to expect that a patent analyst will be interested in these symbols it does not follow that the reader of a patent analysis will be interested. The issue here is not the type of visualisation. Chord diagrams are effective means for visualising relationships between categories within data. Rather the issue is with labelling. This is not confined to patent analysis but is a more general observation on visualisation, the challenge of labelling. In the case of the IPC one challenge is that code descriptions are generally long multi-phrase statements. In practice these descriptions were not created with visualisation in mind. One solution to this problem is to edit the IPC to short one word or two word phrases that seek to capture the dominant meaning of a particular code. This exercise will never be perfect but it provides a means of visually communicating data while at the same time providing opportunities to provide narrative explanations of the nuances to the reader. 5.5 The Short IPC The short IPC was developed by Paul Oldham and Stephen Hall to overcome the challenge of visualising IPC data in dashboard displays such as Tableau, notably labelled vertical bar charts. The short IPC was created by manually editing the complete classification in Vantage Point from Search Technology Inc in order to produce human readable labels on the section, class and subclass levels. An extended version of the short ipc includes ipc descriptions on the group level. Table 5.5 displays a sample of the short IPC. Table 5.5: The Short IPC code description level A01 AGRICULTURE class A01B AGRICULTURAL SOIL WORKING subclass A01C PLANTING subclass A01D HARVESTING subclass A01F THRESHING subclass A01G HORTICULTURE subclass A01H NEW PLANTS subclass A01J DAIRY PRODUCTS subclass A01K ANIMAL HUSBANDRY subclass A01L SHOEING ANIMALS subclass A01M CATCHING subclass A01N BIOCIDES, PESTICIDES AND HERBICIDES subclass A01P TYPE OF BIOCIDAL ACTIVITY subclass A21 BAKING class A21B BAKERS’ OVENS subclass We can join the short IPC to existing tables using straightforward SQL joins. In R with the dplyr package that looks like this. In this case we use an inner join so that only those records that match on the shared field (ipc_subclass and code in this case) are retained. We can also drop any columns that we don’t want and rename others (such as n). That code gives us a human readable version of the data as we see in Table 5.6. Table 5.6: Top Subclasses for USPTO Patent Grants ipc_subclass total description G06F 1101673 ELECTRIC DIGITAL DATA PROCESSING H01L 804560 SEMICONDUCTOR DEVICES A61K 630558 PHARMACEUTICALS/MEDICINES H04N 481881 PICTORIAL COMMUNICATION H04L 422068 TRANSMISSION OF DIGITAL INFORMATION A61B 408181 DIAGNOSIS H04W 317421 WIRELESS COMMUNICATION NETWORKS C07D 259314 HETEROCYCLIC COMPOUNDS G01N 221952 ANALYSING MATERIALS BY THEIR CHEMICAL OR PHYSICAL PROPERTIES B32B 191818 LAYERED PRODUCTS G02B 187885 OPTICAL ELEMENTS H04B 184749 TRANSMISSION G06K 179042 DATA RECOGNITION C12N 169212 BIOTECHNOLOGY/GENETIC ENGINEERING G11C 158372 STATIC STORES C07C 155484 ACYCLIC/CARBOCYCLIC COMPOUNDS H01M 141534 BATTERIES A61F 136140 BLOOD VESSEL FILTERS We will normally want to drop or rearrange columns but we are now in a position to visualise the data in a tool such as Tableau. Figure 5.8 displays a stacked bar chart in Tableau. Note that in some cases it may be necessary to edit longer labels manually, such as “Analysing materials by their chemical or physical properties” that becomes simply “Analysing Materials” in Figure 5.8 below. Figure 5.8: A Stacked Vertical Bar Chart using Short Subclass Labels The use of labels also allows for experiments with visualisation of the same data in other ways. Figure 5.8 displays the same data in the form of a bubble graph. Figure 5.9: A Bubble Graph using IPC Subclass data As this discussion reveals, there are a variety of ways of visualising IPC data. Some of these are best used for exploratory purposes for example using the IPC codes as labels while communication with an audience will normally involve the use of short text labels. In considering the stacked bar chart and bubble chart representations of the date presented above, or the raw and fractionated circle graphs presented earlier, it is always important to bear in mind what forms of visual communication will be effective for the audience in terms of accurate representation of the data. In closing this chapter on the use of the International Patent Classification in patent analytics we will illustrate how an in depth knowledge of the IPC combined with network visualisation techniques allowed the author of this handbook to identify and visualise a needle in a haystack. 5.5.1 Case Study: Finding A Needle in A Haystack In 2013 WIPO was invited by the Food and Agriculture Organisation (FAO) to develop a patent landscape analysis on animal genetic resources for food and agriculture. This landscape analysis covered all the major species of livestock such as cattle, pigs, goats, deer, horses, buffalo etc. as well as birds such as chickens, geese, ducks and so on. At first sight the solution to developing this landscape appeared relatively straightforward. The IPC has a long standing subclass AO1K for New Breeds of Animals that focuses on genetically engineered animals. This seemed like a reasonable starting point. For example, in the 2021 USPTO data on the IPC used above there were 31,712 patent grants across all years that fall under subclass A01K. However, during planning of the landscape study we were also conscious that this new breeds of genetically engineered animals is a fairly narrow category. Based on previous work on biodiversity in the patent system, we thought that other references to animals could be identified by using the Latin names of animals (e.g. Bos taurus for cattle). This was based on the observation that applicants with inventions that directly involve an animal will typically use the Latin name as part of the disclosure. This approach involved starting from a clearly relevant area of the patent classification (AO1K) and Latin terms and expanding outwards with a defined strategy to explore the wider landscape. However, one significant problem with this approach emerged in the course of exploring the data. This was that detailed analysis of the claims, performed in Vantage Point, revealed that for common livestock applicants often used common names. In addition the common names generally appeared in the claims as parts of lists that reflected the way in which claims are framed to maximise the scope of protection. Thus, cattle and other animals would appear in claims as part of lists encompassing mice, rats, humans and other mammals. Additional terms that expanded the landscape still further included grouped names such as bovines, ungulates and ruminants. Patent claims involving humans often involved references to higher order mammals. The practical consequence of this was a requirement to use common names to develop the landscape to be sure that the universe of relevant activity had been captured. Text mining of the US, European and PCT full texts revealed a raw set of 55,595 first filings of patent applications, 98,368 patent publications and 510,595 patent family members that involved the terms. Figure 5.10 displays the rankings for patent activity worldwide based on common and Latin terms for each of the major groups. Figure 5.10: Animals in the Patent System by Terms The scale of this data presented the conundrum of how to proceed considering that the use of animal names appears across the system in diverse areas of technology such as kitchen equipment, games, toys and clothing? In some cases, such as pigs and horses, the use of an animal name may be used in areas of technology that have nothing to do with the animal itself such as pipeline pigs and clothes horses. The answer to this question was found by combining the IPC and CPC for the 55,595 priority records and creating a co-occurrence matrix containing 41,167 codes consisting of 17,953 IPC codes and 34,472 CPC codes. The matrix was created in Vantage Point from Search Technology Inc and then exported to Gephi where it was laid out using the well known Fruchterman Reingold algorithm. Because the data formed dense clusters we then forced the network to expand to more clearly reveal clusters. A common method for identifying structure in raw data is to use approaches such as factor analysis on one or more data fields in an effort to identify clusters. This is built into tools such as Vantage Point. Clustering methods are important but one known issue with factor analysis and other methods such as k-means clustering is the extent to which the resulting clusters can be readily interpreted by humans. However, in this case the authors were able to rely on a large number of classification codes describing the content of the documents. In this case the authors were able to apply the modularity class community detection algorithm that comes built in to Gephi to identify communities or clusters of activity (Blondel et al. 2008). In straightforward terms, this involves iteratively calculating the strength of the links between nodes in the network and allocating nodes to a community (cluster) based on the strongest links until no further allocations can be made. Modularity class is used to partition the network by colouring the different clusters as communities. In common with other areas of patent analysis, network visualisation is an iterative process. Figure 5.11 shows details of the emerging network with the colours representing communities of closely related activity and the IPC codes overlaid to help interpret clusters of activity. Figure 5.11: The Emerging Animal Genetics IPC Network in Gephi Figure 5.12 shows the results of forcing the clusters to separate out to provide a clearer understanding of network structure. Figure 5.12: The Universe of Patent Activity for Animal Genetics In reading Figure 5.12 note that clusters that are less closely related to other clusters, such as 567 Cooking Equipment, are forced to the outside of the network while those with stronger links are more central. Modularity classes within a network created with Gephi are awarded a number (as shown above) that appears in the underlying data table as we can see below in what became the biotechnology cluster in the Gephi data laboratory (showing node and edges tables). Table 5.7 displays the top ten members of class 0. Table 5.7: Modularity Classes in Gephi id label timeset nodeweight modularity_class weighted degree degree 13753 A61K 38/00 NA 11441 0 133354 1844 13754 C12N 15/09 NA 7944 0 120580 1380 13755 C12Q 1/68 NA 6586 0 69583 906 13756 C12N 5/10 NA 5242 0 84578 1031 13757 C07K 14/47 NA 4736 0 51915 664 13758 G01N 33/53 NA 4680 0 59522 810 Each of the clusters in the network were manually reviewed to determine the appropriate labels in Figure 5.12 based on the frequency of the occurrences of IPC/CPC codes (node weight is a count) . As Figure 5.12 makes clear the landscape for animal genetic resources was considerably larger than would have been suggested by a focus on activity under A01K. Co-occurrence analysis combined with community detection allowed the structure of activity to be revealed. This provided a basis for the detailed exploration of individual clusters to a high level of detail through successive rounds of community detection within each cluster. This can be illustrated through two examples. Figure 5.13 shows the communities detected within the Animal Breeding cluster dominated by subclass A01K (genetically engineered animals). Figure 5.14 focuses in on a subcluster within the biotechnology cluster focused on growth hormones and blood coagulation factors. Figure 5.13: Animal Breeding Cluster Figure 5.14: Animal Biotechnology Cluster The important point about this approach is that the use of the classification to create a co-occurrence network when combined with a community detection algorithm allowed the main structural components of activity across 55,000 documents to be structured and explored in detail. As this example suggests, network analysis at the level of the classification is a powerful tool for exploratory data analysis when working with noisy search terms that may be used in multiple areas of the patent system. This is particularly relevant when seeking to navigate the noise involved in full text analysis of patent data. An important insight from this example when planning patent or wider literature analysis is to investigate whether a classification system is available before jumping towards k-means clustering, factor analysis or other methods. In the case of the patent system this is particularly important because the International Patent Classification system is a global human expert curated classification. Ignoring the IPC and the CPC is a mistake. Familiarity with the structure of the IPC combined with network visualisation allows the analyst to map and reveal the structure of patent activity in a way that makes it possible to organise communication on what are often complex topics. This does not mean that a patent analyst should slavishly adhere to the IPC, this would be a mistake in the case of emerging technologies that may be spread across multiple areas of the classification (as was originally the case for nanotechnology). Rather, the IPC is best seen as a starting point for data exploration and mapping before applying other methods. In the case study presented above, having identified the key elements of the structure of activity the authors were able to text mine in to specific clusters to explore the main features of patent activity in an organised way. 5.6 Classification and Patent Overlay Mapping In the preceding discussion we focused on the use of the classification to identify and explore specific areas of patent activity. However, a second area of research of relevance to patent analytics is efforts to map the wider structure of science and patent activity. The single most accessible resource for engaging with the literature on science mapping is the 2010 book by Katy Borner Atlas of Science: Visualizing What We Know that was linked to an exhibition entitled Places &amp; Spaces demonstrating different forms of science mapping for the general public (Boyack, Klavans, and Börner 2005). Other important work on science mapping is represented by work by Ismael Rafols, Alan Porter and Loet Leydesdorff to identify the structure of scientific research using a combination of Web of Science subject area labels, factor mapping at the journal level and co-citation mapping (Rafols, Porter, and Leydesdorff 2010; Klavans and Boyack 2009a). In particular, this research has revealed an emerging consensus on the structure of scientific research as reflected in the network of relations exposed by citations. This analysis reveals a torus like structure to the scientific publications. Figure 5.15 displays the original basemap developed by Rafols, Porter and Leydesdorff based on 221 Web of Science Categories and 18 Factors (Rafols, Porter, and Leydesdorff 2010). Figure 5.15: The Structure of Scientific Research As the authors explain, the structure of the map resembles a torus or donut like structure with Economics, Politics &amp; Geography at one side of the neck of the torus and Computer Science, Physics and Engineering on the opposite side. The lines (edges) in the structure represent the strength of connections between the different underlying subject categories with the labels in Figure 5.15 representing broad macro disciplines identified by the authors as aggregations from the factor mapping. What is important about overlay mapping is that it becomes possible to examine the distribution of publications by different organisations or funded by different research agencies in the map of science. Figure 5.16 displays the distribution of publications from four different universities overlaid on the map of science (Rafols, Porter, and Leydesdorff 2010). Figure 5.16: Four Universities situated in the Map of Science In comparing these university maps we can clearly see distinct clusters of research effort across the four institutions reflected in the subject categories of publications. This, the University of Amsterdam clusters in Clinical Medicine while the European Molecular Biology Laboratory (EMBL) clusters strongly on biomedical science. On contrast, Georgia Tech cluster onto Computer Science, Physics, Engineering and Maths, while the London School of Economics and Political Science (LSE) as a dedicated social science school logically clusters on Economics, Politics, Geography and other social sciences and humanities (e.g. Law) in the underlying map (Rafols, Porter, and Leydesdorff 2010). This type of analysis can also be extended to companies as we see in Figure 5.17 (Rafols, Porter, and Leydesdorff 2010). Figure 5.17: Company Positions in the Map of Science In the case of companies we also observe distinctive clusters of research outputs supported by companies. The power of this form of representation is reflected in the way that it readily confirms some of our underlying expectations. That is we would broadly expect Pfizer to display concentrations in biomedical sciences and clinical medicine while both Nestle and Unilever are more general food and personal goods manufacturers with diverse portfolios. Finally, Shell as an oil and gas company could be expected to display concentration of activity in the Geosciences and closely related fields. One challenge with the science overlay maps is that they depend on Web of Science journal categories and these change with reasonable frequency requiring reasonably regular updates to the maps (Carley et al. 2017). Alternative approach to science mapping include recording user clicks on scientific database websites rather than co-citation analysis. Research by Bollen et al 2009 used click stream data from nearly 1 billion user interactions to map the structure of scientific research as we see in Figure 5.18 (Bollen et al. 2009a). Figure 5.18: Map of Science based on Click Stream Data The map in Figure 5.18 uses an entirely different methodology to that used in previous work to map the structure of science. However, despite important differences, note that the torus structure (in this case inverted) is strikingly familiar while the distribution of subject areas also includes some striking similarities. An important feature of the development of science overlay maps is the use of open source software such as Pajek, Vos Viewer and Gephi and forms part of an increasing drive to transparent and reproducible methods. Researchers have also sought to extend science mapping into the patent system using patent classification systems. 5.7 The Structure of Patent Activity The increasing availability of patent data at scale such as the EPO World Patent Statistical Database and more recently the entire USPTO patent collection, has made large scale analysis of the structure of patent activity increasingly possible and reproducible by others. In 2012 Luciano Kay and collaborators including Nils Newman, Jan Youtie, Alan Porter and Ismael Rafols examined the cited to citing relationship between 760,000 EPO patent documents published between 2000 and 2006 involving 466 IPC codes (Kay et al. 2014). The focus of this research was on mapping technological distance. “Technological distance, or the extent to which a set of patents reflects different types of technologies, is a key characteristic in being able to visualize innovative opportunities” (Kay et al 2012 paraphrasing Breschi, Lissoni, &amp; Malerba, 2003). The argument in the literature here is that patents that cite others in the same or similar technology areas are likely to offer opportunities for incremental innovation while those that cite a variety of different technology areas are more likely to involve radical innovation (Kay et al 2012 citing Olson 2004). The IPC and other patent classifications are typically used to provide the proxy for technology categories where the citations between categories can be used as distance measures (applying various weighting techniques). The novelty of the approach to patent mapping deployed by Kay et al 2012 was to visualise the structure of patent activity using citing-to-cited relationships. The research focused on the use of IPC categories with 1000 or more records and involved a strategy of folding up groups and sub-groups with less than 1000 records to overcome the sparsity of the population of documents in some areas of the system and resulted in a total of 466 categories spanning different areas of the IPC down to the Subgroup level. This was followed by the extraction of the citing patent records for the set and the use of factor analysis (as in the science overlay mapping). The outcome of this exercise was the IPC based ‘base map’ displayed in Figure 5.18. Figure 5.19: Full patent map of 466 technology categories and 35 technological areas As we can clearly appreciate from Figure 5.18 the structure of the patent landscape revealed by citing-to-cited analysis with the IPC is very different to the scientific literature. However, as with the science overlay maps it is possible to compare and contrast the position of different companies within the map. Using the Georgia Tech Global Nanotechnology dataset Kay et al 2012 present two maps involving nanotechnology activity by Samsung and DuPont in Figure 5.20 and Figure 5.21 below. Figure 5.20: Position of Samsung in the Patent Overlay Map Figure 5.21: Position of DuPont in the Patent Overlay Map The utility of this type of analysis and visualisation is that we can immediately detect the very different positions of specific companies in the structure of the landscape of patent activity for nanotechnology. The emergence of nanotechnology in the patent system was initially dominated by the apparently scattered emergence of a new technology across multiple areas of the system. Put simply, nanotechnology did not fit inside existing categories. Patent offices later responded to the emergence of nanotechnology through the creation of a new IPC code (B82) following the development by the EPO of a classification tag Y01N. However, the challenges presented by the emergence of nanotechnology in the patent system raises wider questions of how to identify technological emergence or new and emerging technologies in the patent system (a list that could include graphene, synthetic biology, gene editing and other technologies). Developing indicators of technological emergence would be an important potential tool for informing investments and policy decision-making, particularly where this could be linked to technological forecasting. Figure graphene and biosensors are overlaid on the base map using data from the Georgia Tech Global Nanotechnology dataset. Figure 5.22: Nano Biosensors on the Overlap Map Figure 5.23: Graphene on the Overlap Map In comparing these two visual displays for sub-areas of nanotechnology we can immediately observe the presence of medical devices and pharmaceutical drugs is much more prominent bionanotechnology while Catalysis and Separation, Optics and Electric Power are more prominent for graphene related activity. The development of these types of maps raise many methodological questions that are being explored in the growing literature on science mapping. However, one of the most interesting findings of this 2012 study was that the areas of technology that are closest to each other when mapped using citation analysis do not necessarily appear in the same part of the hierarchy of the IPC. This leads Kay et al 2012 to conclude that: “The finding suggests that technological distance is not always well proxied by relying on the IPC administrative structure, for example, by assuming that a set of patents represents substantial technological distance because the set references different IPC sections”. As such, and as we have sought to highlight above, rather than seeing the IPC as involving a strict separation between areas of technology it is important to focus on the relationships between technology areas. Patent mapping of this type provides a way of visualising these relationships and allowing new and novel observations to emerge. We have focused here on one example of patent mapping with other research in the same period involving the use of inventor data and US patent data and the IPC (Boyack and Klavans 2008; Leydesdorff, Kushnir, and Rafols 2012; Schoen et al. 2012). More recent work has demonstrated the utility of overlay maps in predicting likely trends in development in specific technology fields. For example, Song et al 2016 used the entire US patent data for 1976 to 2016 to construct a map using IPC classes that was then used to attempt to predict likely directions of growth for Hybrid Electric Vehicles (HEVs) (Song et al. 2019). Figure 5.24 reproduces a figure from this more recent work displaying the total technology space for Hybrid Electric Vehicles and its predicted areas of growth. Figure 5.24: Song et al 2017 Hybrid Electric Vehicle Technology Space These developments in the use of the IPC at scale are being accompanied by debates within the scientific literature on the appropriate statistical techniques to more accurately map and predict technological developments (see for example, Yan and Luo (2019)). At the same time, the increasing availability of patent data at scale is opening up new research questions such as measuring innovation at the city level using the IPC and US patent data (Kogler, Heimeriks, and Leydesdorff 2018). 5.8 Conclusion This chapter has provided a detailed account of the use of the International Patent Classification in patent analytics. Starting with an introduction to the structure of the IPC we have then moved through a range of methods for visualising IPC data in order to communicate IPC based analysis to a wider audience. An important limitation of the IPC in terms of communication is that it consists of hard to decipher codes or long descriptive strings. In this chapter we presented the informal “short IPC” as a solution to this problem. Using the WIPO patent landscape report on animal genetic resources we demonstrated that visualisation of IPC data can be a key step in understanding, navigating, partitioning and communicating complex patent landscapes. Network visualisations of IPC data are particularly powerful tools for the analyst seeking to understand an area of technology and for communication with audiences. For this reason, and commonly using open source tools such as Pajek, Vos Viewer or Gephi, network visualisations are an increasing feature of many patent landscape reports. Readers interested in getting started with network visualisation can work through a practical demonstration using Gephi in the WIPO Manual on Open Source Patent Analytics. In the final section of this chapter we moved up in scale to consider the growth of science and patent mapping in order to inform investment, management and policy decision making. The increasing use of network and mapping techniques using the International Patent Classification forms part of a broader trend towards the use of multiple methods for patent landscape analysis at multiple scales. This convergence involves the use of multiple patent and non-patent literature data fields, the application or further development of a range of statistical measures, techniques from advanced natural language processing such as topic modelling and word or document embeddings, and other techniques. The combination falls within the description, and is a further development of technology mining or tech mining as originally defined by Alan Porter and Scott Cunningham in 2004 (Porter and Cunningham 2004). In Chapter 7 we will focus on combining the use of the patent classification with text mining techniques. References "],["citations.html", "Chapter 6 Patent Citations 6.1 Non Patent Literature 6.2 Literature and Patent Citation Data with the Lens 6.3 Retrieving Citations at Scale with PATCITE 6.4 The US PatentsView Non-Patent Literature Table 6.5 Patent Citations 6.6 Navigating Patent Networks 6.7 Counting Citations by Patent Families 6.8 Patent Citations by Generation 6.9 Citations and Knowledge Spillovers", " Chapter 6 Patent Citations This chapter focuses on the use of patent citations in patent analytics. Patent citations take two main forms: Citations of the scientific literature and other material such as news articles and websites, known as the Non-Patent Literature or NPL Citations of patent documents. The patent citation system is similar to the familiar academic citation system. However, patent citations differ from academic citations because they limit the scope of what an applicant can claim to be new or novel or as involving an inventive step. As the American economist Suzanne Scotchmer reminds us when approaching the patent system applicants are ‘standing on the shoulders of giants’ (Scotchmer 1991). Put simply new applicants are confronted by the combined weight of the scientific literature, other material and existing patent applications that make up the prior art. The prior art limits the scope of what may be claimed by new applicants and is recorded in patent citations. Citations within the patent system have become an important focus for research in fields such as econometrics, scientometrics and innovation studies. Citations of the non-patent literature are important focus of research because they help to reveal the closeness of the relationship between scientific research and innovative activity reflected in the patent system. Citations of patent documents are a focus of research because the number of citations that a patent document or patent family attracts is an indicator of social and economic value (A. B. Jaffe and Rassenfosse 2017). In addition, analysis of patent citations can lead to the identification of similar patent documents in a technology field, technology spillovers and technology trajectories. A rich literature has emerged around patent citation analysis and we will highlight some of the key sources in the course of this chapter. Recent work by A. B. Jaffe and Rassenfosse (2017) provides an accessible and detailed overview of social scientific research involving patent citations. For those seeking to ground themselves in key literature the work by Adam Jaffe and Manuel Trajtenberg (A. Jaffe and Trajtenberg 1998, 2002), along with work by Bronwyn Hall (B. Hall, Jaffe, and Trajtenberg 2001; B. H. Hall, Jaffe, and Trajtenberg 2005; B. H. Hall and Harhoff 2012), Colin Webb and Helene Dernis at the OECD (Webb et al. 2005) and Dietmar Harhoff (Harhoff et al. 1999; Harhoff, Scherer, and Vopel 2003) are essential reading. For those interested in exploring the wider literature on patent citations a Lens public collection is available to assist readers with getting started. The collection is dynamic and will automatically update to the latest literature on patent citations. In this chapter we will begin with the non-patent literature and move step by step through the issues that need to be considered when working with patent citation data. We will use examples from synthetic biology and CRISPR genome editing technology to illustrate approaches to the non-patent literature and patent citations and finish with recent research on identifying technology paths with citation data. 6.1 Non Patent Literature Non-patent literature mainly takes the form of scientific publications such as journals, books, or chapters but also extends to other types of materials such as manuals, news reports, drawings and websites. As we will see below, viewed from a data science perspective the non-patent literature is that it generally takes the form of messy free text that requires extensive cleaning. However, from approximately 1980 onwards NPL has become an increasingly important focus for research. For those seeking to track the emergence of research on NPL citations we will simply provide some useful way points. An important starting point of research on non patent literature is work by Carpenter, Cooper, and Narin (1980) on the links between basic academic research and patent activity using US patent citation data. This was followed in the mid-1990s by more detailed studies with larger scale data by Narin and collaborators (F. Narin, Hamilton, and Olivastro 1995; Francis Narin, Hamilton, and Olivastro 1997). This was accompanied by the growing proliferation of technology specific studies using citations such as work by Meyer (2000) on the relationship between nanoscale technologies and the cited literature. Citation based studies have tended to be heavily focused, for reasons of accessibility, on US patent data. However, in the mid-2000s work by Callaert, Looy, et al. (2006) examined 10,000 citations from the US and the European Patent Offices. This was followed by work to address the significant problems that exist with noise in NPL data using machine learning models (Callaert, Grouwels, and Looy 2011). The growing availability of NPL citation data is reflected in the growth of national and sectoral studies such as work by Fukuzawa and Ida (2015) exploring the links between scientific articles and patents for leading researchers in Japan. Recent work by Ding et al. (2017) has focused on the characteristics of scientific articles that facilitate knowledge flows between science and technology while Chen (2017) has explored the textual similarities between scientific articles and the contents of patent applications. Research by Rizzo et al. (2018) has focused on the closeness of publicly funded research and radical inventions in UK filings at the European Patent Office. As this very brief set of way markers suggests, a significant body of work on diverse topics has emerged around the non-patent literature. While much of the original research focused on US patent data the creation of the EPO World Patent Statistical Database (PATSTAT) has made the wider non-patent literature available in a single table and served as a spur for research (Webb et al. 2005; Callaert, Grouwels, and Looy 2011; Karvonen and Kässi 2013). The recent PatentsView service from the USPTO now makes NPL citations available as a single table containing over 6 million raw references that can be freely downloaded. As such, data on non-patent literature citations is becoming more and more accessible for research. We will consider the case of PatentsView in more detail below. However, one of the most important recent development are efforts by patent databases, led by the open access Lens database to electronically link literature databases and patent databases together. 6.2 Literature and Patent Citation Data with the Lens As discussed in the chapter on the scientific literature, free electronic access to sources of the scientific literature is increasingly available through services such as Crossref, PubMed and Microsoft Academic Graph. The Lens has now developed a Scholar Search service that includes approximately 291.4 million scholarly works from PubMed, Crossref and Microsoft Academic. These records are then linked to citations in patent over 110 million patent documents covered by the Lens Patent Search. The importance of this approach is that it allows the user to navigate the non-patent literature linked to a particular record or to extract the cited literature from a record and create a collection to download and analyse. As we will also see, the recent PATCITE service also allows users to retrieve data on citations at scale. To illustrate the possibilities opened up by combining the scientific and patent literature we will use the example of a patent search for synthetic biology and then move into exploration of the controversial subject of genome editing. 6.2.0.1 Retrieving Cited Literature from a Patent Search We will begin with a simple patent search of the titles and abstracts for terms relating to synthetic biology in the Lens. title: (“synthetic biology” OR (“synthetic genome” OR (“synthetic genomes” OR “synthetic genomics”))) OR abstract: (“synthetic biology” OR (“synthetic genome” OR (“synthetic genomes” OR “synthetic genomics”))) You can try this exact query and example live by following this link. To make the most of the Lens we recommend that you register as a user (registration is free) as this will allow you to create collections for export. At the time of writing this search generated 287 patent results and 776 cited works that can be viewed in the Cited Works panel as illustrated below: Registered users can create a public or private collection that can be shared with others and can also download the citation data up to a maximum of 50,000 records. As an alternative to creating a collection an export button is provided in the Cited Works tool bar with options to download in CSV, RIS, Bibtex and JSON formats. The CSV (comma separated values) option is particularly suited to text mining or visualisation in tables while the popular Bibtex format will be useful for creating bibliographies for researchers writing in markdown. Table 6.1 displays a small selection of the results. Table 6.1: Sample Literature Fields from the Lens for Synthetic Biology Title Publication Year DOI Pathway optimization and key enzyme evolution of N-acetylneuraminate biosynthesis using an in vivo aptazyme-based biosensor 2017 10.1016/j.ymben.2017.08.001 Transcriptome-based identification of the optimal reference CHO genes for normalisation of qPCR data 2017 10.1002/biot.201700259 The N-Acetylmuramic Acid 6-Phosphate Phosphatase MupP Completes the Pseudomonas Peptidoglycan Recycling Pathway Leading to Intrinsic Fosfomycin Resistance 2017 10.1128/mbio.00092-17 Efficient whole-cell biocatalyst for Neu5Ac production by manipulating synthetic, degradation and transmembrane pathways 2016 10.1007/s10529-016-2215-z Droplet immobilization within a polymeric organogel improves lipid bilayer durability and portability 2016 10.1039/c6lc00391e A wide range of other fields are also available with the downloaded data such as authors, keywords, abstracts (where available), MeSH terms (Medical Subject Headings), Chemicals, source urls and the number of patent documents that reference an article among others. This is therefore a very rich set of data for further exploration. Taking our small sample data for synthetic biology we can identify the articles that are the top cited in the patent dataset as displayed in Table 6.2. Note that the Referenced by Patent Count column refers to the total known count of patent documents citing the article. This measure will therefore generally favour older and foundational literature. This is revealed by the dominance of citations to the Basic Local Alignment Search Tool (BLAST) algorithm that is very widely used in biology. Table 6.2: Sample Literature Fields from the Lens for Synthetic Biology Title Publication Year Referenced by Patent Count Basic Local Alignment Search Tool 1990 8553 Continuous cultures of fused cells secreting antibody of predefined specificity 1975 7772 A general method applicable to the search for similarities in the amino acid sequence of two proteins 1970 6370 Gapped BLAST and PSI-BLAST: a new generation of protein database search programs 1997 5978 Amino acid substitution matrices from protein blocks 1992 2347 Melamine Deaminase and Atrazine Chlorohydrolase: 98 Percent Identical but Functionally Different 2001 1692 Comparison of biosequences 1981 1627 Rapid and efficient site-specific mutagenesis without phenotypic selection 1985 1568 The tac promoter: a functional hybrid derived from the trp and lac promoters 1983 1161 One-step inactivation of chromosomal genes in Escherichia coli K-12 using PCR products 2000 836 The availability of this type of data opens up a wide range of research opportunities and the maximum export of 50,000 records per query is likely to prove ample for most research purposes. These opportunities include. Refining search strategies by text mining the titles and abstracts and keywords of cited literature; Exploring networks of patent documents citing a key piece of literature; Examining available data on the funding of scientific research cited in patent documents as part of innovation studies; Assessing issues around the closeness of scientific research to inventions. One illustration of patent exploration using our sample dataset might involve genome editing with CRISPR/CAS9. Patent activity in this field has attracted widespread attention following litigation between the University of California and the Harvard-MIT Broad Institute and the patent landscape linked to the CRISPR dispute is discussed in detail by Egelie et al. (2016) (see also Ledford (2016), Ledford (2017), Ledford (2018)). While not linked directly to patent activity, genome editing in humans using CRISPR/CAS9 has also recently attracted international media attention (Cyranoski and Ledford 2018). Two key researchers in this field are Jennifer Doudna at Berkeley and Feng Zhang at the Harvard-MIT Broad Institute. Both appear in the NPL citations for our simple patent query for synthetic biology. By splitting the data so that each author name appears on its own row we can identify our authors of interest in the dataset as show in Table 6.3 Table 6.3: CRISPR Authors Cited in Patent Literature Lens ID Author/s Publication Year Title https://www.lens.org/018-893-049-787-145 Jennifer A. Doudna 2016 New CRISPR-Cas systems from uncultivated microbes. https://www.lens.org/049-383-529-000-174 Jennifer A. Doudna 2014 Enhanced homology-directed human genome engineering by controlled timing of CRISPR/Cas9 delivery. https://www.lens.org/009-802-337-761-385 Feng Zhang 2015 Cpf1 is a single RNA-guided endonuclease of a class 2 CRISPR-Cas system. https://www.lens.org/150-951-500-543-136 Feng Zhang 2013 Double Nicking by RNA-Guided CRISPR Cas9 for Enhanced Genome Editing Specificity In Table 6.3 we see four publications by the key researchers cited in the sample dataset. Clicking on one of the links provides access to details on the records and also to patent documents that cite the literature at the Lens. In Figure 6.1 we have used the record in the third row of Table 6.3 above. In Figure 6.1 we can see the publication record and also the 170 patent documents that cite this paper. Figure 6.1: Patent Documents Citing A Key CRISPR Research Article This suggests that one opportunity for using the cited literature identified from a raw search is to begin to build a portfolio of documents that cite key researchers in a field such as CRISPR. In the case of the four articles identified above we simply used the hyperlink to access the data, selected Patent Citations and then View full patent data. By viewing the full patent data for each link we were able to add the citing patents to a new CRISPR collection in a few minutes (you must be logged in to create a collection). At the time of writing a collection of 304 patent documents in 256 families linked to these CRISPR articles . This collection is is publicly accessible at https://www.lens.org/lens/collection/167967. As this makes clear, linking the scientific and patent literature together makes it very easy to construct an exploratory patent portfolio in minutes. In the past this might have taken weeks or months. However, it is now also possible to work on a larger scale. 6.3 Retrieving Citations at Scale with PATCITE The PATCITE tool in the Lens is a recent introduction that allows a user to paste in a set of article or patent identifiers to retrieve citation data. The advantage of PATCITE is that it is possible to do this in bulk at the level of thousands of identifiers, such as the widespread Document Object Identifiers (dois) for the scientific literature. This will normally be more convenient when working with data from other datasets. To briefly illustrate PATCITE we will use the four documents identified from the CRISPR publications identified above. “10.1038/nature21059 10.7554/elife.04766 10.1016/j.cell.2015.09.038 10.1016/j.cell.2013.08.021” We paste these numbers into the option to Explore the cited scholarly work found in patent literature. This will then produce a screen showing the four articles and a Citing Patents list. At the time of writing this list contained 304 Members. Figure 6.2 displays the summary of results and demonstrates that it is possible to retrieve a patent portfolio based on the use of document identifiers (dois) that can then be exported (see Export results in the bottom left of Figure 6.2). Figure 6.2: PATCITE Results for Four Key CRISPR Articles PATCITE also features analysis tools such as rankings of applicants citing the documents and the visualisation of networks of citations. The visualisation of networks is likely to be of particular interest as it allows for the exploration of other literature cited in a patent document. By way of illustration, Figure 6.3 shows all patent citations linked to a literature record. Figure 6.3: Literature Citation Network for A CRISPR related Patent Filing PATCITE includes options to export both the cited literature (where using patent numbers as the starting point) or citing patent documents. This is particularly useful when working at a larger scale. In recent work on synthetic biology, P. D. Oldham and Hall (2018) mapped authors of scientific articles on synthetic biology identified in Web of Science into the global patent system by matching author names to inventor names. The literature dataset on which the research was based consisted of 4,463 publications containing 3,970 dois. These dois can be accessed here if you would like to reproduce this test. Figure 6.4 reveals that PATCITE identified 893 of the 3,970 scientific article identifiers in patent documents. Figure 6.4 displays the scientific records with the highest number of citing patent documents from a set of 2,349 patent families and 3,323 patent documents. Figure 6.4: Patent Citations for Synthetic Biology Baseline Literature Dataset Each of the datasets can be downloaded in Excel format for further analysis. As this example makes clear PATCITE addresses issues of scale in exploring the relationship between the scientific literature and the patent literature. As discussed above in the case of CRISPR this opens up the possibility of creating collections of patent data based on links with the scientific literature either as a starting point for a search strategy, mapping the impacts of research, or exploring the closeness of relationships between the scientific literature and the patent literature in innovation studies. In the case of new and emerging areas of science and technology, such as synthetic biology PATCITE also opens up the possibility of overcoming some of the limitations of key word based searches. In the case of synthetic biology it can be argued that it is emerging within the wider field of genetic engineering and biotechnology and uses much the same language. This makes it difficult to develop a keyword strategy that adequately captures the field without capturing unrelated activity. At the same time, an additional challenge with keyword strategies is that analysts through the selection of particular terms inevitably impose their own definitions on emerging fields. For example, in the case of synthetic biology should we assume that any reference to a synthetic gene, or to protein engineering or systems biology in a patent document should be treated as synthetic biology? PATCITE offers the possibility of beginning these explorations directly from the scientific literature and following through into the patent literature and merits serious consideration by researchers seeking to map emerging areas of science and technology in the patent system. Specifically, literature based patent searching could provide the basis for landscape construction and also be used as part of a strategy for validating the outcomes of key word based queries. One logical question for researchers seeking to match the scientific literature into patent data is the issue of data capture. That is, it is not immediately clear whether the 3,077 dois that were not identified in PATCITE were not identified because they are absent from patent data or because of limitations in capture. The answer to this question may be a combination of the two. In practice, the ability of an analyst to interrogate data capture at the database level, such as the accurate identification of dois, is likely to be limited. However, we can gain an insight into these issues using the raw citation data from the US PatentsView service. 6.4 The US PatentsView Non-Patent Literature Table For most patent analysts the Lens literature and PATCITE service is the logical starting point for research, for the straight forward reason that it is so easy to use and can generate a targeted patent collection for exploration within a few minutes. However, this may not suit all purposes, particularly where larger scale data is required. It is also a very good idea to have an understanding of what the raw NPL looks like in understanding the strengths and limitations of different databases. In the case of offline patent databases such as PATSTAT a table is available containing the non-patent literature for subscribers (also accessible through the online version of PATSTAT). However, the USPTO, through the PatentsView service makes a non-patent literature table available for download (presently as a 2.7 GB tab separated zipped file).23 Engaging with the raw non-patent literature data reveals that it is a free form text field. Table 6.4 shows a sample of entries from the over 6 million entries in the 2018 USPTO PatentsView non-patent literature table. Table 6.4: A sample of Non Patent Literature patent_id text 9339622 English Translation of Chinese Examination Report; Application No. 2007800266164; 5 pages. 5013322 Surgery News-An Advertising supplement, Aug. 1, 1985, vol. 3, No. 15, Clayman Ovoid Model No. 8743 and Kratz/Johnson 7 mm Lightweight Model No. 8663, (2 pages). 8773357 U.S. Office Action dated Dec. 23, 2011 in U.S. Appl. No. 12/571,157. 7307640 Duke, “Dreamcast Technical Specs”, Sega Dreamcast Review, Sega, Feb. 1999, www.game-revolution.com. 8543711 Ranjan, S. and Rolia, J., Fu, H., and Knightly, E., “QoS-Driven Server Migration for Internet Data Centers,” In Proc. of IWQoS 2002, Miami, FL, May 2002. 5849555 \" J. Hughes et al., \"\"How Does Pseudomonas Fluorescens, the Producing Organism of the Antibiotic Pseudomonic Acid A, Avoid Suicide?\"\", FEBS Letters, 122(2) pp. 322-324 (1980). \" 8811330 Kaitz et al., “Changing the status of Subchannelization in OFDM mode,” IEEE 802.16 Broadband Wireless Access Working Group, IEEE C802.16d-03/80, IEEE, New York, New York (Nov. 13, 2003). 9500933 Jain et al., “Efficient Nonlinear Frequency Conversion with Maximal Atomic Coherence”, The American Physical Society, Physical Review Letters, vol. 77, No. 21, Nov. 18, 1996, pp. 4326-4329, 4 pages. 6653062 English Translation of Migulina. 8137555 Communication Relating to the Results of the Partial International Search for corresponding International Patent Application No. PCT/US2011/031412 mailed Aug. 9, 2011. 6855523 Biebricher, et al. (1986) Nature 321: 89-91. 8960456 Office Action issued Oct. 4, 2013 in U.S. Appl. No. 13/268,712 by Didehvar. 6576467 Feinberg et al., &amp;#8220;A Technique for Radiolabeling DNA Restruction Endonuclease Fragments to High Specific Activity,&amp;#8221; 8735564 Li et al; Detection of Human Papillomavirus Genotypes With Liquid Bead Microarray in Cervical Lesions of Northern Chinese Patients; Cancer Genetics and Cytogenetics, Elsevier Science Publishing, New York, NY, US; vol. 182; No. 1; Mar. 6, 2008; pp. 12-17; Abstract. 9440232 Fungi (Wikipedia.com accessed Jun. 3, 2013). 9169348 USPTO Office Action dated Sep. 9, 2008 for copending U.S. Appl. No. 11/391/571. 6940750 Jian-Gang Zhu et al. “Ultrahigh Density Vertical Magnetoresistive Random Access Memory (Invited),” Journal of Applied Physics, vol. 87, No. 9, May 1, 2000, pp. 6668-6673. 8343171 U.S. Appl. No. 60/990,062, filed Nov. 26, 2007. 8046478 MCL Paper Abstracts; Ahanger et al.; “A Language to Support Automatic Composition of Newscasts”; Journal of Compuoter Information Technology ; vol. 6, No. 3; 1998. 9763641 Ophir et al., “Elastography: Ultrasonic Estimation and Imaging of the Elastic Properties of Tissues,” Proc Instn Mech Engrs 213(Part H) (1999) 203-233. As we can see from this small sample of over 6 million entries in the USPTO NPL data, the individual entry fields can reasonably be described as a messy text field. Among the issues that we encounter are partial references, spelling mistakes such as “Journal of Computer Information Technology”, abbreviations such as “Proc Instn Mech Engrs” and considerable variation in the presence of dois that will all need to be addressed to successfully extract the literature references. To extract meaningful information from this table we would need to think about identifying patterns. For example, we might look for document identifiers (dois) for the scientific literature and note that most begin with https:://doi.org. We would then discover that the references to dois within the table are limited and might switch to using titles to cross match with other databases such as Crossref or PubMed. In short, when seeking to work with the NPL data, experimentation with regular expression based pattern matching and development of a strategy would rapidly become necessary to achieve meaningful results. To illustrate this we will use the example of web addresses in this table. While our aim is not complete accuracy in the extraction of web addresses, we can illustrate the growing relevance of web sites as sources of prior art in the US patent data. If we were looking for web addresses we could use an approach that detects the presence of http in a reference as a distinctive string. In practice this would filter this large table down to 304,590 entries containing this term. We would then do further work to identify the domains etc. using a regular expression pattern such as www\\\\..*?\\\\.com. We could also look at modifying the regular expression pattern to capture alternative URL endings such as .org, .net etc. This is certainly doable but could rapidly become quite complicated. An alternative approach would be to recognise that others have worked on this kind of problem before with similar types of text data. We can therefore look at using existing solutions for this particular task rather than repeating work on writing regular expression patterns. In the case of the R programming language a solution to this problem is provided by Tyler Rinkr’s recent qdapregex package in R that complements his larger qualitative data analysis package qdap. We would install and load this package as follows. install.packages(&quot;qdapregex&quot;) library(qdapRegex) qdapregex contains a function to extract urls from texts called ex_url() without needing to work on regular expressions. Here we create a new web object containing the text and extract the urls with ex_url() (for extract url). What we would like to do is to identify the top domain names (such as google.com) appearing in the references table. In reality the way in which URLs are expressed in the references is quite messy and requires quite a lot of tidying up. We would probably need to do some more work to tidy up and validate the data for truly accurate results, but the code below takes us most of the way for the purposes of illustration. library(tidyverse) library(qdapRegex) library(stringi) # Our aim is to extract urls and then reduce to the domain # ex_url returns a list object web &lt;- npl$text %&gt;% ex_url() # process the list and return a data frame url &lt;- web %&gt;% map(., `[[`, 1) %&gt;% # extract the first element from the list of results discard(., is.na) %&gt;% # drop NA for Not Available tibble(url = .) %&gt;% # convert to tibble unnest() %&gt;% # unnest list column mutate(url = str_replace_all(.$url, &quot;http://|https:|http|http:|:|//&quot;, &quot;&quot;)) %&gt;% mutate(url = str_replace_all(.$url, &quot;www.|&gt;&quot;, &quot;&quot;)) %&gt;% mutate(domain = sub(&quot;/.*&quot;, &quot;&quot;, url)) %&gt;% mutate(domain = str_trim(domain, side = &quot;both&quot;)) %&gt;% mutate(domain = stringi::stri_reverse(domain)) %&gt;% # reverse string mutate(domain = str_replace(domain, &quot;^[.]|^,|^;&quot;, &quot;&quot;)) %&gt;% # remove junk mutate(domain = stringi::stri_reverse(domain)) # reverse back # count up domains and filter out blank results domain &lt;- url %&gt;% count(domain, sort = TRUE) %&gt;% filter(domain != &quot;&quot;) This code parses the results down to 214,756 domains Table 6.5: Top Domains in PatentsView Non-Patent Literature domain n web.archive.org 12973 en.wikipedia.org 10563 gsmarena.com 3962 ncbi.nlm.nih.gov 3277 ieeexplore.ieee.org 2879 youtube.com 2834 msdn.microsoft.com 1935 amazon.com 1819 microsoft.com 1562 citeseerx.ist.psu.edu 1506 w3.org 1490 ietf.org 1428 3gpp.org 1108 research.microsoft.com 1093 clinicaltrials.gov 936 cisco.com 922 google.com 920 tools.ietf.org 909 sciencedirect.com 857 merriam-webster.com 850 While this data would require further cleaning we now have a working idea of what the top web domains are across the US patent collection. In particular we can see that applicants make particular use of the Internet Archive at http://web.archive.org/ and the English language version of Wikipedia, with the third result focusing on the Global System for Mobile Communication (GSM) website GSM Arena https://www.gsmarena.com/. We can also see that in some cases such as Microsoft (or Google), specific sub-domains such as the Microsoft Developers Network (MSDN) at https://msdn.microsoft.com/en-us/ are included. If we were to do further work we would want to trim these down to the respective core domain. As this suggests, the apparently simple task of extracting and ranking web domains involves more thought than might initially be suggested. However, awareness of existing tools can radically reduce the work involved. In this section we have seen that access to the non-patent literature in patent databases has improved dramatically in recent years. As a result of the integration of the scientific literature and patent literature by the Lens it is now possible to enter scientific literature of interest and retrieve a patent portfolio in a matter of minutes. Similar developments are taking place among commercial providers such as the subscription based Dimensions database that applies machine learning to scientific publications, grant information, clinical trials data and patent data. It is likely that other companies will be working on similar initiatives. When coupled with other developments such as non-patent literature tables in PATSTAT and access to raw non-patent literature with PatentsView it is clear that large scale analysis of the NPL literature is now possible. The ability to work with such data at scale will typically involve programmatic skills, but it is important to bear in mind that many other fields involve finding solutions to very similar problems, such as extracting URLs from texts. Investment in research on solutions to similar problems will often radically reduce the amount of work required and allow for the detailed exploration of the non-patent literature. We now turn to the use of patent citations. 6.5 Patent Citations Patent citations are citations to other patent documents. They take the form of backward (cited) and forward (citing) citations. Backward citations, also referred to as back citations or cited patent documents, refer to earlier patent applications or grants that affect the scope of the claims of an application. Forward citations or citing documents refer to later filings of applications that are affected by the scope of the claims of the cited document. Patent citations have two main sources (A. B. Jaffe and Rassenfosse 2017; Hegde and Sampat 2009): inventors and their patent attorneys patent examiners The different sources of patent citations have important implications. Specifically, the two different sources of citations may have very different motives for including a citation (Webb et al. 2005). Thus, patent applicants and their attorneys will have an interest in disclosing references that have a limited impact on claims to novelty and inventive step. In the United States, and possibly other jurisdictions, applicants are expected to provide the prior art they are aware of as part of a duty of candour (Webb et al. 2005; Cotropia, Lemley, and Sampat 2013). This may lead to practices such as seeking to draft around the prior art (Cotropia, Lemley, and Sampat 2013). This perhaps explains why Cotropia, Lemley, and Sampat (2013) found that patent examiners typically ignore prior art provided by applicants. In contrast, patent examiners can be expected to focus more closely on identify those that impact novelty and inventive step. Prior art searches by examiners are widely regarded as the highest quality of citations, because this involves a search by trained examiners for relevant prior art affecting an application. However, it is important to recognise that citation practices vary between patent offices (A. Jaffe and Trajtenberg 2002; Webb et al. 2005). Thus, in the United States examiners are expected to list all relevant prior art while at the European Patent Office the examination guidelines stipulate that the European Search report include only the most relevant references (Webb et al. 2005). The practical upshot of this is that citations from the USPTO will often be longer than those from the EPO. As such, it is important to be aware of the differences between patent offices in citation practices. Bearing these issues in mind, A. B. Jaffe and Rassenfosse (2017) highlights that in broad terms patent citations provide insights in two main areas: the impact of inventions on other applicants and their economic and social value; as proxies for knowledge flows and networks. We are now in a position to begin navigating patent citation networks. 6.6 Navigating Patent Networks In the discussion below we will focus on the basic issues involved in navigating patent citations using the CRISPR Cas9 genome editing cases above as our example. We will start with a fictional search of a patent database for CRISPR that identifies EP2784162B1 from the Harvard-MIT Broad Institute for Engineering of systems, methods and optimized guide compositions for sequence manipulation by inventors Le Cong, Feng Zhang and other collaborators. We will then explore different ways of counting and navigating patent citations. If you wish to explore the citations discussed below using the Lens you can access the reference document EP2784162B1 here. Note that counts of citations may vary across different databases. In the working examples below we use data from Clarivate Analytics Derwent Innovation database. Because of its greater coverage of national collections worldwide Derwent Innovation will often involve higher scores than other databases. 6.6.1 Back citations Back citations are earlier patent filings (either applicants or grants) that may be listed by the applicant or an examiner during search and/or examination. Table 6.6 shows a selection of patent documents (applications or grants) cited by EP2784162B1 and patent documents citing this document. In total for EP2784162B1 there were 40 documents in the cited table and 22 documents in the citing table. Table 6.6: EP2784162B1 Cited to Citing Relationships cited patents publication number citing patents US4235871A EP2784162B1 US10000772B2 US4774085A EP2784162B1 US10077445B2 US4797368A EP2784162B1 WO2016049163A2 WO2013176772A1 EP2784162B1 WO2016049251A1 US20130074819A1 EP2784162B1 WO2016069591A2 WO2014099744A1 EP2784162B1 WO2016182893A1 Table 6.6 illustrates the basic relationship between a reference document (EP2784162B1), its cited documents (e.g US4235871A) and citing documents (e.g. US10000772B2). The cited documents reference earlier patents and inform us about technologies that are informing and shaping the claims of our target document (EP2784162B1). Table 6.7 shows a selection of the cited documents ranked by the number of later filings that cite them (citing_count). The citing count in this case refers to the global total of citing patent documents. We can clearly see that the older documents are generally those with the highest citations. That is, while many patent documents will never attract citations, of those that do, older documents will typically attract higher citations scores than younger ones (Webb et al. 2005). Table 6.7: EP2784162B1: Top Cited Patent Documents by Global Citing Count first_applicant citing_count cited_patent_count publication_number publication_year title PAPAHADJOPOULOS DEMETRIOS P 1858 6 US4235871A 1980 Method of encapsulating biologically active materials in lipid vesicles Liposome Technology Inc.  1276 4 US4837028A 1989 Liposomes with enhanced circulation time Technology Unlimited Inc.  1050 10 US4501728A 1985 Masking of liposomes from RES recognition Syntex (UA.) Inc.  960 2 US4897355A 1990 N[&amp;#969;,(&amp;#969;-1)-dialkyloxy]-and N-[&amp;#969;,(&amp;#969;-1)-dialkenyloxy]-alk-1-yl-N,N,N-tetrasubstituted ammonium lipids and uses therefor Biogen Inc.  828 13 US4873316A 1989 Isolation of exogenous recombinant proteins from the milk of transgenic mammals One issue to bear in mind when working with patent citations is that there are a variety of reasons why a citation may be awarded. In some cases what may appear to be a relatively minor technical feature of a claimed invention may attract a citation from a patent document in an otherwise unrelated field. This will reflect the reality that patent claims are considered against the much wider background of the existing prior art. We will come back to this issue below. Furthermore, cited patents that have received the largest number of citations may not in fact be the closest to the new claimed invention. In this case the most important cited document is WO2013176772A1 relating to RNA directed target DNA modification and modulation of transcription.24 Table 6.8 displays this family member filed by Jennifer Doudna and colleagues at the University of California at Berkeley with an earliest priority from the 25th of May 2012 (provisional application US201261652086P). Table 6.8: The Berkeley CRISPR Patent first_applicant citing_count cited_patent_count publication_number publication_year title THE REGENTS OF THE UNIVERSITY OF CALIFORNIA 307 2 WO2013176772A1 2013 METHODS AND COMPOSITIONS FOR RNA-DIRECTED TARGET DNA MODIFICATION AND FOR RNA-DIRECTED MODULATION OF TRANSCRIPTION In this case we can see that a relatively young document has attracted a very significant number of citing documents relative to its age. In practice, documents that are relatively young but are attracting a significant number of citations should be an important focus of our attention. These will often either be highly original, and therefore disruptive in the technology space, and/or their claims will be broadly drawn. While older documents may attract more citations this may apply to a wider range of fields. Younger documents that are attracting citations are likely in technology terms to be closer to the reference document (EP2784162B1). This is particularly true where the cited document and the target document share the same International Patent Classification (IPC) or Cooperative Patent Classification (CPC) codes. Thus, the cited document with the highest level of citations in our set US4235871A shares the C12N subclass with the reference document EP2784162A1 and also shares the C12N15 group. However, it does not share a sub-group with the reference document. In contrast our Berkeley patent document WO2013176772A1 shares IPC code C12N15/63 and Cooperative Patent Classification (CPC) codes C12N15/102, C12N15/113, C12N15/63, C12N15/907 and C12N9/22. The fact that the documents share a significant number of classification codes suggests technological closeness and, in this case, rivalry. The rivalrous nature of this particular case will become clearer when we consider forward citations. 6.6.2 Forward Citations As noted above, forward citations or citing documents are later patent filings that cite a reference document (EP2784162B1). Table 6.9 displays a selection of the documents citing our reference document from the Broad Institute (EP2784162B1). The first point that stands out is that this data is dominated by the Broad Institute (which is a joint Harvard and MIT entity), and MIT with the bulk of forward citations arising from other filings by the Broad Institute and MIT accounting for 17, Brigham and Womens Hospital in Boston for 2, the University of California accounting for 2 and Vertex Pharmaceuticals for 1 citation. Table 6.9: A selection of Patent Documents Citing EP2784162B1 first_applicant cited_patent_count citing_count publication_number publication_year title University of California 241 0 US10000772B2 2018 Methods and compositions for RNA-directed target DNA modification and for RNA-directed modulation of transcription University of California 262 0 US10077445B2 2018 Methods and compositions for RNA-directed target DNA modification and for RNA-directed modulation of transcription THE BROAD INSTITUTE INC 134 8 WO2016049163A2 2016 USE AND PRODUCTION OF CHD8+/- TRANSGENIC ANIMALS WITH BEHAVIORAL PHENOTYPES CHARACTERISTIC OF AUTISM SPECTRUM DISORDER THE BROAD INSTITUTE INC 104 9 WO2016049251A1 2016 DELIVERY, USE AND THERAPEUTIC APPLICATIONS OF THE CRISPR-CAS SYSTEMS AND COMPOSITIONS FOR MODELING MUTATIONS IN LEUKOCYTES THE BROAD INSTITUTE INC 98 6 WO2016069591A2 2016 COMPOSITIONS, METHODS AND USE OF SYNTHETIC LETHAL SCREENING An alternative way of viewing patent citations is to visualise them as a tree of relationships. Figure 6.5 displays the set of citing applicants and citing documents for EP2784162B1 as a tree grouped on the first applicant. Figure 6.5: Citing Documents Grouped by Applicants In this case 9 of the 22 documents involve inventors from the Broad Institute (Feng Zhang and Le Cong) while the two citing documents from the University of California involve four of the inventors on the Berkeley document cited by the Broad Institute (WO2013176772A1). There are actually two separate types of activity that emerge in the cited and citing documents for EP2784162A1. We observe competing filings by two groups of inventors, one from Berkeley and the other from Harvard-MIT Broad Institute. A filing from Berkeley that occurred before the Broad Institute filing is cited by the Broad Institute, but later filings from Berkeley cite the Broad Institute filing. The Broad Institute filings are self-citing one of their own earlier filings in later filings. In the first case the citation data reveals competition in this space. The second case would however require further investigation of whether the citations are inserted by the applicants (and therefore less likely to affect the scope of the newer filings) or whether they are inserted by examiners (and may therefore impact on the scope of the claims). A review of the source of the citation in the first of the Broad Institute citing documents WO2016049163A2 reveals that it was inserted by the applicant and has no recorded impact on the claims (typically marked by XYI). Note also that even where an earlier filing does have an impact on the claims it may be possible to identify ways to modify the claims to accommodate the prior art. We now have a basic grounding in back and forward patent citations using our CRISPR case. What we have learned is that: Back citations may be from a wide range of technology fields in the prior art; Back citations may not have a direct impact on the scope of the later filing, particularly where they are inserted by the applicant rather than examiners; Older patent documents generally attract more citations than newer ones; Recent documents that are attracting a significant number of citations deserve close attention; Attention is required to International Patent Classification/Cooperative Patent Classification codes to determine how close cited inventions are to each other; Competition can be observed between applicants where they are citing each other; The sources of citations matter. Citations by examiners have a greater impact than citations inserted by applicants. Each of these observations merits further exploration than can be provided in this chapter. The background literature discussed in this chapter explores a number of these topics. We now turn to the question of how to approach counts of patent citations and the implications of counting citations by patent families. Once again we will use the CRISPR example as a real world case. 6.7 Counting Citations by Patent Families As we have seen above, one method or exploring the landscape of patent citations is to focus on individual documents. However, as we will now see, conducting analysis on a per document basis may miss the majority of patent citations associated with the wider patent family and the key document or documents within a family. As such counts limited to individual documents may radically underestimate the impact of a claimed invention within technology space. We can illustrate this for our reference document EP2784162B1 and its wider INPADOC patent family. At the time of writing this document forms part of a patent family with 277 members that has 369 cited patents and 717 citing patent documents . Table 6.10 displays the top ranking publications based on citing patents within this family. Table 6.10: Broad Institute CRISPR Patent Family Ranked by Citing Patent Count publication_number count_of_citing_patents title_original US8697359B1 274 CRISPR-Cas systems and methods for altering expression of gene products WO2014093622A2 157 DELIVERY, ENGINEERING AND OPTIMIZATION OF SYSTEMS, METHODS AND COMPOSITIONS FOR SEQUENCE MANIPULATION AND THERAPEUTIC APPLICATIONS | DÉLIVRANCE, FABRICATION ET OPTIMISATION DE SYSTÈMES, DE PROCÉDÉS ET DE COMPOSITIONS POUR LA MANIPULATION DE SÉQUENCES ET APPLICATIONS THÉRAPEUTIQUES WO2014093712A1 125 ENGINEERING OF SYSTEMS, METHODS AND OPTIMIZED GUIDE COMPOSITIONS FOR SEQUENCE MANIPULATION | FABRICATION DE SYSTÈMES, PROCÉDÉS ET COMPOSITIONS DE GUIDE OPTIMISÉES POUR LA MANIPULATION DE SÉQUENCES WO2014093655A2 122 ENGINEERING AND OPTIMIZATION OF SYSTEMS, METHODS AND COMPOSITIONS FOR SEQUENCE MANIPULATION WITH FUNCTIONAL DOMAINS | FABRICATION ET OPTIMISATION DE SYSTÈMES, DE PROCÉDÉS ET DE COMPOSITIONS POUR LA MANIPULATION DE SÉQUENCE AVEC DES DOMAINES FONCTIONNELS US20140179770A1 121 Delivery, Engineering and Optimization of Systems, Methods and Compositions for Sequence Manipulation and Therapeutic Applications This data makes clear that our original reference document EP2784162B1, with 23 citing documents at the time of writing, was not reflecting the wider citation landscape for this patent family. Furthermore, as in the litigation surrounding this case discussed by Egelie et al. (2016) and Ledford (2018), the most important individual document in this family is in fact US granted patent US8697359B1. Specifically, while, as we have seen, the Broad Institute patent family cites the Berkeley patent application, in reality the Broad Institute patent attorneys used the expedited examination procedure at the USPTO with the effect that the Broad Institute patent filing was granted before the Berkeley filing was examined.^[[Source: Broad Communications 2022FOR JOURNALISTS: STATEMENTS AND BACKGROUND ON THE CRISPR PATENT PROCESS, Updated February 28] The important insight from a patent family based perspective is that a focus on citation counts for individual documents will often miss the wider picture. As such, it is important, wherever possible, to construct citation analysis at the patent family level. This will not only assist with identifying the most important documents in a family based on citing counts but in larger datasets will allow for the identification of the most important patent families in terms of their impact upon other actors within the technology space. When working with patent citations on a family level bear in mind that the number of family members and thus of citations will depend on the family definition. Thus, simple or DOCDB families will be smaller than INPADOC families as we have seen in the earlier discussion of Patent Families. The citing landscape for those families is therefore also likely to be smaller. In thinking about calculating the number of citing patent documents we might be tempted to simply sum the count of citing patents provided with the export of the data from the patent database. For this working example we would obtain an answer of 5,138 as the gross figure. However, that is not actually what we want. The figure that we want is the count of distinct or unique citing documents to arrive at the citing count for the patent family. The basic process for making this calculation is to: Separate out the citing patent numbers onto new rows; Identify any duplicate numbers. Remove or ignore the duplicates and count the distinct documents. This procedure gives the correct figure of 718 citations directly linked to this patent family as we will see in the calculation below. This calculation can be performed in a number of different ways, depending on the tools you are using, with commands such as DISTINCT or unique() and so on. The important idea is to avoid over counting the citing documents by ensuring that any duplicates of the same document are not included in the count. The code below shows a step by step approach to this type of calculation in R. There will be more efficient routes to achieving this in R and other languages. The aim here is to show the steps in a transparent way. This basically consists of separating, trimming, filtering duplicates and counting. library(tidyverse) EP2784162B1 %&gt;% # table containing the citing data separate_rows(citing_patents, sep = &quot;;&quot;) %&gt;% # separate citing documents onto own rows mutate(citing_patents = str_trim(citing_patents, side = &quot;both&quot;)) %&gt;% # trim whitespace mutate(duplicated = duplicated(citing_patents)) %&gt;% # identify duplicates filter(duplicated == FALSE) %&gt;% # filter to unique documents select(citing_patents) %&gt;% # choose the column to count tally() # count n 718 If we were to work with a larger dataset containing multiple patent families we would therefore want to perform the above process based on each first filing (as the first family member) to arrive at the count of citing documents. Bear in mind that this choice will depend on the analytical task at hand. However, as we have seen in the case of EP2784162B1 this document is not the most important document in the family. As such, when working with patent search results it is important to move up one level to capture patent family data and then identify the most important documents in the set based on citation counts. The ability to do this will however depend in part on the options provided by the patent databases you have access to. 6.8 Patent Citations by Generation Patent citations exist in what are often called layers or generations. Thus, a first generation forward citation is a document citing one of our reference documents such as EP2784162B1. A second generation citation is a document that cites the first generation document and so on. This is easier to appreciate where they are displayed in a tree structure. Figure 6.6 displays a tree map for PCT family members of US8697359B1. As the complete family set consists of 277 INPADOC family members we have restricted the set to PCT documents for ease of visibility. In the online version of this chapter the nodes can be selected and will expand or contract. Figure 6.6: First and Second Generations of Citations from PCT filings for a CRISPR Family In Figure 6.6 the first layer consists of the PCT family members of US8697359B1. Then we see the first generation of citing patents followed by a selection of the second generation of citing patents. In both theory and practice, the citation tree could extend through multiple generations until no more generations of forward citations can be identified. An alternative way of viewing a citation tree is to cluster the records on the applicant names in the different generations. To achieve accurate results you should expect to clean the applicant names before hand. Figure 6.7 displays the same data with applicant names. Figure 6.7: Citations from PCT filings for a CRISPR Family by Applicants The visualisation of patent citations across multiple generations draws our attention to competitive activity in the technology space for CRISPR. However, it also draws our attention to the issue of knowledge spillovers that have been a detailed focus of analysis on the scientific literature for patent citations to which we now briefly turn. 6.9 Citations and Knowledge Spillovers Put simply, knowledge spillovers occur where the knowledge provided by an applicant informs later inventions by other unrelated applicants that may or may not be working in the same technology space. One way to address knowledge spillovers is to weight the citation data to remove forward citations by the same applicant. In this case the approach taken is to identify and remove any incidence of the Broad Institute from the first generation of forward citations leaving only those affected by the reference family. Figure 6.8 displays the impact of this approach. Figure 6.8: Detecting Knowledge Spillovers What this leaves behind therefore is other applicants. If we were to compare this family with other patent families, the applicant with the highest score in terms of knowledge spillover would arguably be the applicant with the patent family with the largest number of other applicants within the forward citing landscape. Note that this simple approach does not address the issue of direct competitors. As we can see in Figure 6.8 the University of California (Berkeley) appears in the list of distinct applicants. It is arguably the case that viewed from the perspective of this applicant, they are the originators of the CRISPR technological breakthrough and therefore the source of the knowledge spillover. Depending on ones purpose, this could of course be settled through comparison of the patent families and counting the number of affected applicants. The “winner” in this case would be the applicant with the highest score. An alternative, but complementary, way of thinking about how to approach knowledge spillovers would be to focus on forward citations using the International Patent Classification (IPC) or Cooperative Patent Classification (CPC). This approach would define knowledge spillovers in terms of the impacts of a claimed invention outside the technology space of the invention. That is, upon other areas of the patent system. Figure 6.9 displays the IPC Subclasses for the documents in the source family (origin) and the first and second generations of citations. Figure 6.9: IPC Subclasses by Citing Generations Figure 6.9 reveals that the top IPC subclass is for biotechnology and genetic engineering followed by Pharmaceuticals and Medicines. As we move from the source family into the first generation of citations (x documents) and the second generation (x documents) this pattern intensifies. As we move into the first and second generation we can see that some IPC categories actually decrease, such as new breeds of animals under A01K (for animal related biotechnology) although we would probably not want to read too much into this for the recent documents. As noted above, one feature of citations is that a citation may be awarded for a technical feature that is not central to the claimed invention or wider technology area. In the case of the Broad Institute CRISPR patent family the appearance of IPC subclasses for brushes and dentistry reflect this. These documents in the second generation actually focus on an invention encouraging children to brush their teeth and have no relationship with CRISPR or genetic engineering. As such, we should bear in mind that apparent spillovers across generations of citations may not involve the core features of a cited invention. Additional factors to consider when considering citations are the sources of citations. Many analysts might privilege citations awarded by examiners with an impact on the claimed invention (marked with X or Y). This would sharpen the focus of the citing network. Other factors to consider would be that patent documents are typically awarded more than one IPC class and that IPCs therefore form clusters and networks. In addition, we have focused in this discussion on the Subclass level. While this is common, it is also somewhat crude as subclasses are commonly quite broad. Thus, a more detailed IPC based analysis would look at groups and possibly specific subgroups within biotechnology (C12N) and their clusters and networks. This issue is addressed in the chapter on classification but is not further explored here. Figure 6.9 also usefully highlights that we can detect trends or trajectories in technology classes across the generations of citations. The proliferation of IPCs across the generations of citations also suggests that a filing, or set of filings, may be part of one or more technology paths or trajectories. At a more advanced level, citation networks and classification codes can be used for the identification of technology paths or trajectories. The analysis of technology trajectories is often traced to the work of Dosi (1982) following on from the work of Thomas Kuhn on the structure of scientific revolutions. Work by Hummon and Dereian (1989) focused on the analysis of a set of articles and scientific events that led to the development of DNA theory. Building on work by Garfield, Sher, and Torpie (1964) they used network analysis to identify the main paths between 40 events (mainly the publications of papers) that led to the identification of the structure of DNA. Whereas earlier network analysis had focused on the analysis of the nodes within the citation network Hummon and Dereian (1989) focused on the links. Specifically, by dividing a citation network into a set of sub-graphs and calculating the strength of the links between the papers across the sub-graphs they were able to use three counts of the traversal links that revealed “define the main path through the citation network” leading to the characterization of the structure of DNA and its confirmation (Hummon and Dereian 1989). Figure 6.10 reproduces the results of this analysis where numbers in the network refer to specific papers and/or events in the story of DNA. Figure 6.10: The Main Path for DNA Theory The starting point for the main path revealed by these calculations was the Isolation of nucleic acid by Miescher in 1869 and ends with the first identification of codons producing amino acids and the role of RNA at number 40. For our purposes, the essential issue here is that the calculation of the linkages between nodes in the network, rather than analysis of the nodes as such, can identify the key trajectory or main path in the citation network. As Hummon and Dereian (1989) points out this is similar to the calculation of the centrality of nodes in a network (which is based on calculating the shortest distance that needs to be traversed between nodes in a network). Main path analysis has more recently been applied to patent citations. Here we will focus on the work of Christopher Magee at MIT and his collaborators whose recent work provides a good overview of the topic. In a review of existing work on main path analysis in areas such as Fuel Cells, Local Area Networks and the miniaturization of semi conductors, among others, Magee and collaborators observe that existing approaches run the risk of dropping important patents that contribute to the emergence of a technology field [ref]. This issue is also reflected in the problem that main path analysis typically identifies one main path when in practice there may be a range of paths representing contributing domains along with sub-domains within with a main path that lead to transformations (discontinuities). The method demonstrated by Magee and collaborators consists of the following basic steps collecting a set of patents for the technology domain. This step involves retrieving patents specific to a recognizable body of knowledge using key words, applicants or inventors. A classification overlap method (using the acronym COM) is then used to identify highly relevant documents. Essentially this step consists of identifying documents that share overlapping classifications between the now defunct United States Patent Classification (UPC) and the International Patent Classification or, in recent work, the Cooperative Patent Classification (CPC) (Park and Magee 2017; Magee et al. 2018; Benson and Magee 2012, 2014). generating the knowledge network by retrieving the back and forward citations from the initial reference set identified above. Measuring knowledge persistence. Magee et al argue that this is the key step in overcoming the limitations of other approaches to main path analysis using patent citation data (Park and Magee 2017). Highly persistent patents (that they call HPPs) are citations of patents that persist across multiple generations (layers) of citations in the backward and forward network. By searching both backwards and forwards they also argue that the problem of missing other paths can be overcome. They explain the concept of knowledge persistence as follows: ” The main concept of knowledge persistence is that a new invention is created by the recombination of existing pieces of knowledge and so, similar to Mendelian genetic inheritance, a proportion of knowledge in a patent is incorporated in its descendant patents. Therefore, in the patent system, cited and citing patents can be interpreted as ancestors and descendants from the genetic inheritance perspective.” (Park and Magee 2017):5 Tracing main paths from high persistence patents. This actually consists of the calculation of the high persistence patents at the level of layers (generations) by retrieving the forward and backward sets of each patent and across the network (global). An example of the type of analysis that results from these steps is reproduced in Figure 6.11 for the case of Solar Photovoltaic Systems (Park and Magee 2017). Figure 6.11: Main Patent Path Analysis Results for Solar Photovoltaic Systems When we consider the raw network of citations in the top left of Figure 6.11 and the two main paths revealed in the network, it becomes clear that one of the purposes of main path analysis is to reduce the complexity of citation networks by extracting the main path(s) that involve high persistence patents in the specific technology domain. In recent work this type of analysis has also been applied to trace the history of the emergence of CRISPR as a breakthrough technology from underlying genome engineering technology (Magee et al. 2018). Main path analysis led them to identify three main paths, for cloning and restriction endonucleases, for core genome editing and, for endonucleases and related enzymes. Figure 6.12 shows the main paths in the genome engineering data leading to genome editing. Figure 6.12: Core Genome Editing Path In Figure 6.12 the term RE refers to Restriction Endonuclease. What this figure tells us, is that the analysis reveals that there are six clusters of activity that make up the CRISPR main path. The colours represent the technology cluster and the numbers represent the actual patent numbers. The key features of this path are that they represent advances in synthetic restriction endonucleases using zinc finger nucleases (ZFN) and transcriptor activator like effector nucleases (TALENs). These patents in relation to genome editing in the field of genome engineering are then followed, and contribute to the rise of CRISPR. Magee et al. (2018) use a separate set of CRISPR roots (based on CPC classifiers) to explore the roots of the CRISPR patents and their overlap with the earlier genome engineering patent activity presented above. Main path analysis in the case of patent citations has emerged as an area of research for technology trajectories over the last decade or so. As the work above reveals the method continues to be refined to more accurately capture paths that contribute to an emerging breakthrough and the technological sub-clusters within an emerging technology area. The particular strength of this approach is that it reduces the complexity of citation networks and makes it easier to identify the most important paths and clusters within the network. However, a possible weakness of this method is the dependence of the classification overlap method which depends on the US classification (discontinued in 2015) and the IPC/CPC. It remains to be seen whether the use of classification codes would be as robust using purely IPC or CPC codes as a basis for selection. Nevertheless, in drawing attention to main path analysis our purpose is to highlight that citation analysis combined with classification and citation metrics is an important field of research that increasingly promises to make navigating citation networks significantly easier. Here it is important to recall that the task of the patent analyst is ultimately to recognise complexity but also reduce that complexity to accurate information that can be communicated to the relevant audience. Main path analysis could potentially become an important feature of the analytical tool kit by identifying the main clusters and way markers influencing the trajectory of a technology area. For this reason it deserves closer critical attention. References "],["textmining.html", "Chapter 7 Text Mining 7.1 The USPTO Patent Granted Data 7.2 Words 7.3 Removing Stop Words 7.4 Lemmatizing words 7.5 Terms by Technology 7.6 Combining Text Mining with Patent Classification 7.7 From words to phrases (ngrams) 7.8 Terms over time 7.9 Correlation Measures 7.10 Conclusion", " Chapter 7 Text Mining Text mining involves the extraction of information from text sources. In the case of the scientific literature and patent data, text mining is closely associated with the concept of technology or tech mining as popularised by Alan Porter and Scott Cunningham (Porter and Cunningham 2004). However, the rise and rise of social media has expanded text mining into the pursuit of insights from platforms Twitter and other social media platforms involving topic modelling and sentiment analysis to inform decision making on public perceptions of technology and responses to products. A dynamic Lens collection on text mining has been created to complement this chapter and will allow you to explore the diverse uses of text mining. What we might call classic approaches to text mining are rapidly being blended with or replaced by machine learning approaches to Natural Language Processing. We consider machine learning based approaches in the next chapter. In this chapter we focus on some of the basics of text mining and argue that rather than jumping into machine learning based approaches a great deal can be achieved using standard text mining approaches. Standard approaches to text mining have the advantage that they are relatively straightforward to implement and are transparent to the analyst. This is not always true for machine learning based approaches. Perhaps more importantly, an understanding of standard text mining techniques provides a platform for engaging with machine learning based approaches considered in the next chapter. To introduce text mining we will use the popular tidytext approach using R. This differs from the classic use of a corpus of texts and the creation of document terms matrices that you might have come across elsewhere. An major advantage of the tidytext approach is that the data is easy to understand, visualise and keep track of during transformation steps. In particular, it is easy to retain the document identifiers that are the key to joining to patent text data with other patent data. This chapter draws heavily on and reproduces code examples from Text Mining with R: A Tidy Approach by Julia Silge and David Robinson. This book is strongly recommended for anyone seeking to engage in text mining and is available free of charge at https://www.tidytextmining.com/. We will use code examples from Text Mining with R to illustrate different approaches. However, we will use a large dataset of texts from US PatentsView service in the worked examples. We will also demonstrate how the International Patent Classification allows us to focus our text mining efforts in relevant areas of the patent system. The examples in this chapter are designed to illustrate tidy text mining at the scale of millions of records. If the worked examples prove challenging on your computer we recommend that you reduce the size of the examples. The examples in Text Mining with R using the texts of Jane Austen and other classic literature are also highly accessible for working at a smaller scale. 7.1 The USPTO Patent Granted Data In the last chapter on patent classification we used the ipcr table from the US PatentsView Service to explore uses of the International Patent Classification. In this chapter we will use the patents table for patents granted in the United States. At the time of writing that table consists of 7.8 million documents with the patents table containing identifier information, the titles and the abstracts. You can download the latest version of this table to your machine from the following address. &quot;https://s3.amazonaws.com/data.patentsview.org/download/patent.tsv.zip&quot; Building on the discussion in the last chapter on patent classification we will also download the latest version of the ipcr table that is available from here. &quot;https://s3.amazonaws.com/data.patentsview.org/download/ipcr.tsv.zip&quot; One issue with .zip files is that they may not always read in correctly without being unzipped first. Unzip the files on your machine either using a built in application or programatically. We then read in the file. We will be using R and the tidytext package but it is straightforward to read in this data in Python with Pandas. We will read in a selection of the columns that we will use. To assist with joining our tables we will rename the id column in the grants table as this is called patent_id in the ipc table. We will also create a publication number by joining up the country, number and kind columns. That can assist us with looking up documents online or preparing data for joining to other databases (such as PATSTAT). We can also vary the separator e.g. use “_” depending on the format used by the external database. Table 7.1 displays a selection of the data. library(tidyverse) library(vroom) grants &lt;- vroom::vroom(&quot;data/text_mining/patent.csv.gz&quot;, show_col_types = FALSE) %&gt;% select(id, country, number, kind, title, abstract, date) %&gt;% rename(patent_id = id) %&gt;% mutate(year = lubridate::year(date)) %&gt;% unite(publication_number, c(country, number, kind), sep = &quot;&quot;, remove = TRUE) Table 7.1: A Sample of Fields in the Edited Grants Table patent_id publication_number title 10000000 US10000000B2 Coherent LADAR using intra-pixel quadrature detection 10000001 US10000001B2 Injection molding machine and mold thickness control method 10000002 US10000002B2 Method for manufacturing polymer film and co-extruded film 10000003 US10000003B2 Method for producing a container from a thermoplastic 10000004 US10000004B2 Process of obtaining a double-oriented film, co-extruded, and of low thickness made by a three bubble process that at the time of being thermoformed provides a uniform thickness in the produced tray In the previous chapter we worked with the PatentsView IPC table. The raw table contains a ‘patent_id’ field that we can link with other tables and a set of columns with different ipc levels that we can use to construct the four character subclass (e.g. C12N) with. We will use this table below with the patent_id field as the basis for joining with the titles and abstracts. Table 7.2 shows a sample of patent ids and IPC subclass from the cleaned ipcr field. This code also illustrates the use of the dplyr::case_when() function to correct missing zeros in IPC classes in the raw ipcr table as an alternative to the use of if statements. ipcr &lt;- vroom::vroom(&quot;data/text_mining/ipcr.csv.gz&quot;, show_col_types = FALSE) %&gt;% mutate(ipc_class = case_when( nchar(ipc_class) == 1 ~ paste0(0,ipc_class), nchar(ipc_class) &gt; 1 ~ ipc_class ) ) %&gt;% unite(ipc_subclass, c(section, ipc_class, subclass), sep = &quot;&quot;, remove = FALSE) %&gt;% select(patent_id, ipc_subclass) %&gt;% mutate(ipc_subclass = str_to_upper(ipc_subclass)) Table 7.2: Patent Ids and IPC Subclasses in the Cleaned IPCR Table patent_id ipc_subclass 6864832 G01S 9954111 H01L 10048897 G06F 10694566 H04W D409748 D2404 7645556 G03F 8524174 B01L 10008744 H01M 11046328 B60W 9508104 G06Q 7.2 Words A primary task in text mining is tokenizing. A token is a unit in a text, typically a word or punctuation. Phrases are formed from combinations of tokens and sentences are formed from a set of tokens with marker (a full stop) at the end of the sentence. Tokenizing is therefore the process of breaking down texts into their constituent tokens (elements). Tokenizing normally focuses on words (unigrams), phrases (bigrams or trigrams) but extends to sentences and paragraphs. The tidytext package has a function called unnest_tokens() that by default will tokenize words in a text and will also remove punctuation and turn the case to lowercase. The effect of converting to lowercase is that words such as drone, Drone or DRONE will all be converted to the same case (drone) making for more accurate groupings and counts. Removing punctuation limits the amount of pointless characters in our results. What is important about tidytext is that it preserves the patent_id as the identifier for each word. This means that we know which document each individual word appears in. This makes it very powerful for dictionary based matches of patent documents. To illustrate, we create a table called grant_words. Depending on how much memory you have this should take a few minutes to run. By default the tidytext package will convert the text to lowercase and remove punctuation. Converting to lowercase makes counts accurate and removing punctuation removes text elements that are not useful for most tasks. Note that you will not always want to convert text to lowercase. library(tidytext) grant_words &lt;- grants %&gt;% select(patent_id, publication_number, title) %&gt;% unnest_tokens(word, title) When we have processed the text we will see that out 7.9 million individual document titles have expanded to 60,839,863 rows containing words in documents as we see in Table 7.3 which shows words appearing in the titles per document. The important point here is that we know exactly what words appear in each patent document which is a very powerful tool. Table 7.3: Words Appearing in the Titles of Two Patent Documents patent_id publication_number word 10000000 US10000000B2 coherent 10000000 US10000000B2 ladar 10000000 US10000000B2 using 10000000 US10000000B2 intra 10000000 US10000000B2 pixel 10000000 US10000000B2 quadrature 10000000 US10000000B2 detection 10000001 US10000001B2 injection 10000001 US10000001B2 molding 10000001 US10000001B2 machine 10000001 US10000001B2 and 10000001 US10000001B2 mold 10000001 US10000001B2 thickness 10000001 US10000001B2 control 10000001 US10000001B2 method 10000002 US10000002B2 method 10000002 US10000002B2 for 10000002 US10000002B2 manufacturing 10000002 US10000002B2 polymer 10000002 US10000002B2 film As this makes clear, it is very easy to break a document down into its constituent words with tidytext. 7.3 Removing Stop Words As we can see in Table 7.3 there are many common words such as “and” that do not contain useful information. We can see the impact of these terms if we count up the words as we see in Table 7.4. Table 7.4: Top Words in Grant Titles word n and 3266729 for 2974726 method 1859165 of 1732262 a 1499277 system 1014262 device 997272 apparatus 978236 with 739534 the 651349 in 588004 methods 366751 an 340798 to 336512 same 300901 using 300292 control 294340 having 271477 process 244300 image 232694 In Table 7.4 we can see that common words will rise to the top but do not convey useful information. For this reason a common approach in text mining is to remove so called ‘stop words’. Exactly what counts a stop word may vary depending on the task at hand. However, certain words such as a, and, the, for, with and so on are not useful if we want to understand what a text or set of texts is about. In tidytext there is a built in table of stop words and lists of stop words can be found on the internet that you can readily edit to meet your needs. We can see some of the stop words from tidytext in Table 7.5. In reality tidytext includes three lexicons of stop words (onix, SMART and snowball) that you can use or adapt for your needs. There are also options to add your own. Table 7.5: Tidytext Stop Words word lexicon a SMART a’s SMART able SMART about SMART above SMART according SMART accordingly SMART across SMART actually SMART after SMART Applying stop words to our grant titles is straightforward. In the code below we first create a new column that matches words in the stopword list. We then filter them out (the same code can be written in various ways). grant_words_clean &lt;- grant_words %&gt;% select(patent_id, word) %&gt;% mutate(stop = word %in% tidytext::stop_words$word) %&gt;% filter(stop == FALSE) This reduces our 60.8 million row dataset to 44 million, losing nearly 17 million words. Now when we count up the words in the grants titles we should get some more useful results as we see in Table 7.6. Table 7.6: Cleaned Words in Patent Titles word n method 1859165 system 1014262 device 997272 apparatus 978236 methods 366751 control 294340 process 244300 image 232694 data 232002 display 218565 systems 201809 circuit 192200 semiconductor 187196 thereof 184853 processing 184550 vehicle 175908 assembly 174347 manufacturing 172285 power 163450 optical 156712 We can make two observations about Table 7.6. The first of these is that there are some words that appear quite commonly in patents such as “thereof” that we would want to add to our own stop words list (others might be words like comprising). However, we would want to take care with other potential stop words that may provide us with information about the type of patent claims (such as composition for composition of matter, methods and process) that we would probably want to keep for some purposes (for example if examining patent claims). 7.4 Lemmatizing words The second observation is that there are pluralised forms of some words, such as method, methods, process, processing, processes and so on. These are words that can be grouped together based on a shared form (normally the singular such as method and process). These groupings are known as lemmas. It is important to emphasise that lemmatizing is distinct from stemming words, which reduces words to a common stem. There are a variety of tools out there for lemmatizing text. We will use the textstem package by Tyler Rinker (Rinker 2018) in R which provides a range of options for lemmatizing words. In the code below we will create a new column with the lemmatized version of words called lemma. We will filter out any digits at the same time. Here we lemmatize the words that we have already applied stop words to. library(textstem) grant_lemma &lt;- grant_words_clean %&gt;% mutate(word = str_trim(word, side = &quot;both&quot;)) %&gt;% mutate(lemma = lemmatize_words(word)) %&gt;% mutate(digit = str_detect(word, &quot;[[:digit:]]&quot;)) %&gt;% filter(digit == FALSE) %&gt;% select(-digit) If we count up our lemmatized words we will see a dramatic change in the scores for terms such as ‘method’ and ‘process’ as we see in Table 7.7. Table 7.7: Outcome of Lemmatization of Words in Patent Titles lemma n method 2225916 system 1216071 device 1122573 apparatus 991158 process 451231 control 420688 It is important to be cautious with lemmatizing to ensure that you will be getting what you expect. However, it is a particularly powerful tool for harmonising data to enable aggregation for counts. As we will also see, this is an important for topic modelling and network visualization. We now have a clearer idea of the top terms that appear across the corpus of US patent grants. As we will see below, if we know what words we are interested in we could simply identify all of the documents that contain those words for further analysis. As discussed in the discussion of the patent landscape for animal genetic resources in the last chapter on classification, the use of a dictionary of terms such as pig, horse, cow and so on allows us to capture a universe of documents that contain animal related terms. However, this will also catch a lot of extraneous noise from uses of terms that are difficult for the analyst to predict in advance (such as pipeline pigs and clothes horses). What is important in the case of the patent system is that we can leverage the patent classification to help us refine our text mining efforts. To illustrate this we will start by using a well known measure known as “term frequency inverse document frequency” (TFIDF) to understand what terms are distinctive in our set for particular areas of technology. 7.5 Terms by Technology As an experiment we will see what the terms are in four areas of technology using IPC subclasses. For this experiment, we will use examples that are distinctive and some that are likely to overlap, these are: Plant agriculture (A01H), Medicines and pharmaceuticals (A61K), Biotechnology (C12N) Computing (G06F) To do this we will start by identifying the patent_ids that fall into these subclasses with the results shown in Table 7.8. ipc_set &lt;- ipcr %&gt;% select(patent_id, ipc_subclass) %&gt;% filter(., ipc_subclass == &quot;A01H&quot; | ipc_subclass == &quot;A61K&quot; | ipc_subclass == &quot;C12N&quot; | ipc_subclass == &quot;G06F&quot;) Table 7.8: IPC Subclasses for TF_IDF patent_id ipc_subclass 10048897 G06F PP10471 A01H 9600201 G06F 9138474 A61K 9898174 G06F 9836588 G06F This gives us a total of just over 2.4 million patent_ids that have one or more of these classifiers. We now join the two tables by the patent_id using inner_join. In SQL (here written with dplyr in R) an inner join is a filtering join that will keep only shared elements in the two tables (the patent_ids). We then count the words and sort them as we see in Table 7.9. ipc_words &lt;- ipc_set %&gt;% inner_join(grant_lemma, by = &quot;patent_id&quot;) %&gt;% count(ipc_subclass, lemma, sort = TRUE) Table 7.9: Examples of Lemmatized Words by IPC ipc_subclass lemma n G06F method 597874 G06F system 507874 G06F device 252371 G06F datum 210127 G06F apparatus 205122 A61K method 190228 For the calculation that we are about to make using the code provided by Silge and Robinson we first need to generate a count of the total number of words for each of our technology areas (subclasses). To do that we group the table by ‘ipc_subclass’ so that we can count up the words in each subclass (rather than the whole table). It is important to ungroup() the data at the end. The reason for this is that without ungrouping any future operations such as counts will be performed on the grouped table leading to unexpected results and considerable confusion. ipc_total_words &lt;- ipc_words %&gt;% group_by(ipc_subclass) %&gt;% summarise(total = sum(n)) %&gt;% ungroup() Next we join the two tables together using left_join(). Unlike inner_join(), left_join() will keep everything in our original ipc_words table on the left hand side (that we will be applying the TFIDF to). ipc_words &lt;- ipc_words %&gt;% left_join(ipc_total_words, by = &quot;ipc_subclass&quot;) We are now in a position to apply the term frequency inverse document frequency calculations (using bind_tf_idf) as we see in Table 7.10. library(tidytext) ipc_words_tfid &lt;- ipc_words %&gt;% bind_tf_idf(word, ipc_subclass, n) %&gt;% arrange(desc(tf_idf)) ipc_words_tfid Table 7.10: Term Frequency, Inverse Frequency and TFIDF Scores ipc_subclass word n tf idf tf_idf A01H inbred 5275 0.0149659 0.6931472 0.0103735 A01H cultivar 7995 0.0226828 0.2876821 0.0065254 A01H rose 3421 0.0097058 0.2876821 0.0027922 A01H calibrachoa 641 0.0018186 1.3862944 0.0025211 A01H hydrangea 584 0.0016569 1.3862944 0.0022969 G06F server 22500 0.0016522 1.3862944 0.0022905 The application of the bind_tf_idf function gives us a term frequency score (tf) an inverse document frequency score (idf) and a term frequency inverse document frequency score as a product of term frequency and inverse document frequency. In straightforward terms, this gives us a calculation of how distinctive a particular term is in the set of words within a document or group of documents. We can visualise the distinctive terms from the titles of publications for the four distinctive subclasses in Figure 7.1. ipc_names &lt;- c( &#39;A01H&#39;=&quot;Plant Agriculture (A01H)&quot;, &#39;C12N&#39;=&quot;Biotechnology (C12N)&quot;, &#39;A61K&#39;=&quot;Pharmaceuticals (A61K)&quot;, &#39;G06F&#39;=&quot;Computing (G06F)&quot; ) ipc_words_tfid %&gt;% arrange(desc(tf_idf)) %&gt;% mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% group_by(ipc_subclass) %&gt;% top_n(15) %&gt;% ungroup() %&gt;% ggplot(aes(word, tf_idf, fill = ipc_subclass)) + geom_col(show.legend = FALSE) + labs(x = NULL, y = &quot;tf_idf&quot;) + facet_wrap(ipc_subclass ~., labeller = as_labeller(ipc_names), ncol = 2, scales = &quot;free&quot;) + coord_flip() Figure 7.1: TFIDF scores for distinctive terms in patent titles by IPC subclass In the original example that we are copying here from Silge and Robinson, the different novels written by Jane Austen were mainly distinguished not by the language used but by the names (proper nouns) of the individual characters in each novel, such as Darcy in Pride and Prejudice and Emma and Knightley in Emma. In contrast, when ranked by tfidf our patent data displays very distinctive terms associated with each subclass, with some overlap between A01H and C12N. In the case of plant agriculture and biotechnology this will arise from genetically modified plants (which fall into both categories of the system), Note that in the case of A61K we appear to have some partial words (e.g. yl) arising from the process of splitting the text into tokens. A solution here would be to filter out words with 2 or 3 characters or less unless there is a good reason not to. This example quite vividly illustrates the point that term frequency inverse document frequency, and its multiple variants in use by search engines, is a very powerful means of picking out distinctive terms in groups of documents and for modelling the topics that these documents are about. However, these examples also illustrate the importance of the patent classification. 7.6 Combining Text Mining with Patent Classification In reality, the distinctive terms that we observe in Figure 7.1 reinforce the point made in the last chapter: the patent classification is our friend. In contrast with many other forms of document systems the patent system uses a sophisticated human and machine curated international classification system to describe the content of patent documents. The importance of engaging with the classification to assist with text mining is revealed by the terms in the different subclasses: the classification does a lot of work for us. Once we understand the classification we are able to target our attention to areas of the system that we are interested in. Put another way, rather than starting by text mining the titles or other text elements of all patent documents we will save ourselves a lot of time and effort by targeting our efforts to specific areas of the system using the international patent classification as our guide. We can briefly illustrate this point using the topic of drones using the patent titles. grant_lemma %&gt;% filter(lemma == &quot;drone&quot;) %&gt;% inner_join(ipcr, by = &quot;patent_id&quot;) %&gt;% count(ipc_subclass, sort = TRUE) ipc_subclass n B64C 816 G05D 567 B64D 368 H04W 368 G08G 319 G01S 267 In practice, we would want to use larger text segments such as titles and abstracts rather than simply the titles (see below). However, this example illustrates that if we wanted to start exploring the topic of drone technology we would probably want to include B64C (Aeroplanes and Helicopters), G05D (control systems) and B64D (aircraft equipment) as part of our search strategy for the straightforward reason that these are areas of the patent library where documents containing these words are to be found. We can scale up this example to focus on a broader topic area of wider policy interest: biodiversity. In previous work, Oldham, Hall and Forero text mined the full text of 11 million patent documents for millions of taxonomic species names using regular expressions (P. Oldham, Hall, and Forero 2013). This research also revealed that it is possible to capture the majority of biodiversity related terms by focusing on specific areas of the patent classification. Use of a set of English terms within those areas of the system would then capture the universe of documents that need to be captured. The two elements of that search strategy involved a set of terms that commonly appear in biodiversity related documents and a set of IPC classes. The search terms are presented below. As these are search terms that are designed to be used in a search engine they include plurals and are not stemmed to avoid capturing many irrelevant terms. biodiversity_ipc_words &lt;- tibble(word = c(&quot;species&quot;,&quot;genus&quot;,&quot;family&quot;,&quot;order&quot;,&quot;phylum&quot;,&quot;class&quot;,&quot;kingdom&quot;,&quot;tribe&quot;,&quot;dna&quot;, &quot;nucleic&quot;,&quot;nucleotide&quot;,&quot;amino&quot;,&quot;polypeptide&quot;,&quot;sequence&quot;,&quot;seq&quot;,&quot;seqid&quot;, &quot;protein&quot;, &quot;proteins&quot;,&quot;peptides&quot;,&quot;peptide&quot;,&quot;enzyme&quot;,&quot;enzymes&quot;,&quot;plant&quot;,&quot;plants&quot;,&quot;animal&quot;,&quot;animals&quot;,&quot;mammal&quot;, &quot;mammals&quot;,&quot;mammalian&quot;,&quot;bacteria&quot;,&quot;bacterium&quot;,&quot;protozoa&quot;,&quot;virus&quot;,&quot;viruses&quot;,&quot;fungi&quot;,&quot;animalia&quot;,&quot;archaea&quot;, &quot;chromista&quot;,&quot;chromist&quot;,&quot;chromists&quot;,&quot;protista&quot;,&quot;protist&quot;,&quot;protists&quot;,&quot;plantae&quot;,&quot;eukarya&quot;,&quot;eukaryotes&quot;, &quot;eukaryote&quot;,&quot;prokarya&quot;,&quot;prokaryote&quot;,&quot;prokaryotes&quot;,&quot;microorganism&quot;,&quot;microorganisms&quot;,&quot;organism&quot;,&quot;organisms&quot;, &quot;cell&quot;,&quot;cells&quot;,&quot;gene&quot;,&quot;genes&quot;,&quot;genetic&quot;,&quot;viral&quot;,&quot;biological&quot;,&quot;biology&quot;,&quot;strain&quot;,&quot;strains&quot;,&quot;variety&quot;,&quot;varieties&quot;, &quot;accession&quot;)) For this example we will scale up to include words from both the titles and abstracts of US granted patents. We will do this by uniting the titles and the abstracts into one string and then breaking that into words. Note that this is RAM intensive and if working on a machine with limited RAM you may wish to filter the grants data to a specific year to try this out. As an alternative, it is now very cost effective to create a powerful virtual machine with a cloud computing service (Amazon Web Services, Google Cloud, Microsoft Azure etc.) that can be destroyed when a task is completed. The use of cloud computing is beyond the scope of this Handbook but is extremely useful for working with data at scale using either VantagePoint (on Windows Server) or with programming languages such as R, Python, Spark or Google Big Query. Many guides to getting started with creating virtual machines are available online and packages exist in R and Python that allow you to create and manage virtual machines from your desktop. Other options include Remote Desktop (for accessing remote virtual Windows machines as if they are your local desktop). We start by restricting the data to the identifier and title and abstract which we combine together before splitting into words. We then remove stop words. For this illustration we will not perform additional clean up steps. words_ta &lt;- grants %&gt;% select(patent_id, title, abstract) %&gt;% unite(text, c(title, abstract), sep = &quot;. &quot;) %&gt;% unnest_tokens(word, text) %&gt;% mutate(stop = word %in% tidytext::stop_words$word) %&gt;% filter(stop == FALSE) %&gt;% select(-stop) patent_id word 10000000 coherent 10000000 ladar 10000000 intra 10000000 pixel 10000000 quadrature 10000000 detection This gives us a raw dataset with 893,067,757 rows that reduces to 475,833,395 when stop words are removed. We now match our cleaned up terms to our biodiversity dictionary. We now want to reduce the set to those documents that contain our biodiversity dictionary. With this version of the US grants table we obtain a ‘raw hits’ dataset with 2,692,948 rows and 805,675 raw patent grant documents. In the second step we want to join to the IPC to see what the top subclasses are. We combine these steps in the code below. We use inner_join to filter the documents to those containing the biodiversity terms and left_join for the IPC. biodiversity_words &lt;- words_ta %&gt;% inner_join(biodiversity_ipc_words, by = &quot;word&quot;) %&gt;% left_join(ipcr, by = &quot;patent_id&quot;) This will produce a dataset with our biodiversity terms and 2084 IPC subclasses. Table 7.11 shows the top 20 subclasses. Table 7.11: Top 20 IPC Subclasses for Biodiversity Terms ipc_subclass n C12N 1151393 A61K 1034746 A01H 590274 C07K 514985 H01M 348670 G01N 344056 H01L 323959 G11C 309924 C12Q 288365 H04W 259230 G06F 247670 C12P 245579 C07H 220449 C07D 175588 H04L 136105 A01N 134383 H04N 80492 A61B 76849 C07C 64242 H04B 55282 A total of 1,773 IPC subclasses include our biodiversity terms. In considering Table 7.11 we can make three key observations. The first of these is that we have a clear concentration of the biodiversity related terms in certain areas of the patent system. The second observation is that this data includes subclasses in section G and H of the classification that are very unlikely to have anything to do with biodiversity as such. For example, words in our set such as ‘order’ for the taxonomic rank are likely to create a lot of noise from systems that involve ordering in all its various forms (and should perhaps be removed). The third observation is that we can safely exclude areas that are likely to constitute noise by classification. One feature of the patent system is that documents receive multiple classification codes to describe the contents of a document. Excluding subclasses such as G01N will only have the effect of excluding documents that don’t contain one of the other ‘keep’ classifiers. That is, documents classified as both C12N and GO1N will be retained because of the presence of C12N. Drawing on this logic we can arrive at a set of subclasses that can be used as a filter for the biodiversity terms and constitute core classifiers for biodiversity. Note that in one case (subclass C02F) we would confine a search engine based search to C02F3 (for biological treatments of waste water and sewage) as the only relevant group in the subclass. biodiversity_ipc_subclass &lt;- tibble( ipc_subclass = c(&quot;A01H&quot;,&quot;A01K&quot;,&quot;A01N&quot;,&quot;A23L&quot;,&quot;A23K&quot;,&quot;A23G&quot;,&quot;A23C&quot;,&quot;A61K&quot;,&quot;A61Q&quot;, &quot;C02F&quot;,&quot;C07C&quot;,&quot;C07D&quot;,&quot;C07H&quot;,&quot;C07K&quot;,&quot;C08H&quot;,&quot;C08L&quot;,&quot;C09B&quot;,&quot;C09D&quot;, &quot;C09G&quot;,&quot;C09J&quot;,&quot;CO9K&quot;,&quot;C11B&quot;,&quot;C11C&quot;,&quot;C11D&quot;,&quot;C12M&quot;,&quot;C12N&quot;,&quot;C12P&quot;, &quot;C12Q&quot;,&quot;C12R&quot;,&quot;C12S&quot;,&quot;C40B&quot;)) At present we have a set of biodiversity words. What we want to do next is to filter the documents to the records that contain a biodiversity word AND appear in one of the subclasses above. We then want to count up the patent_ids and obtain the grants (containing the titles, abstracts and other information) for further analysis. We achieve this by first filtering the data to those containing the subclasses, then we count the patent identifiers to create a distinct set and join on to the main patent grants table using the patent ids. Table 7.12 shows the outcome of joining the data back together again. Table 7.12: Joining biodiversity words filtered by IPC subclasses to identify the original texts patent_id publication_number title 10000393 US10000393B2 Enhancement of dewatering using soy flour or soy protein 10000397 US10000397B2 Multiple attached growth reactor system 10000427 US10000427B2 Phosphate solubilizing rhizobacteria bacillus firmus as biofertilizer to increase canola yield 10000443 US10000443B2 Compositions and methods for glucose transport inhibition 10000444 US10000444B2 Fluorine-containing ether monocarboxylic acid aminoalkyl ester and a method for producing the same 10000446 US10000446B2 Amino photo-reactive binder At the end of this process we have 345,975 patent grants that can used for further analysis using text mining techniques. Our purpose here has been to illustrate how the use of text mining can be combined with the use of the patent classification to create a dataset that is much more targeted and a lot easier to work with. Put another way, it is a mistake to see the patent system as a giant set of texts that all need to be processed. It is better to approach the patent system as as a collection of texts that have already been subject to multi-label classification. That classification can be used to filter to collections of texts for further analysis. More advanced approaches to those suggested here for refining the texts to be searched, such as the use of matrices and network analysis were discussed in the previous chapter and we return to this topic below. Other options include selecting texts on the IPC group or subgroup level (bearing in mind that for international research not all patent offices will consistently use these levels). The key point here however is that we have moved from a starting set of 7.9 million patent documents and reduced the set to 338,837 documents that are closer to a target subject area. In the process we have reduced the amount of compute effort required for analysis and also the intellectual effort required to handle such large volumes of text. 7.7 From words to phrases (ngrams) We have focused so far on text mining using individual word tokens (unigrams). However, in many cases what we are looking for will be expressed in a phrase consisting of two (bigram) or three (trigram) strings of words that articulate concepts or are the names of entities (e.g. species names). The tidytext package makes it easy to tokenize texts into bigrams or trigrams by specifying an argument to the unnest_tokens() function or using the unnest_ngrams() function. We will focus here on bigrams. Note that for chemical compounds which often consist of strings of terms linked by hyphens, attention would need to be paid to adjusting the tokenisation to avoid splitting on hyphens. biodiversity_texts &lt;- biodiversity_patent_ids %&gt;% unite(text, c(title, abstract), sep = &quot;. &quot;) Table 7.13 shows a selection of the bigrams appearing in the biodiversity related texts. Table 7.13: Biodiversity Related Bigrams patent_id n publication_number bigram 10000393 12 US10000393B2 enhancement of 10000393 12 US10000393B2 of dewatering 10000393 12 US10000393B2 dewatering using 10000393 12 US10000393B2 using soy 10000393 12 US10000393B2 soy flour 10000393 12 US10000393B2 flour or We have created a dataset of bigrams that contains 31,612,494 rows with a few lines of code. However, if we inspect the bigrams we will see that the data contains phrases including many stop words. In reality, in patent analysis we are almost always interested in nouns, proper nouns and noun phrases. To get closer to what we want, Silge and Robinson present a straightforward approach to removing stop words that involves three steps: splitting the bigrams into two columns containing unigrams using the space as the separator Filtering out stop words in the two columns Combining the two columns back together again. These steps comply with a common analysis pattern called split - apply - combine. In this case we split up the texts, then apply a function to transform the data and recombine. Becoming familiar with this basic formula is helpful in thinking about the steps involved in text mining and in data analysis in general. Depending on the size of the dataset this will take some time to run because it iterates over word one and word two columns identifying stop words in each row. clean_biodiversity_bigrams &lt;- biodiversity_bigrams %&gt;% separate(bigram, into = c(&quot;one&quot;, &quot;two&quot;), sep = &quot; &quot;) %&gt;% filter(!one %in% tidytext::stop_words$word) %&gt;% filter(!two %in% tidytext::stop_words$word) %&gt;% unite(bigram, c(one, two), sep = &quot; &quot;) As before this will radically reduce the size of the dataset to 9,321,285 although the set may still contain many irrelevant phrases as we can see in Table 7.14. Table 7.14: Biodiversity Bigram Terms patent_id n publication_number bigram 10000393 12 US10000393B2 soy flour 10000393 12 US10000393B2 soy protein 10000393 12 US10000393B2 protein dewatering 10000393 12 US10000393B2 dewatering agents 10000393 12 US10000393B2 dewatering wastewater 10000393 12 US10000393B2 wastewater slurries We now have a set of bigrams with many irrelevant phrases. We could simply filter these phrases for terms of interest. In the chapter on patent citation analysis we focus on genome editing technology which is closely linked to genome engineering and synthetic biology. The extraction of bigrams allows us to identify patent grants containing these terms of interest in the title or abstracts as we can see in the code below and Table 7.15. clean_biodiversity_bigrams %&gt;% filter(bigram == &quot;genome editing&quot; | bigram == &quot;gene editing&quot; | bigram == &quot;crispr cas9&quot; | bigram == &quot;synthetic biology&quot; | bigram == &quot;genome engineering&quot;) %&gt;% count(bigram) Table 7.15: Genome Editing Bigrams patent_id n publication_number bigram 10000800 4 US10000800B2 synthetic biology 10006054 12 US10006054B1 genome editing 10011850 12 US10011850B2 genome editing 10011850 12 US10011850B2 genome editing 10017825 12 US10017825B2 genome editing 10047355 54 US10047355B2 gene editing This is a powerful technique for identifying useful documents based on dictionaries of terms (bigrams or unigrams). At an exploratory stage it also be very useful to arrange a bigrams set alphabetically so that you can see what terms are in the immediate vicinity of a target term. This can pick up variants that will improve data capture and links to correlations between terms discussed below. However, we are still dealing with 9.5 million terms. Can we make this more manageable? The answer is yes. We can apply the tf_idf calculations we used for unigrams above to the bigrams. In this case we are applying the tf_idf calculation to the 345,975 patent grants in the biodiversity set (rather than IPC subclasses) to ask the question: What bigrams are distinctive to each document? As with the previous example, the code provided by Silge and Robinson in Text Mining with R is straightforward (Silge and Robinson 2016). biodiversity_bigrams_tfidf &lt;- clean_biodiversity_bigrams %&gt;% count(patent_id, publication_number, bigram) %&gt;% bind_tf_idf(bigram, patent_id, n) %&gt;% arrange(desc(tf_idf)) patent_id publication_number bigram n tf idf tf_idf 10081800 US10081800B1 lactonase enzymes 1 1 12.75405 12.75405 10721913 US10721913B2 drop feeder 2 1 12.75405 12.75405 10993976 US10993976B2 treating uremia 2 1 12.75405 12.75405 3960890 US3960890A production fluoroalkylphenylcycloamidines 1 1 12.75405 12.75405 4077976 US4077976A benzamides employed 1 1 12.75405 12.75405 4649039 US4649039A radiolabeling methionine 1 1 12.75405 12.75405 In the example above we focused in on genome editing and related topics by filtering the bigrams table to those documents containing those terms. This produced 212 patent documents containing the term. In the next step we calculated the tf_idf scores for the biodiversity bigrams which produced a table with 7,497,419 distinctive bigrams compared with the 9,538,209 cleaned bigrams that we started with. The question here is did we lose anything by applying tf_idf? The answer is no. The tf_idf results include the 212 documents relating to genome editing, the same as before. In this case tf_idf has made the important contribution of limiting the data to distinctive terms per document and in the process reducing the amount of data that we have to deal with. In short, tf_idf can be a very useful short cut in our workflow. biodiversity_bigrams_tfidf %&gt;% filter(bigram == &quot;genome editing&quot; | bigram == &quot;gene editing&quot; | bigram == &quot;crispr cas9&quot; | bigram == &quot;synthetic biology&quot; | bigram == &quot;genome engineering&quot;) %&gt;% count(patent_id) %&gt;% nrow() ## [1] 326 In the next step we want to identify the patent_ids for our genome editing set and then create a table containing all of the distinctive bigrams. This is a very powerful technique for identifying useful documents based on dictionaries of bigrams (or trigrams) for analysis because bigrams and trigrams typically convey meaningful information (such as concepts) that individual word tokens commonly do not. For the patent analyst working on a specific topic it is often straightforward to create a workflow that involves: Initial exploration with one or more keywords as a starter set; Identify relevant ipc subclasses or groups to restrict the data; Retrieve the documents; Create bigrams for the titles, abstracts, descriptions and claims for in depth analysis. Leverage tf_idf scores per document (or other relevant grouping) to identify distinctive terms and filter again. The use of tf_idf scored is forms part of a process called topic modelling whereby statistical measures are applied to make predictions about the topics that a document or set of documents are about. This in turn is linked to a variety of approaches to creating indicators of technological emergence with which we will conclude this handbook. One very useful approach to topic modelling and technological emergence is to measure this emergence of particular words or phrases over time. 7.8 Terms over time Our aim in examining the use of terms over time is typically to gain visual information on the following issues: The first emergence of a term; Trends in the frequency of use of a term over time; The most recent use of a term or terms. We can graph the emergence of terms by linking our genome editing terms to the patent publication year. To do that we need to link our patent documents to the year in the grants table. years &lt;- grants %&gt;% select(patent_id, year) ge_year_counts &lt;- gene_editing_bigrams %&gt;% left_join(years, by = &quot;patent_id&quot;) %&gt;% group_by(year) %&gt;% count(bigram) %&gt;% ungroup() Figure 7.2 shows trends in the use of the genome editing terms by term. ge_year_counts %&gt;% ggplot(aes(year, n)) + geom_point() + geom_smooth() + facet_wrap(~ bigram, scales = &quot;free_y&quot;) + scale_y_continuous() + labs(y = &quot;Frequency of genome editing words&quot;) Figure 7.2: Frequency Trends for Genome Editing Terms This example illustrates that we can readily map the emergence of terms and the frequency of their use in patent data. It is important to point out some of the limitations of this approach that will be encountered. In the case of the present data we are working with the US patent grants data. The calculation is therefore for the emergence of terms in granted patents. As such, this does not apply to patent applications unless we explicitly include that table. A second limitation, in terms of US data, is that in the United States patent documents were only published when they were granted. It was only in 2001 that the US started publishing patent applications. Third, the data is based on publication dates. The earliest use of a term will occur in a priority application (the first filing). To map trends in the emergence of concepts over time we would therefore preferably use the priority date. In the latter case, as the actual priority document, such as as US provisional application, may not be published we are making an assumption that the terms appeared in the documents filed on the earliest priority data. A more fundamental issue however is that our analysis of trends is restricted to the titles and abstracts of the US patent collection. For a more comprehensive and accurate treatment we would want to extend the analysis to the description and claims. This would considerably expand the size of the data we would need to work with and thus demand engagement with cloud computing and the use of tools such as Apache Spark (for parallel computing). As such, in reality when mapping trends in the emergence of terms and in potentially seeking to forecast likely trends based on the existing data we would want to make some adjustments to this approach for patent data. Nevertheless, mapping of the emergence of terms is relatively straightforward. A simpler example on which the example above is based is available in Silge and Robinson 2017 (at pages 76 to 77), using texts from presidential inaugural speeches. 7.9 Correlation Measures Word and phrases in a text exist in relationship to other words and phrases in the text. As we will see in the chapter on machine learning, an understanding of these relationships and methods for calculating and predicting these relationships have been fundamental to advances in Natural Language Processing in recent years. One of the most useful of these measures in text mining is co-occurrence. That is, what words or phrases occur with other words and phrases in a text or group of texts. In the discussion above we created a data frame containing the distinctive bigrams in each text associated with genome editing. We can take a look at the phrases linked to our target terms in Figure 7.3 below. Table 7.16: Genome editing phrases patent_id publication_number bigram n tf idf tf_idf 10000800 US10000800B2 oligonucleotide constructs 3 0.1578947 10.114995 1.5971045 10000800 US10000800B2 validated sequences 2 0.1052632 10.808142 1.1376992 10000800 US10000800B2 analysis polymorphism 1 0.0526316 10.808142 0.5688496 10000800 US10000800B2 biology quantitative 1 0.0526316 10.808142 0.5688496 10000800 US10000800B2 constructs sets 1 0.0526316 10.808142 0.5688496 10000800 US10000800B2 desired rois 1 0.0526316 10.808142 0.5688496 10000800 US10000800B2 provide validated 1 0.0526316 10.808142 0.5688496 10000800 US10000800B2 validated rois 1 0.0526316 10.808142 0.5688496 10000800 US10000800B2 mutation screening 1 0.0526316 10.451467 0.5500772 10000800 US10000800B2 quantitative nucleic 1 0.0526316 9.981464 0.5253402 10000800 US10000800B2 synthetic biology 1 0.0526316 9.421848 0.4958867 10000800 US10000800B2 acid analysis 1 0.0526316 7.369557 0.3878714 10000800 US10000800B2 acid constructs 1 0.0526316 5.695294 0.2997523 10000800 US10000800B2 wide variety 1 0.0526316 5.106266 0.2687509 10000800 US10000800B2 nucleic acid 2 0.1052632 2.549202 0.2683370 10006054 US10006054B1 encoding genome 1 0.0476190 11.367758 0.5413218 10006054 US10006054B1 include inserts 1 0.0476190 11.367758 0.5413218 10006054 US10006054B1 inserts encoded 1 0.0476190 11.367758 0.5413218 10006054 US10006054B1 ogrna targeting 1 0.0476190 11.367758 0.5413218 10006054 US10006054B1 rna ogrna 1 0.0476190 11.367758 0.5413218 What we are seeking to understand using this data is what are the top co-occurring terms. To do this we will cast the data into a matrix where the bigrams are mapped against each other producing a correlation value. Using the widyr package we then pull the data back into columns with the recorded value. There are a variety of packages for calculating correlations and cooccurrences with texts. We will use udpipe package by Jan Wijffels for illustration of this approach (Wijffels 2022). The widyr package offers a pairwise_count() function that achieves the same thing in fewer steps. However, udpipe is easy to use and it also offers additional advantages such as parts of speech tagging (POS) for nouns, verbs and adjectives (Robinson 2021). First, we calculate term frequencies for our bigrams (in practice we have these but we illustrate from scratch here) using the patent_id as the identifier and the bigram. Next, we cast the data into a matrix of the bigrams against the bigrams (dtm). This transforms our dataset containing 4,430 rows into a large and sparse matrix with 3.6 million observations (where most values are zero because there is no correlation). Then apply a correlation function (in this case Pearson’s, also known as Pearson’s R, but a range of other correlation functions are available) to obtain the correlation coefficient. Finally, the matrix is transformed back into a data.frame that drops empty (0) items in the sparse matrix. In the cooccurrence data frame term1 is the source and term2 is the target with a range of scores expressed from -0.n to 1. library(udpipe) dtf &lt;- document_term_frequencies(gene_editing_tfidf, document = &quot;patent_id&quot;, term = &quot;bigram&quot;) dtm &lt;- document_term_matrix(dtf) correlation &lt;- dtm_cor(dtm) cooccurrence &lt;- as_cooccurrence(correlation) The cooccurrence matrix contains 3.6 million rows of co-occurrences between bigrams in the dataset. We filter the dataset to “genome editing” in Figure 7.3 below in order to see the outcome for one of the terms. Figure 7.3: Cooccurrence Scores for Gene Editing The cooccurrence value for the matching term (in this case “genome editing”) is always 1. Inside the actual matrix this displays on the diagonal in the centre of the matrix and gives rise to the expression “removing the diagonal” or self-reference. We can do this by filtering to keep any value that does not equal 1 as we see in Table 7.17. Table 7.17: Ranked Cooccurrence Scores for Gene Editing with Diagonal Removed term1 term2 cooc genome engineering genome editing -0.3913164 genome editing genome engineering -0.3913164 genome editing gene editing -0.3725150 gene editing genome editing -0.3725150 genome engineering gene editing -0.2669573 gene editing genome engineering -0.2669573 The data from the title and abstracts using tf_idf now suggests the strong correlation between gene editing and genome engineering in our example dataset. In common with the network visualisations discussed in the last chapter, this data can also be visualised in a network where term1 constitutes the source node, term2 the target node and cooc (or n) the weight of the edge between source and target nodes. For simplicity in presentation we will select only terms with a cooccurence score over .10 for genome editing. The code here is adapted directly from the udpipe documentation. library(igraph) library(ggraph) cooccurrence_clean %&gt;% filter(term1 == &quot;genome editing&quot;) %&gt;% arrange(cooc) %&gt;% filter(cooc &gt; .10) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(width = cooc, edge_alpha = cooc), edge_colour = &quot;pink&quot;) + geom_node_text(aes(label = name), col = &quot;darkgreen&quot;, size = 2) + theme_graph(base_family = &quot;Arial Narrow&quot;) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;Genome Editing Cooccurrence Network&quot;) Figure 7.4: Genome Engineering Network The visualisation of networks is a powerful tool for making decisions about how to proceed with analysis. In particular, visualisation of networks of terms is an extremely useful device when seeking directions for further analysis, for example on the use of genome editing in therapeutics or in agriculture and other applications. The use of these methods is not confined to patent researchers with programming skills. VantagePoint from Search Technology Inc provides many of these tools out of the box and has considerable strength of allowing greater freedom and precision in interactive exploration and refinement of the data. 7.10 Conclusion Recent years have witnessed a dramatic transformation in the availability of patent data for text mining at scale. The creation of the USPTO PatentsView Data Download service, formatted specifically for patent analysis, represents an important landmark as does the release of the full texts of EPO patent documents through Google Cloud. Other important developments include the Lens Patent API service that provides access to the full text of patent documents under a range of different plans, including free access. It remains to be seen whether WIPO will follow these developments by making the full texts of PCT documents freely available for use in patent analytics. Growing access to data also presents challenges, such as how to download, store and update large patent datasets. A second challenge is how to transform such data from XML or JSON (from calls to APIs) into formats that can be used for analysis. Finally, analysis steps themselves require data cleaning, text mining skills, some statistical and machine learning skills. In many cases software packages in R and Python or the use of VantagePoint ease the way in creating patent analytics workflows. However, the immediate practical challenge for the patent analyst will be text mining with datasets that are either too large for memory or are unwieldy on their platforms. We close this chapter with some suggestions on ways forward. In the preceding discussion we suggested that there is a process for working with textual data at scale. This consists of the following steps. Use text mining with a range of terms to identify other terms that define the universe of things you are interested in; Identify the relevant areas of the patent classification that the terms fit into; Filter the data down to the selected terms and patent classification codes; Use measures such as term frequency inverse document frequency to identify distinctive terms in your data either at the level of words or phrases and experiment until the set meets your needs; Section the data into groupings for analysis using either the IPC as a guide, groupings of terms arising from tfidf and/or use Latent Dirichlet Allocation (LDA) and similar techniques for topic modelling; Use experimental visualisation (such as network visualisation) along the way to test and refine your analysis. As analysis proceeds text mining will increasingly move into matrix operations and correlation and co-occurrence measures to identify and examine clusters of terms. These operations are directed to opening the path to producing analytical outputs on your topic of interest that are meaningful to non-patent specialists. Rather than seeing the steps outlined above as obligatory it is important to recognise when and where to use particular tools. For example, it turned out that there was not much to be gained from LDA on genome editing in terms of topic modelling because the topic had already been identified. As such, identifying the appropriate tool for the task at a particular moment in the workflow is an important skill. It is also important to recognise that analysts seeking to reproduce the steps in this Chapter will often be pushing the boundaries of their computing capacity. Here is is important to emphasise that an important principle when working with data at scale is to identify the process for reducing scale to human manageable levels as soon as is practical. It is inevitable however, that working at scale creates issues where data will not fit into memory (out of memory or oom) or processing capacity is insufficient for timely analysis. Of all the tasks involved in patent analytics, text mining and machine learning rapidly push at the boundaries of computing capacity. The solution to compute limitation is not to buy every bigger computers but to recognise that extra capacity can be acquired in the cloud through companies such as Amazon Web Services, Google Cloud or Microsoft Azure (among many others). For example, a large Windows Server can be set up to run VantagePoint on large datasets or virtual machines for work with Python and R. For large scale parallel computing of texts services such as Databricks Apache Spark clusters can be created that will cost a few dollars an hour to run. When processing is completed data can be exported and analysis and payment for the service can stop. This ability to scale up and scale down computing capacity as required is the major flexibility offered by cloud computing services and we recommend exploring these options if you intend to work with text data at scale. Many tutorials exist on setting up virtual machines and clusters with different services. The text mining techniques introduced in this chapter are part of a wider set of techniques that can be tailored for specific needs. However, rapid advances in machine learning in recent years are transforming natural language processing. The topics discussed in this chapter are a useful foundation for understanding these transformations. We now turn to the use of machine learning in patent analytics. References "],["machinelearning.html", "Chapter 8 Machine Learning 8.1 From Dictionaries to Word Vectors 8.2 Word Vectors with fastText 8.3 Training Word Vectors for Drones 8.4 Using Word Vectors 8.5 Exploring Analogies 8.6 Patent Specific Word Embeddings 8.7 Machine learning in Classification 8.8 Machine Learning for Text Classification 8.9 From Vector Based Models to Transformers 8.10 Conclusion", " Chapter 8 Machine Learning In recent years Artificial Intelligence has become a focus of discussion for its potentially transformative and disruptive effects on economy and society. The recent rise of artificial intelligence in the patent system is the focus of a landmark 2019 WIPO Technology Trends Report “Artificial Intelligence”. The in depth review of patent activity revealed that it is one of the fastest growing areas of patent activity with inventions that are applicable across diverse fields such as telecommunications, transportation, life and medical sciences, personal devices and human-computer interactions. Sectors identified in the WIPO report include banking, entertainment, security, industry and manufacturing, agriculture and networks. Perhaps the best known “flagship” initiative for artificial intelligence is the pursuit of self-driving cars by Tesla and Google among others. However, many companies, including those that work in the domain of patent analytics, are increasingly claiming that they apply artificial intelligence as part of their products. When approaching artificial intelligence it is important to look beyond the hype and marketing strategies to the underlying technology. In practical terms this can be described as computer based approaches to classification with respect to images and texts. The dominant approach to classification involves a range of computational machine learning approaches that have been undergoing rapid development in recent years. Examples of the use of machine learning approaches to classification tasks include predictive text entry on mobile phones, a technology anticipated in the 1940s by researchers in China See Wikipedia. A patent for an assistive device for deaf people involving predictive text was awarded in 1988 (US4754474A). The rise of mobile phones witnessed an explosion in the development and use of predictive text applications. Predictive text is also widely used in search engines to suggest phrases that a user may wish to use in their search. Other applications include spam filtering for emails or suggesting similar items that a customer might be interested in on online shops. While text classification is perhaps the main everyday area where machine learning is encountered in practice image classification has been the major focus of development and is reflected in the prominence of image classification challenges on Kaggle. The implementation of image classification approaches is reflected in everyday terms in image searches in online databases which suggest relevant images and suggestions for tagging of images and persons in social media applications. Image classification is also an important area of innovation in areas such as medical diagnostics, robotics, self-driving cars or facial recognition for security systems. A separate but less visible area of development is control systems. The online data science platform Kaggle serves as a host for competitions and challenges in machine learning such as image classification and can provide an insight into the nature of machine learning developments. A 2017 report by the UK Royal Society “Machine learning: the power and promise of computers that learn by example” provides a valuable overview of machine learning approaches.25. For our purposes, the Royal Society report highlights the key underlying feature of machine learning: learning by example. As we will see in this chapter machine learning approaches commonly involve training a statistical model to make predictions about patterns (in texts or in images). Training of machine learning models is normally based on the use of examples. The quality of the predictions that are produced by a model is heavily dependent on the quality and the number of examples that it is trained on. The development of machine learning models proceeds in a cycle from the collection and pre-processing of training data, to the development of the model with the training data followed by evaluation of the performance of the model against previously unseen data (known as the evaluation or test set). Based on the results more training data may be added and the parameters of the model adjusted or tuned to optimise performance. When a robust model has been developed it can then be used in production to automate the classification tasks. Machine learning involves a range of different algorithms (that may at times be used in combination), examples include the well known Principal Component Analysis (PCA), linear regression, logistic regression (for classification), decision-trees, K-means clustering, least squares and polynomial fitting, and neural networks of a variety of types (e.g convolutional, recurrent and feed forward). Some of the algorithms used in machine learning predate the rise in popularity of the term machine learning and would not be accepted as machine learning (e.g. PCA and regression models). Readers interested in learning more about the algorithms involved in machine learning will discover a wide range of often free online machine learning courses such as from popular platforms such as Coursera, Udemy, Edx and Data Camp to name but a few. For text classification the Stanford Course “Stanford CS224N: NLP with Deep Learning” provides 20 hours of video lectures that provides a detailed insight into many of the topics addressed in this chapter.26 However, while it is important to engage with the background to machine learning algorithms, in reality machine learning is becoming increasingly accessible for a range of classification tasks in two ways. through fee based online cloud services offered by Google, Amazon, Microsoft Azure and others that will perform specific classification tasks at scale such as image classification without a requirement for advanced training; the availability of free open source packages such as scikit learn, fastText (Facebook), keras and spaCy (Explosion AI) One of the challenges writing about machine learning is making the processes involved visible. To make it easier to engage with machine learning we will use the free Python Natural Language Processing library spaCy and the associated fee based Prodigy annotation and model training tool from Explosion AI in Germany. While scikit learn, fasttext and keras are also major tools, spaCy and Prodigy have the considerable advantage of allowing end to end transparency in writing about the steps involved in developing and applying machine learning models. This chapter focuses on a basic machine learning workflow involving the following steps: Creating seed terms from word vectors to build a text classification model, training the model and testing it. Named Entity Recognition. Training a model to recognise entities of interest within the texts identified by the true or false model. Using a model to classify and identify named entities in a text 8.1 From Dictionaries to Word Vectors “You shall know a word by the company it keeps (Firth, J. R. 1957:11)” In the last chapter we explored approaches to text mining that do not involve the use of machine learning models. This involved text mining to identify terms for a dictionary that would allow for the identification of texts that contain one or more terms using ngrams. Dictionary based methods provide powerful tools for analysis. However, they suffer from two main issues. they will only ever return exactly the same terms that are in the dictionary. That is, they cannot identify nearby or closely related terms. dictionary based methods can be very memory intensive particularly if the dictionaries involved are very large. To overcome the first of these issues it is now common practice to combine dictionary approaches with a statistical model to more accurately classify texts at various levels of detail. That is to add a statistical learning component to improve classification. In the case of the second issue it is important to bear in mind that machine learning models can be more demanding on memory and computational resources than dictionary based methods. However, as we will discuss below, the availability of machine learning models allows for the development of strategies to minimise demands on memory and computational power such as initially training a model with a very large dictionary and then deploying a purely statistical based model without the dictionary. Much however will depend on the precise task at hand and the available memory and compute power available. In this chapter we assume that you will be using a laptop with a modest 16Gb of Ram. In the last chapter we used Vantage Point to create a co-occurrence matrix to build search terms and to refine analysis of a dataset. A co-occurrence matrix can be built in Vantage Point either as a count of the number of times that words or phrases in a dataset co-occur with each or using measures such as cosine similarity. One straightforward way of thinking about a word vector is as a co-occurrence matrix where words are transformed into numeric values and the vocabulary is cast into a multidimensional vector space. Within that space words with the same or similar meanings will be closer (in terms of scores or weights). More precisely, words that share similar contexts will have the same or similar meanings while words with dissimilar meanings will be further away. This observation is an important departure point for word vectors compared with a straightforward co-occurrence matrix that counts the number of times that words occur together in a given set. The reason for this is that the focus is on the context, or the company that a word is keeping. As we will see in a moment, word vectors are learned representations of the relationships between words based on minimization of the loss (error) of a predictive model.27 The seminal paper on word vectors by Mikolov et al 2013 neatly summarises the problem they seek to address as follows: “Many current NLP systems and techniques treat words as atomic units - there is no notion of similarity between words, as these are represented as indices in a vocabulary. (Mikolov et al. 2013) The problem that Mikolov and co-authors identify is that the development of approaches such as automatic speech recognition is constrained by dependency on high quality manual transcripts of speech containing only millions of words while machine translation models are constrained by the fact that “…corpora for many languages contain only a few billions of words or less” (Mikolov et al. 2013). Put another way, the constraint presented by approaches at the time was that the examples available for computational modelling could not accommodate the range of human uses of language or more precisely, the meanings conveyed. Mikolov et. al. successfully demonstrated that distributed representations of words using neural network based language models outperformed the existing Ngram (words and phrases) models on much larger datasets (using 1 million common tokens from the Google News corpus) (Mikolov et al. 2013). “We use recently proposed techniques for measuring the quality of the resulting vector representations, with the expectation that not only will similar words tend to be close to each other, but that words can have multiple degrees of similarity [20]. This has been observed earlier in the context of inflectional languages - for example, nouns can have multiple word endings, and if we search for similar words in a subspace of the original vector space, it is possible to find words that have similar endings [13, 14]. Somewhat surprisingly, it was found that similarity of word representations goes beyond simple syntactic regularities. Using a word offset technique where simple algebraic operations are performed on the word vectors, it was shown for example that vector(”King”) - vector(”Man”) + vector(”Woman”) results in a vector that is closest to the vector representation of the word Queen [20].” This observation has become one of the most famous in computational linguistics and is worth elaborating on. In a word vector it was found that. King - Man + Woman = Queen In a 2016 blog post on “The amazing power of word vectors” Adrian Coyler provides the following illustration of how this works. Note that the labels do not exist in the vectors and are added purely for explanation in this hypothetical example.28(https://blog.acolyer.org/2016/04/21/the-amazing-power-of-word-vectors/) (#fig:coyler_vectors)Word Vector Example (Coyler 2016) Let us imagine that each word in each individual vector has a distributed value across hundreds of dimensions across the corpus. Words like King, Queen, Princess have a high similarity in vector space with the word Royalty. In contrast, King has a strong similarity with Masculinity while Queen has a strong similarity with Femininity. Deducting Man from King and adding Woman can readily be seen to lead to Queen in the vector space. Other well known examples from the same paper lead to the calculation that “big-bigger” = “small:larger” etc. 8.2 Word Vectors with fastText To illustrate the use of word vectors we will use the fastText machine learning package developed by Facebook. FastText is a free text classification and representation package produced by Facebook that provides downloadable multi-language models for use in machine learning. At present vector models are available for 157 languages. FastText can be used from the command line or in Python (fasttext) or in R with the fastrtext package. FastText is a good way to get started with word vectors and machine learning because it is very easy to install and use. Fasttext is under active development with the latest updates posted on the fasttext website. FastText was developed by researchers including Tomas Mikolov as an alternative to the increasingly popular deep learning models for text classification at scale. It is a lightweight tool that emphasizes speed in classification (Bojanowski et al. 2016; Joulin, Grave, Bojanowski, and Mikolov 2016; Joulin, Grave, Bojanowski, Douze, et al. 2016) and arguably outperforms deep learning models. To get started follow the fastText instructions to install fasttext from the command line or in Python. We will demonstrate fasttext in Python but it is easy, if not easier, to run from the command line. git clone https://github.com/facebookresearch/fastText.git cd fastText sudo pip install . # or sudo python setup.py install Verify the installation python &gt;&gt;&gt; import fasttext &gt;&gt;&gt; If this has worked correctly you should not see anything after import fasttext. In the Terminal we now need some data to train. The fastText example uses Wikipedia pages in English that take up 15Gb. In the terminal download the smaller version as follows. $ mkdir data $ wget -c http://mattmahoney.net/dc/enwik9.zip -P data $ unzip data/enwik9.zip -d data As this is an XML file it needs to be parsed. The file for parsing is bundled with fastText as wikifil.pl and you will need to get the path right for your installation. If in doubt download fasttext from the command line, make and then cd into the directory for this step. Record the path to the file that you will need in the next step in Python. perl wikifil.pl data/enwik9 &gt; data/fil9 Check that the file has parsed on the command line. head -c 80 data/fil9 Train word vectors import fasttext model = fasttext.train_unsupervised(&#39;/Users/colinbarnes/fastText/data/fil9&#39;) 8.3 Training Word Vectors for Drones We will use a small set of patent texts on drones from the drones package for illustration. Ideally use the largest possible set. However, for better results use texts in a single language and regularise the texts so that everything is lower case. You may also improve results by removing all punctuation. If we wished to do that in R by way of example we access the title, abstract and claims table (tac) in the drones training package. We would then combine the the fields, convert to lowercase and then replace all the punctuation with a space. We might tidy up by removing any double spaces created by removing the punctuation library(tidyverse) library(drones) drones_texts_vec &lt;- drones::tac %&gt;% unite(text, c(&quot;title_english&quot;, &quot;abstract_english&quot;, &quot;first_claim&quot;), sep = &quot; &quot;) %&gt;% mutate(text = str_to_lower(text)) %&gt;% mutate(text = str_replace_all(text, &quot;[[:punct:]]&quot;, &quot; &quot;)) %&gt;% mutate(text = str_replace_all(text, &quot; &quot;, &quot; &quot;)) %&gt;% select(text) head(drones_texts_vec) This cleaned up anonymised text is available in the data folder of this handbook and in the drones package . Note that patent texts can be messy and you may want to engage in further processing. Once we have the data in a cleaned cleaned up format we can pass it to fast text in the terminal. There are two available models for word vectors in fast text. These are: skipgrams (identify a word from closely related words) cbow (predict a word from the context words around it) Next in the terminal we navigate to the fasttext folder and provide our csv or simple text file as an input and specify the output. $ ./fasttext skipgram -input /handbook/data/fasttext/drones_vector_texts.csv -output /handbook/data/fasttext/drones_vec There are total of 4.4 million words (tokens) in the vocabulary with xxx distinct words. It takes about 30 seconds for fasttext to process these words. These words boil down to 19,125 words in total. Read 4M words Number of words: 19125 Number of labels: 0 Progress: 100.0% words/sec/thread: 50624 lr: 0.000000 avg.loss: 1.790453 ETA: 0h 0m 0s The processing creates two files in our target directory. The first is drones_vec.bin containing the model and the second is drones_vec.vec. The second file is actually a simple text file that contains the weights for each individual terms in the vector. Here is a glimpse of that file for the word device. device 0.21325 0.11661 -0.060266 -0.17116 0.16712 -0.03204 -0.54853 -0.30647 0.023724 -0.047807 -0.068384 -0.22845 -0.08171 0.046688 0.26321 -0.51804 -0.02021 0.099132 -0.27856 0.33479 -0.027596 -0.27679 0.31599 -0.32319 0.048407 -0.067782 -0.086028 0.070966 -0.27628 -0.43886 -0.23275 0.15364 -0.037609 0.16732 -0.55758 0.24021 -0.21904 -0.00074375 -0.2962 0.41962 0.069979 0.039564 0.31745 -0.11433 0.15294 -0.4063 0.16489 -0.17881 -0.24346 -0.17451 0.19218 -0.13081 -0.052599 0.12156 -0.023431 -0.066951 0.19624 0.11179 0.17482 0.34394 0.17303 -0.32398 0.54666 -0.30731 -0.1117 -0.017867 0.081936 -0.068579 -0.15465 0.057545 0.026571 -0.3714 0.029978 0.081706 0.017101 0.036847 -0.13174 0.24701 -0.10006 -0.11838 -0.045929 -0.13226 0.20067 0.12056 0.43343 0.052317 -0.030258 0.066875 -0.1951 0.12343 0.031962 -0.52444 0.041814 -0.64228 0.13036 0.040553 0.30254 -0.15474 -0.57587 0.29205 Note two points here. The first is that the default vector space is 100 dimensions but popular choices go up to 300. Note also that there will be common stop words (and, the, etc.) in the model that we may want to remove. The second main point is that the file size of the .bin file is nearly 800Mb and may get much larger fast. From the terminal we can print the word vectors for specific words as follows: $ echo &quot;drone autonomous weapon&quot; | ./fasttext print-word-vectors /Users/colinbarnes/handbook/data/fasttext/drones_vec.bin drone 0.38326 0.4115 0.28873 -0.35648 -0.24769 -0.22507 0.18887 0.012016 0.51823... autonomous 0.41683 0.39242 0.16987 -0.028905 0.38609 -0.57572 -0.44157 -0.51236... weapon 0.20932 0.59608 0.21891 -0.42716 0.19016 -0.76555 0.23395 -0.63699 -0.12079... 8.4 Using Word Vectors One common use of word vectors is to build a thesaurus. We can also check how our vectors are performing, and adjust the parameters if we are not getting what we expect. We do this by calculating the nearest neighbours (nn) in the vector space and then entering a query term, The higher the score the closer the neighbour is. $ ./fasttext nn /Users/colinbarnes/handbook/data/fasttext/drones_vec.bin Query word? drone codrone 0.69161 drones 0.66626 dron 0.627708 microdrone 0.603919 quadrone 0.603164 stabilisation 0.594831 stabilised 0.579854 naval 0.572352 piloted 0.571288 stabilise 0.564452 Hmmm, OK but maybe we should try UAV Query word? uav uavgs 0.768899 aerial 0.742971 uavs 0.710861 unmanned 0.692975 uad 0.684599 ua 0.667772 copter 0.666946 usv 0.652238 uas 0.644046 flight 0.643298 This is printing some words that we would expect in both cases such as plurals (drones, uavs) along with types of drones but we need to investigate some high scoring terms, for example in set one we have the word codrone which is a specific make of drone. The word dron may be the word for drone in another language. In the second set we have uavgs which may stand for UAV Ground School along with terms such as uad which stands for unmanned aerial device. So, this reveals that we are obtaining some meaningful results on single terms that can guide our construction of a search query. We could also look at this another way by identifying terms that may be sources of noise. Here we will use the word bee. Query word? bee honey 0.861416 hive 0.830507 bees 0.826173 hives 0.81909 honeybee 0.81183 queen 0.806328 honeycombs 0.803329 honeybees 0.784235 honeycomb 0.783923 beehives 0.783663 This is yielding decent results. We could for example use this to build a term exclusion list and we might try something similar with the word music (to exclude words like musician, musical, melodic, melody) where it is clear they cannot be linked to drones. For example, there may be drones that play music… but the words musician, melodic and melody are unlikely to be associated with musical drones. 8.5 Exploring Analogies Word vectors are famous for being able to predict relationships of the type ”King” - ”Man” + ”Woman” = “Queen” We can experiment with this with the drones vector we created earlier using the analogies function in fasttext. In the terminal run: $ ./fasttext analogies /Users/colinbarnes/handbook/data/fasttext/drones_vec.bin Query triplet (A - B + C)? drone autonomous bee honey 0.687067 larvae 0.668081 larva 0.668004 brood 0.664371 honeycombs 0.655226 hive 0.653633 honeycomb 0.651272 bees 0.632742 comb 0.626189 colonies 0.61386 what this tells us is that drone - autonomous + bee = honey or larvae, or larva or brood. We can more or less reverse this calculation. Query triplet (A - B + C)? bee honey drone drones 0.651486 codrone 0.63545 stabilisation 0.592739 dron 0.582929 na 0.572516 # drop NA from the underlying set microdrone 0.571889 naval 0.541626 continuation 0.54127 proposition 0.540825 déstabilisation 0.536781 What this example illustrates is that, as we might expect, terms for bees and terms for drones as a technology occupy different parts of the vector space. It is fundamentally quite difficult to conceptualise a 100 or 300 dimension vector space. However, Google has developed a tensorflow projector and a video that discusses high dimensional space in an accessible way A.I. Experiments: Visualizing High-Dimensional Space. The Distill website offers good examples of the visualisation of a range of machine learning components. We can view a simplified visualisation of the term drones in 200 dimension vector space (on a much larger model than we have discussed above) in Figure 8.1. Figure 8.1: The term drone in 200 dimension vector space Here we have selected to display 400 terms linked to the source word drones across the representation of the vector space. As with network analysis we can see that clusters of association emerge. As we zoom in to the vector space representation we start to more clearly see nearest points in this case based on Principle Components Analysis (PCA) in Figure 8.2. Figure 8.2: Zooming in to the Drones cluster The representation of terms in vector space in these images is different to those we viewed above and more clearly favours bees and music, although closer inspection reveals words such as ‘winged’, ‘terrorized’. ‘missile’ and ‘predator’ that suggest the presence of news related terms when compared with the patent data used above. This visualisation highlights the power of the representation of words as vectors in vector space. It also highlights that the vector space is determined by the source data. For example, many vector models are built from downloads of the content of Wikipedia (available in a number of languages) or on a much larger scale from internet web pages through services such as Common Crawl. For patent analytics, this can present the problem that the language in vector models lacks the specificity in terms of the use of technical language found in patent documents. For that reason you may be better, as illustrated above, wherever possible it is better to use patent domain specific word embeddings. 8.6 Patent Specific Word Embeddings The increasing accessibility of patent data, with both the US and the EP full text collections now available free of charge, has witnessed growing efforts to develop word embedding approaches to patent classification and search. A very good example of this type of approach is provided by Julian Risch and Ralf Krestel at the Hasso Plattner Institute at the University of Potsdam with a focus on patent classification (Risch and Krestel 2019). Risch and Krestel test word embedding approaches using three different datasets: the WIPO-alpha dataset of 75,000 excerpts of English language PCT applications accompanied by subclass information (Automated Categorization in the International Patent Classification 2003); A dataset of 5.4 million patent documents from the USPTO between 1976-2016 called USPTO-5M containing the full text and bibliographic data.^[USPTO Bulk Products, now more readily available from PatentsView A public dataset of with 2 million JSON formatted USPTO patent documents called USPTO-2M created by Jason Hoou containing titles, abstracts and IPC subclasses. 29. [NOTE THIS IS ACCESS PROTECTED! EXCLUDE AS NOT ACTIVE] These datasets are primarily intended to assist with the automatic classification of patent documents. They used fastText on 5 million patent documents to train word embeddings with 100, 200 and 300 dimensions based on lower-case words occurring 10 or more times and with a context window of 5. This involved a total of 28 billion tokens and, as they rightly point out, this is larger than the English Wikipedia corpus (16 billion) but lower than the 600 billion tokens in the Common Crawl dataset (Risch and Krestel 2019). As part of an open access approach focusing on reproducibility the resulting datasets are made available free of charge30 The word embedding were then used to support the development of a deep neural network using gated recurrent units (GRU) with the aim of predicting the main subclass for the document using 300 words from the title and abstract of the documents. The same network architecture was used to test the WIPO set, the standard Wikipedia embeddings. the USPTO-2M set and the USPTO-5M (restricted to titles and abstracts). The specific details of the experiments performed with the word embeddings and GRU deep neural network are available in the article. The main finding of the research is to demonstrate that patent specific word embeddings outperform the Wikipedia based embeddings. This confirms the very crude intuition that we gained from the very small samples of data on the term drones compared with the exploration of Wikipedia based embeddings. One important challenge with the use of word vectors or embeddings is size. FastText has the considerable advantage that it is designed to run on CPU. That is, as we have seen it can be run on a laptop. However, the word embeddings provided by Risch and Krestel, notably the 300 dimension dataset, may present significant memory challenges to be used in practice. The word embeddings generated by the work of Risch and Krestel demonstrate some of these issues. Thus, the 100 dimension word embeddings vectors are 6 gigabytes in size, the 200 dimensions file is 12Gb and the 300 dimensions is 18Gb. In practice, these file sizes are not as initially intimidating as they appear. Thus, the 6Gb 100 dimension vectors easily run in fasttext on laptop with 16Gb of RAM. Nevertheless, you should expect to require increased storage space to accommodate the size of datasets associated with machine learning and it is well worth investing in additional RAM for running machine learning tasks on CPU. However, the recent rise of state of the art Transformer models has introduced new demands in terms of size and a transition to GPU rather than CPU based processing. We will return to this at the end of this discussion. Having introduced the basics of vector space models we now turn to a small practical example of text classification. We will then move on to Named Entity Recognition. 8.7 Machine learning in Classification The rise of machine learning has been accompanied by increasing attention to the possibility of automating the classification of patent documents. As patent filings come in to patent offices they need to be initially screened and classified in order to pass them to the relevant section of examiners. The ability to automate, or partly automate, this task could represent a significant cost saving for patent offices. At the same time, the determination of IPC or CPC classification codes that should be applied to describe the contents of a patent documents by examiner could be assisted by machine learning based approaches to produce predictions based on past experience on how similar documents were classified. There is a growing body of literature on this topic and in order to understand existing progress we recommend searching for recent articles that will provide an overview of the state of the art and exploring articles that are cited in the references and articles that cite recent reviews. To assist in that process an open access collection on patent classification has been created on the Lens at [https://www.lens.org/lens/search/scholar/list?collectionId=199722(https://www.lens.org/lens/search/scholar/list?collectionId=199722)]. In the next section we will focus on a step by step example of text classification tasks to introduce the approaches. 8.8 Machine Learning for Text Classification To illustrate text classification we will use the Prodigy annotation tool from explosion.ai. explosion.ai is the company behind the very popular spaCy open source Python library for Natural Language Processing. spaCy is free. However, the Prodigy annotation tool involves an annual subscription fee. We focus here on the use of Prodigy because it allows us to be very transparent about text classification, named entity recognition and image classification. We also use Prodigy because of the tight integration with spaCy and ability to rapidly move models into production. The ability to move models into production is an important consideration when deciding on which tools to use. Academic tools are often focused on research purposes and any code may be of variable quality in terms of robustness and maintenance. This is not an issue with tools such as Prodigy and spaCy that focus on rapidly moving models into production. In thinking about classification and other machine learning tasks it is helpful to think in terms of a set of steps. In the case of the drones data that we explored above we can identify three steps that we might want to perform. Step 1: Is this text about drone technology (yes/no) Step 1 is a filtering classification. We build a classification model that we can use to filter out anything in our raw data that does not involve drone technology. Step 2: Identify the main classes of drone technology (multilabel) This is a multilabel classification step. That is, a technology may fall into one or more areas of drone technology. We want to train a model to recognise the types that we are interested in. Step 3: Name Entity Recognition This is not a classification step as such but seeks to identify specific terms of interest in the texts that are selected by the classification models created in Step 1 and Step 2. 8.8.1 Step 1: Binary Text Classification We will start with a binary classification task where we will classify text from the new dronesr package that updates the data in the older drones package used above. Prodigy contains a text classification function called textcat.teach that teaches a model what texts to accept and reject. In this case we create a dataset called drones, use a blank English model and import a small set of texts containing valid drone technology documents and a set that are noise. The texts will be labelled as accept/reject for the term DRONE. To assist the model with learning about the texts we have added a set of terms such as “autonomous vehicle”, “musical” and “bee” in a patterns file for the model to spot. prodigy textcat.teach drones blank:en ./textcat_texts.csv --label DRONE --patterns textcat_patterns.jsonl The tool will then show each record highlighting the seed terms where relevant as we see in Figure 8.3. For a binary task the choices are simply accept (green) or reject (red). Figure 8.3: Binary Classification in Prodigy The process of classification can lead to insights about possible multilabel classification labels, such as navigation in Figure 8.4. Figure 8.4: Binary Classification in Prodigy Our terms also pick up one of the false positives on drones as we see in in Figure 8.5. Figure 8.5: Binary Classification in Prodigy The outcome of the classification process is a set of annotations that take an accept/reject format. An example of an accept and reject text is provided below. The format is new line Json (jsonl) that is increasingly widely user and in this truncated version we can see that the start and end position of the match terms are recorded along with the label and the answer. Other machine learning software commonly adopts a similar format. {&quot;text&quot;:&quot;Newness and distinctiveness is claimed in the features of shape and configuration of an unmanned aerial vehicle as shown in the representations.&quot;,&quot;_input_hash&quot;:411114389,&quot;_task_hash&quot;:887216913,&quot;spans&quot;:[{&quot;text&quot;:&quot;unmanned aerial vehicle&quot;,&quot;start&quot;:88,&quot;end&quot;:111,&quot;pattern&quot;:-1050519944}],&quot;label&quot;:&quot;DRONE&quot;,&quot;meta&quot;:{&quot;pattern&quot;:&quot;7&quot;},&quot;_view_id&quot;:&quot;classification&quot;,&quot;answer&quot;:&quot;accept&quot;,&quot;_timestamp&quot;:1647448549} {&quot;text&quot;:&quot;The utility model provides a pair of be used for honeybee to breed device is equipped with honeybee passageway and worker bee special channel on the interior box...&quot;,&quot;_input_hash&quot;:1202848216,&quot;_task_hash&quot;:-1506636545,&quot;spans&quot;:[{&quot;text&quot;:&quot;honeybee&quot;,&quot;start&quot;:49,&quot;end&quot;:57,&quot;pattern&quot;:-1721291956},{&quot;text&quot;:&quot;honeybee&quot;,&quot;start&quot;:91,&quot;end&quot;:99,&quot;pattern&quot;:-1721291956},{&quot;text&quot;:&quot;bee&quot;,&quot;start&quot;:122,&quot;end&quot;:125,&quot;pattern&quot;:899165370}],&quot;label&quot;:&quot;DRONE&quot;,&quot;meta&quot;:{&quot;pattern&quot;:&quot;15, 15, 13, 15, 15, 15, 15, 15, 13, 0, 13, 0, 15, 0, 0, 15&quot;},&quot;_view_id&quot;:&quot;classification&quot;,&quot;answer&quot;:&quot;reject&quot;,&quot;_timestamp&quot;:1647452612} Once a number of annotations have been created it is possible to build a model to test out the approach and make adjustments. Model building is often conducted on the command line and involves dividing a set of texts into examples that are used to train the model and a set of examples that are used to test or evaluate the model. The second set is critically important because it is a set of examples with known answers that the model has never seen before. This exposes a fundamental point about the form of supervised machine learning that we are using. It is extremely important to have data that is already labelled to use in training and evaluation. The creation of annotations is the hidden workload of machine learning. With the development of transfer learning it is possible to use fewer annotations than before. However, you should still expect to think in terms of hundreds, thousands and possibly many more depending on the task. An important strength of tools like prodigy is that it is possible to rapidly generate annotations and experiment. 8.8.2 Step 2: Multilabel Text Classification When the number of records has been narrowed down to remove noise we can start to engage in multi-label classification. Important issue to consider here are the purposes of classification (what is the objective) and following definition of the objective is the number of labels to use. For our toy example with a handful of texts on drone patent documents a number of labels suggest themselves. controls (user and in vehicle) communication (wifi, bluetooth) navigation (GPS, collision avoidance, this is distinct from drones tracking objects) power (batteries, charging pads) sensing (scanning, imaging etc) take off and landing tracking That is quite a number of labels and this list is by no means conclusive. Here we would probably want to check the IPC codes for relevant categories. However, one challenge with the IPC categories is that they may be too broad or too specific. However, a review of IPC codes could assist with establishing categories in the areas above. This would have the advantage that you already have labelled texts in some of these categories and could save valuable time in labelling. That is, if you already have labelled data avoid relabelling unless you really have to. Let’s take a look at the top IPC descriptions for the drones data to see if any of them match up with our rough list from reviewing the documents in prodigy. We can see the top 10 IPC subclasses by the count for the new dronesr dataset in Table 8.1 below. Table 8.1: Top 10 IPC descriptions for the drones data subclass description n H04W WIRELESS COMMUNICATION NETWORKS 23938 B64C AEROPLANES 13108 G06F ELECTRIC DIGITAL DATA PROCESSING 10797 H04L TRANSMISSION OF DIGITAL INFORMATION 10614 H04N PICTORIAL COMMUNICATION 9076 G01S RADIO DIRECTION-FINDING 8042 G05D CONTROLLING NON-ELECTRIC VARIABLES 6470 G06T IMAGE DATA GENERATION 6448 B64D AIRCRAFT EQUIPMENT 6372 G06Q DATA PROCESSING SYSTEMS/METHODS 5942 We can immediately see that while the terms do not exactly match out existing list we can see communications (wireless communication, transmission of digital information), navigation (radio direction finding), sensing or imaging (pictorial communication &amp; image data generation) can be detected in the list while control systems for drones are addressed by controlling non-electric variables and computing by electric digital data processing and data processing system and methods. With the exception of aeroplanes these are not very suitable as labels and would ideally be shortened or codes (which are hard to recall would be used). What this brief discussion makes clear is that the drones data is already labelled by a classification scheme. The IPC is also a multilabel classification scheme because patent documents commonly receive multiple classification codes in order to more accurately describe the content of a document. This gives us two choices: If the existing classification scheme is suitable for our purpose then we can use the existing classification (the IPC) to create a classification model. If the classification scheme doesn’t work for our purposes (it is either too broad or too specific) then we should focus on either adding specific labels to more finely describe groups of documents or, if the labels are too specific, we should concentrate on grouping. In preparing to create a classification model it is very important to have one or more test runs to assess whether your approach is likely to work. That is, approach the task as an iterative and experimental process. One reason to use a paid tool such as Prodigy is that it is designed to let you experiment. It is normally a mistake to imagine that you will accurately classify hundreds or thousands of documents in one go. It is better to start with experimental sessions and then classify in batches, We will illustrate this process with our drones dataset by starting out with an initial set of labels and assessing how useful they are. Figure 8.6 shows the classification of a patent abstract from the drones set. Figure 8.6: Multi Label Classification We have chosen a small set of 5 labels to classify the texts. We could have used more labels, however in reality the larger the label set is the more difficult it becomes for a human annotator to manage them. Indeed, the authors of Prodigy suggest that it can often be more efficient to use one label at a time and go through multiple rounds of iteration on each text for the same label. We have tested this approach and it works because it makes the decision-making process easier by reducing the choice to yes/no for a specific label. It is normal to run into cases that can challenge your classification scheme quite quickly. Figure 8.7. Figure 8.7 is for an drone that will land and deliver a liquid to a plant. Figure 8.7: Binary Classification in Prodigy In our existing scheme the label that is appropriate here is the single label Imaging. However, this highlights a need to focus on the purpose of the classification and establishing a clear definition of that purpose prior to annotating. That is, what is the objective of classification? It would be easy for example when viewing this type of example to add a label such as Agriculture. In practice, such as label would refer to the proposed USE of a claimed invention. Our classification scheme is directed to identifying the key technologies involved in the document rather than use. As this brief discussion makes clear, establishing clarity on the purpose of multi-label classification is central to avoiding distractions and successful completion of the task. However, clear definition of the task will often only appear after a period of initial experimentation. For that reason, rather than jumping in with classification it is good practice to define a period of experimentation. That period may involve multiple sessions and experiments. This process will typically involve reconsideration of the labels and a tighter definition of the label set. For example, what is the work that the Processing label is doing in the set above? Could the label be more tightly defined or should another label be used. For example, by processing we could be referring to machine learning (IPC G06N20/00 under computing). If our specific interest is in machine learning rather than processing in general we should change the label. At the end of the classification process we will have a set of texts that contain multiple labels. In the case of prodigy and spaCy these will generally be exported as new line json (jsonl) where each document is contained in a single line. Other machine learning tools may use similar or simpler formats. We show two examples below with the long abstract texts abbreviated. {“text”:“The invention discloses a driving safety radar for a vehicle, and relates to the technical field drones…”options”:[{“id”:“CONTROLS”,“text”:“CONTROLS”},{“id”:“COMMUNICATION”,“text”:“COMMUNICATION”},{“id”:“IMAGING”,“text”:“IMAGING”},{“id”:“NAVIGATION”,“text”:“NAVIGATION”},{“id”:“PROCESSING”,“text”:“PROCESSING”}],“_view_id”:“choice”,“accept”:[],“config”:{“choice_style”:“multiple”},“answer”:“ignore”,“_timestamp”:1647597250} {“text”:“The aircraft (FM) according to the invention comprises a helicopter drone (HD) on which a 3D scanner (SC) is mounted via an actively rotatable joint (G). The 3D scanner (SC) has at least one high-resolution camera (C)…”,“options”:[{“id”:“CONTROLS”,“text”:“CONTROLS”},{“id”:“COMMUNICATION”,“text”:“COMMUNICATION”},{“id”:“IMAGING”,“text”:“IMAGING”},{“id”:“NAVIGATION”,“text”:“NAVIGATION”},{“id”:“PROCESSING”,“text”:“PROCESSING”}],“_view_id”:“choice”,“accept”:[“IMAGING”,“PROCESSING”],“config”:{“choice_style”:“multiple”},“answer”:“accept”,“_timestamp”:1647597274} At the end of the multi-label classification task we will have set of texts that can be used to train a model. In prodigy and spaCy a few hundred examples will typically be enough to start experiments. As the number of annotations increases so should the accuracy of the model that is trained. At that point the annotator should switch focus to reviewing and correcting the annotations generated by a model rather than annotating from scratch. Annotating texts is the most labour intensive aspect of machine learning and in some cases is outsourced to specialist companies. Machine learning tools such as Prodigy can take advantage of advances such as transfer-learning which means that it is possible to make fewer annotations to arrive at useful results. 8.8.3 Step 3: Named Entity Recognition Named entity recognition is the task of training a model to recognise entities such as: dates (DATE) persons (PERSON) organisations (ORG) geographical Political Entities (GPE) locations (mountains, rivers) (LOC) quantities (QUANTITY) A considerable amount of time and investment has gone into training what can be called ‘base’ models in multiple languages that will do a pretty good job of identifying these entities. There is therefore no need to start from scratch and most base models are trained on millions of texts (such as Wikipedia or news texts). Rather than seeking to build your own model from scratch the best course of action is to either: focus on improving existing labels if they meet your needs add new labels for entities using a dictionary of terms as support. We will focus on the second approach. One challenge in Natural Language Processing is that it is commonly difficult for a model to capture all of the different entities that we may be interested in. This is particularly true for entities that involve multiple words. Most machine learning models are token based. That is, they focus on individual words (tokens) and entities that span multiple tokens (e.g. 3 or more words) can be difficult for models to accurately and consistently capture. For that reason, it is often desirable for the user to add a dictionary (thesaurus) of terms that they are interested in with a label. In other words, the machine learning approach is combined with a dictionary approach. The use of dictionaries or thesauri with a model is a very powerful way of adding new entities and creating annotations that a model can learn from to automate the addition of a new entity type. In Prodigy (an annotation tool) and spaCy (the actual models) it is very easy to add match patterns or an entity ruler. The reason that it is often desirable to combine machine learning models with dictionary based approach can be easily seen in Figure 8.7. Figure 8.8: Entity Recognition with the Medium Sized spaCy English Model In this case the model has only recognised the date elements of the texts, rather than entities we might be interested in such as the word drone. The reason for this is that the model has not been exposed to the the types of entities that we are interested in. To improve the recognition we will add a dictionary (in spaCy this is an entity ruler and written in jsonl). For the purpose of illustration we will use similar labels to our classification experiment above. Table 8.2 shows the outcome of turning a column of words and phrases with labels into jsonl that can be used as an entity ruler attached to a spaCy model. Note that we specify that the terms will match on lower and upper case versions. We also include plural versions of the same terms (this can also be addressed using lemmatization rules). If seeking to get started with spaCy we recommend using the Phrase Matcher first and the terms.to-patterns recipe to create patterns files to use with annotations. If that does not meet your needs then you can then move on to creating your own code to write more advanced entity_ruler patterns. Explosion.ai have created a Rule-based Matcher Explorer that you can use to experiment with different types of patterns. Table 8.2: Terms converted to jsonl for an entity ruler in spaCy text label jsonl ﻿Drone TYPE {\"label\":\"TYPE\",\"pattern\":[{\"LOWER\":\"drone\"}],\"id\":\"text\"} Unmanned aerial vehicle TYPE {\"label\":\"TYPE\",\"pattern\":[{\"LOWER\":\"unmanned\"},{\"LOWER\":\"aerial\"},{\"LOWER\":\"vehicle\"}],\"id\":\"text\"} UNMANNED AERIAL VEHICLES TYPE {\"label\":\"TYPE\",\"pattern\":[{\"LOWER\":\"unmanned\"},{\"LOWER\":\"aerial\"},{\"LOWER\":\"vehicles\"}],\"id\":\"text\"} unmanned vehicle TYPE {\"label\":\"TYPE\",\"pattern\":[{\"LOWER\":\"unmanned\"},{\"LOWER\":\"vehicle\"}],\"id\":\"text\"} autonomous vehicles TYPE {\"label\":\"TYPE\",\"pattern\":[{\"LOWER\":\"autonomous\"},{\"LOWER\":\"vehicles\"}],\"id\":\"text\"} AIRCRAFT TYPE {\"label\":\"TYPE\",\"pattern\":[{\"LOWER\":\"aircraft\"}],\"id\":\"text\"} We can attach the entity ruler directly to a model or, in this case, when using Prodigy we can add this as a set of patterns to use when matching (as we did above). If you are using prodigy you could use the following in the terminal with either the ner.manual or the the ner.teach recipe to create annotations with new entity types that a model can learn from. Recipes will use the patterns file to bring forward texts containing those patterns so they can be annotated. In the code below we have added an empty label “USE” that we will use to start generating an entity recogniser for the types of use that are proposed in drone related patents. Note here that we could adopt a strategy of identifying uses with IPC codes (such as A01H for agriculture) and annotate the texts that fall into a specific area using the IPC as a guide. prodigy ner.manual drones_entities en_core_web_md ./pat_abstract.csv --label ORG,TYPE,COMM,POWER,IMAGING,USE --patterns ./drones_entity_ruler.jsonl In Figure 8.9 below the model has picked up on the term drone from the match patterns as the type throughout the text. We have then expanded the type to variable geometry drone and captured the uses set out in the text. Figure 8.9: Manual Labelling of Entities with Prodigy A second example in Figure 8.10 reveals how we can expand the range of use types as we proceed through the texts. Figure 8.10: Manual Labelling of Entities with Prodigy Through iterative sessions we will be able to generate a range of labels for the data that can then be used to train a named entity recognition model. As we have mentioned above, it will almost always be better to update an existing model because existing models will have been trained on very large volumes of data. However, where the nature of the texts is radically different another strategy is to create your own vectors (using fasttext or gensim) that become the basis for entity recognition. As we have seen above, it is easy to create vectors with tools such as fasttext and NLP libraries such as spaCy make it very easy to add your own vectors to create new models for specialist purposes. Specialist vectors can also be converted for use with Transformer models as the current state of the art in Natural Language Processing. 8.9 From Vector Based Models to Transformers We have focused on vector based models in this chapter because they will be the models you will most commonly encounter and they will run quite happily on a laptop with a limited amount of RAM. In contrast current state of the art transformer models run very slowly on CPU and are designed to be run on GPU which you may not have ready access to. Transformer models emerged from work at Google in 2017 and have rapidly become the cutting edge of NLP (Vaswani et al. 2017). In their 2017 article revealing Transformers the team led by Ashish Vaswani explained that: “The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration…. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.” (Vaswani et al. 2017) Over the last few years Transformers have become the go to models for Natural Language Processing at scale because of the improvements in accuracy they produce compared with recurrent (RNN) or convolutional (CNN) neural network based models. A range of transformer models have emerged such as BERT, XLNet and GPT-2 as well as a whole range of variants. A great deal of work has gone in to making these models available for use by ordinary users (rather than the large technology companies that typically produce them). The AI community site operated by Hugging Face has been particularly important in providing a platform through which transformer models can be integrated into production level NLP tools such as spaCy. The use of Transformer models such BERT with tools such as spaCy will require some setup. However, in the case of spaCy this is very well documented and easy to follow. If you would like to try out the transformer models in spaCy on regular CPU then this is also easy by installing the .trf (for transformers) at the time of installation. The models will run but will be very slow. In addition, Google Colab makes it possible to use GPU to run a spaCy transformer model for free to test it out. For larger scale tests or production level use, cloud service providers such as Amazon Web Services, Google Cloud Platform or Microsoft Azure offer GPU enabled ML pipelines as do services on these platforms such as Databricks Apache Spark (the author uses Databricks on Azure). We have focused on the Prodigy annotation tool and spaCy in this chapter because they are transparent, well documented an easy to explain. However, there are other libraries for NLP at scale including SparkNLP from John Snow Labs that performs natural language natively in spark (rather than through Python as is the case for spaCy). As such, there are an range of choices beyond those highlighted in this chapter. The Hugging Face AI community site has become the main site for the sharing of a wide range of machine learning models involving natural language processing. These models include models for text generation, sentiment analysis, classification and named entity recognition and the ability to live test the models to see if they are a fit for your task as we see in Figure 8.11. Figure 8.11: The Hugging Face AI Community Developments at Hugging Face reveal that an important transition is taking place between the users and developers of NLP machine learning software being the same people to a growing user community who want to try out different models to meet their needs and workflows. 8.10 Conclusion This chapter has addressed developments in machine learning for natural language processing and its implications for patent analytics. We have focused on the emergence of vector space based models and their uses for text classification and name entity recognition tasks using the Prodigy annotation tool and spaCy NLP library models from explosion.ai. In the process we have sought to emphasise some of the main insights from our own work with these and similar tools for patent analysts interested in benefiting from machine learning based approaches. In closing this chapter the following points are relevant to decision-making on machine learning and patent analytics. Is machine learning based NLP really what I need? Machine learning libraries will typically make a big difference in patent analytics if you want to automate entity identification in your workflow. For small to medium scale projects it make sense as you will arrive at faster outcomes with standard NLP approaches and using VantagePoint. Alternatively, some of the plug and play offerings from cloud services that offer text classification or named entity recognition might be the right fit for you. If the answer to the first question is yes, the next question is what tool to use for the particular tasks that you have in mind. The author’s work mainly focuses on the extraction of named entities from text that are relatively straightforward for NER models to manage, such as species, common, country and place names. More specialist tasks, such as the recognition of chemical, genetic or other types of entities would merit investigating libraries that have been designed or adapted for these types of entities. For example Allen AI has created the SciSpacy set of models for working with biomedical, clinical or scientific texts. The Hugging Face community site may also be a good option for identifying models trained on particular tasks. In deciding on a machine learning model or set of models focus on the documentation and the size of the user community. Machine learning can be deeply confusing. Identifying packages and models that are well documented and have active communities will allow you to get things done rather than navigating complex layers of documentation or puzzling over what the statistics from a model run mean. The author chose to use spaCy because it is well documented and supported and included a free course and model project templates for different use cases. What does success look like? When engaging with machine learning it is easy to start pursuing perfection when the objective should be to pursue a model that does a good general job for the task at hand and saves on time and effort that a human would otherwise have to dedicate to the task. Define how to evaluate the outcomes. It is extremely easy to create a model that will perform almost perfectly on 100 records using a 80% (training) to 20% (evaluation) split. That model will perform terribly on real world data even if the metrics produced by the model look great. There is a need to introduce measures that allow you to evaluate the real world utility of a model. For this tools such as prodigy are valuable for assessing (and correcting) a model in practice while VantagePoint from Search Technology Inc is very valuable for large scale assessment of model performance (e.g. on hundreds of thousands of tests). Do not try to be on the cutting edge of machine learning for NLP. In patent analytics, unless you are an academic researcher developing methods, the place to be is generally a year or two behind the cutting edge. In a fast developing field it can take a while to understand what the successful new approaches really are and how you can use them in your work. For example, the present state of the art is represented by transformers but vector based models are much more practical for everyday use on a laptop and can produce excellent results. When investing in NLP libraries it can make very good sense to plan the transition from using one set of models to another. For example, the transition from version 2 to version 3 of spaCy involved some quite radical changes that required planning. It makes good sense to be some distance behind the curve and to plan transitions to newer approaches as they prove their worth, or, in the case of transformers, as they become more accessible to regular users. References "],["conclusion.html", "Chapter 9 Conclusion", " Chapter 9 Conclusion This Handbook has aimed to provide an accessible and practical guide to intermediate and advanced methods and tools for patent analytics. The Handbook is a complement to the WIPO Manual on Open Source Patent Analytics that has been widely used in introductory training in patent analytics. In conclusion it is important to highlight some of the key take home messages that emerge from this wide ranging exploration of methods in patent analytics. The first of these is that patent activity is an outcome of underlying investments in Research and Development. When initiating research for a patent analytics project it makes very good sense to start by investigating the scientific literature. Analysis of the scientific literature not only contributes to the process of identifying search strategies for an area of technology but allows you to become familiar with the main trends and actors involved in a technology area. The growing accessibility of tools for geospatial mapping, such as APIs or the new Research Organization Registry (ROR) and its incorporation into datasets such as OpenAlex, means that it is now possible to rapidly generate useful maps of global research activity. Mapping of this type not only assists with refining the focus of an analytics project but can also assist with identifying potential partners in different countries around the world and competing or emerging approaches. Finally, research organisations are also increasingly important players in the global landscape of patent activity. For these reasons research methods involving the scientific literature are so prominent in this Handbook. Our second main insight is that it is very important to become familiar with different methods for counting patent data and the purposes to which different types of count can be used. This also requires careful attention to terminology in order to avoid misleading an audience. For example, as a matter of good practice the use of the word patent documents to describe patent data rather than ‘patents’ avoids giving an audience the impression that the analysis refers to patent grants rather than applications or a mix of applications and grants. It is important to be clear with audiences about what is being described. For example, where the focus is on identifying trends in research and development the use of priority counts is appropriate and can be readily explained as first filings. An audience will then generally want to see trends in applications and grants which involves engagement with patent families and careful attention to kind codes and effective description. Because different data providers use different and sometimes opaque definitions of patent families it is logical to use the DOCDB or INPADOC families wherever possible. However, in all cases analysis of activity using patent family data should be clearly explained to the reader with added notes on issues around the interpretation of kind codes particularly where analysis is extended to multiple countries. Methodological transparency is critically important where analysis aims to inform commercial or policy decision making. A detailed understanding of the types and issues involved in patent counts is also important in preparation for modelling patent data. Chapter 4 introduced linear regression using widely used models for elucidating trends in existing data. This in turn provided a basis for the exploration of common approaches to forecasting using data on PCT applications at WIPO. As our example of a fictional crisis at WIPO made clear, it is possible to forecast patent activity such as PCT filings at WIPO. However, this requires an understanding both of the data itself and approaches to forecasting and their strengths and weaknesses. For example, we might reasonably feel confident in approaching forecasting of first filings in individual countries or areas of technology but forecasting patent family activity globally would be an entirely different matter. In making the transition from simple counts to modelling patent activity we can readily use common smoothing models but forecasting requires considerable care in selecting the data to be forecast. The global patent system can be likened to the fictional Hogwarts library in that the global patent library houses information that is entirely innocuous and information that could be dangerous in the wrong hands. Knowledge of patent classification systems is central to the ability of an analyst to successfully navigate this system. It is therefore important to recognise the strength of patent classification systems in helping to focus patent analysis by selecting relevant areas of the system. However, as we saw in the case study in Chapter 5 it is also important to recognise the weaknesses of the patent classification for specific projects. Thus, while useful, the classification will commonly be either too broad or too specific for a specific analytics project. In other cases, such as emerging technologies, the classification may not yet have caught up with the latest developments (e.g. in the historic case nanotechnology). These limitations will generally require patent analysts to develop their own groupings. Chapter 5 provided an example of one approach to grouping using network analysis and community detection that became the basis for organising a patent landscape analysis. Finally, it is important to remember that the use of classification in patent analysis forms part of an exercise in communication with an audience who are unlikely to have much time, or be interested in classification symbols or long reams of formulaic text. As such, finding ways to communicate data on technology areas in a way that is understandable, as in the case of the short IPC, is an important element in successful patent analytics. Citation analysis has been a major if not foundational focus of attention in scientometrics and patent analytics. Chapter 6 provided an in depth exploration of patent citations using the example of Nobel prize winning gene editing (CRISPR) technology and the contest between Berkeley and the Broad Institute in this field. Analysis of this worked example provided a basis for the exploration of main path analysis that combines citation analysis with the use of patent classification systems in order to reveal the development of technologies and point towards the forecasting and detection of technological trajectories. Patent citation data is increasingly available at scale through the OECD, the US PatentsView and web services such as the Lens patent API. In addition, the citation connections between the scientific and patent literature are also increasingly openly accessible at a range of different scales to inform patent analytics. Citation analysis is a critically important part of patent analysis because it allows us to identify the contours of technology landscapes using the framework of patent classification systems and increasingly to identify the trajectories of technologies within those landscapes. In future years, the ability to freely access citation data at scale will in the author’s view provide a platform for transformations in the scale and accuracy of patent analytics. Text mining is a key component of patent analytics and is being transformed by growing access to full text patent data thanks to the work of the USPTO PatentsView service and the EPO. Growing access to patent texts at scale is also accompanied by the increasing accessibility of machine learning based approaches to Natural Language Processing (NLP). Chapter 7 examined standard widely used approaches to text mining and demonstrated how text mining could be combined with knowledge of the patent classification to more accurately target texts. Using the worked example of biodiversity patent activity this provided a basis for the analysis of words and phrases (ngrams) in text mining and widely used technique such as Term Frequency Inverse Document Frequency (TFIDF) to identify the distinctive features of texts in areas of the classification and to focus the analysis on areas such as gene editing. The Chapter also demonstrated the analysis of terms over time and the role of the visualisation of networks of terms in patent analytics. These common techniques can be applied either programmatically (e.g. using R or Python) or using specialised analytics software such as VantagePoint for fine grained control. Standard approaches to text mining are extremely effective for many patent analytics tasks either using R, Python or VantagePoint. However, these approaches are increasingly being complemented and for some replaced by machine learning based approaches to Natural Language Processing. Chapter 8 provided an in depth introduction to machine learning as a field that encompasses Natural Language Processing, text classification, named entity recognition and image classification. Machine learning based approaches to text or image analysis are ultimately based on the use of algorithms to recognise patterns. Chapter 8 provided an in depth exploration of machine learning in Natural Language Processing using the fasttext, spaCy and Prodigy libraries. The Chapter concluded by pointing to the increasing accessibility of more accurate transformer models and off the shelf plug and play services provided through companies such as HuggingFace and the major cloud service providers. For patent analytics the promise of machine learning based approaches is that it will be possible to automate tasks such as classification and entity recognition at scale and incorporate models into processing pipelines. The growing accessibility of pretrained models and affordable infrastructure means that these approaches will become increasingly accessible for analysts regardless of their budgets. Above all, the promise of machine learning approaches for text analysis tasks is that it will relieve some of the burden of hard manual processing from the analyst through automation. However, as discussed in Chapter 8 against this we must not underestimate the challenge of training models to perform accurately on the specialised language of patent texts or the images that accompany patents. In machine learning, the quality of training data is king. A great deal of hidden time and labour is required to generate training data that is appropriate for patent analytics. We may hope, as some initiatives already suggest, that with time specialised models will be created to assist with the classification and extraction of entities from patent documents. However, in the meantime patent analysts will be wise to rely on existing easy to use techniques and to progressively experiment with incorporating accessible machine learning models into their workflows. In this way patent analysts can benefit from the undoubted strengths of machine learning approaches while avoiding some of its pitfalls. In closing the Handbook it is appropriate to briefly speculate about what the future of patent analytics might look like. For the author of this Handbook it would be highly desirable to see the increasing availability of patent data in forms that are amenable for patent analytics. The USPTO PatentsView service is the model in this regard followed by the EPO. It is to be hoped that in future the WIPO PCT collection might also be made available. As we have seen in the Handbook considerable manual or computational processing is required on the part of analysts, notably with data cleaning and text processing. Some of this work could be done in advance such as the harmonisation of applicant and inventor names. The OECD has done pioneering work in this area that is linked to work the EPO World Patent Statistical Database (PATSTAT). More recently the USPTO PatentsView service has done admirable work in disambiguating applicant and inventor names and geocoding patent data. It is highly desirable to support and encourage these types of initiatives for the benefit of the wider users of patent data. At the same time we could imagine that a great deal of the hard labour in text based patent analytics could be removed by following the example of the General Index which provides open access to the ngrams of over 57 million scientific articles. A similar initiative by patent offices with their texts could greatly improve the access of patent texts for analytics purposes by wider user communities. In a similar vein, as we saw in the example of fasttext, supporting and perhaps maintaining vector space models could greatly reduce duplication of effort by creating a common baseline. This would allow researchers and commercial providers to focus on the development of more specialist tasks. Finally, as the first Handbook of its type the present work forms part of a wider effort to promote open patent analytics for the benefit of the wider community. I hope that the Handbook has proved useful and that you will contribute through your own work to the promotion of open patent analytics for the benefit of the wider community. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
