# Social Media and Patent Analytics {#social}

\index{social}

### Introduction

This chapter examines the options for using data from social media as part of patent analytics using COVID 19 as an example. 

Over the last decade there has been an explosion in the use of and types of social media. Among the best known of these services are Facebook, Twitter, Instagram and Snapchat along with other services targeting particular types of users such as LinkedIn or ResearchGate.

When approaching social network services it is important to emphasise three points. First, access to data from these services will often vary widely in the whether they make data available free of charge through API services and the amount and type of data that can be retrieved or publicly shared. It is particularly important to check the terms of service and to operate within those terms. Second, the use of social media data raises issues around the privacy of individuals. Recent scandals around the harvesting of personal data for political advertising purposes and the adoption of the [GDPR in the European Union](https://ec.europa.eu/info/law/law-topic/data-protection_en) indicate that the patent analysts should adopt a cautious and considered approach to social media data.

Social media data has primarily been of research interest in two main areas:

-   understanding public attitudes and influences;
-   understanding consumer views and trends with respect to products and technologies.

In this chapter we will focus on the use of data from Twitter. As many readers will already be well aware, Twitter allows users to send short messages of up to 280 characters that may contain text, images, video, links and subject hashtags such as \#patents or user addresses such as @WIPO.

Twitter data has become an important focus of attention for research on public opinions and topics such as sentiment analysis (positive or negative views) in connection with products or other topics.

Twitter data can be accessed in a number of ways:

a)  Using software such as Tweetdeck to track multiple topics (visual only);
b)  Through the Twitter Application Programming Interface (API) using popular packages such as `rtweet` in R or `tweepy` in Python;
c)  Through commercial data providers, notably for data at scale
d)  Through public datasets such as [TweetSets](https://tweetsets.library.gwu.edu/) from George Washington University that provide access to 'dry' Tweet ids that need to be 'hydrated' to obtain access to the content. Public datasets are commonly accessible on the condition that they are for non-commercial research.

It is important to emphasise that Twitter's Terms of Service *do not allow* the public sharing of the full content of Tweets. Rather, public datasets are restricted to tweet identifiers (tweet ids) and the actual data must be retrieved from Twitter through a process known as 'hydration'. What this also means is that if you encounter a dataset of Tweets that includes anything other than Tweet Ids it will normally be violating the terms of service.[^social-1]

[^social-1]: At the time of writing the [Terms of Service](https://developer.twitter.com/en/developer-terms/agreement-and-policy) under Content Redistribution state that "If you provide Twitter Content to third parties, including downloadable datasets or via an API, you may only distribute Tweet IDs, Direct Message IDs, and/or User IDs (except as described below). We also grant special permissions to academic researchers sharing Tweet IDs and User IDs for non-commercial research purposes."

In this chapter we will illustrate the process of obtaining Twitter data through the API as this will be the main way patent analysts will access Twitter data. We will also provide a brief guide to hydrating Twitter IDs for larger scale projects. Our main focus will be exploring the analytics that we can perform with Twitter data including text mining and geographic mapping.

### Resources

The most important background resource for social media analysis at the time of writing is the book `Mining the Social Web: Data Mining Facebook, Twitter, LinkedIn, Github, Instagram and More` [@lens.org/149-229-036-878-405] by Matthew A Russell and Mikhail Klassen. This book was originally published in 2011 and with a third edition published in 2019. Because web services and terms and conditions may change in various ways over the years it is important to use the latest edition. For example, in the wake of the Cambridge Analytica and related scandals Facebook made changes to its API with some uses requiring review and approval.

`Mining the Social Web` is accompanied by a set of Github Repository <https://github.com/mikhailklassen/Mining-the-Social-Web-3rd-Edition> providing free access to a set of Python Jupyter Notebooks for each chapter. These run in a self standing browser based binder application that allows you to run the code and experiment.

In the case of R resources appear to be more dispersed. The Twitter Recipes in the original version of `Mining the Social Web` have been translated for R Users by Bob Rudis in the 2018 electronic book `21 Recipes for Mining Twitter Data with rtweet` <https://rud.is/books/21-recipes/>.

The rise of social media has been accompanied by increasing research in the social sciences. This is a fast moving area but highly cited publications that will assist with orienting you are X,Y\<Z.

### Accessing Twitter Data

Twitter provides access to data in a range of ways:

a)  Historic data as a paid service
b)  Monthly public archives of the twitter stream (for example from <https://archive.org/>)
c)  Present day data provided through the Twitter API
d)  A COVID-19 endpoint
e)  A new [Academic Twitter](<https://developer.twitter.com/en/solutions/academic-research>) service for academic users
f)  New free curated datasets

One important feature of Twitter data is that it is generally forward looking. That is you can conduct a search of Twitter from now looking forwards but you cannot look backwards over time. For historic data you either need to pay for access or use a free dataset and hydrate the tweets.

In terms of access to Twitter data the only legitimate means is through the Twitter API. The Twitter API allows a user to either conduct searches or to create one or more applications. This means that you will need to engage with the Twitter API and you will preferably also be willing to work with the data in R using packages such as `rtweet` or Python using `Tweepy`. A number of free services offer means to download data to Excel through services such as plugins to the Chrome browser. This is fine and may be a good way to get started. However, it raises the question of how to analyse the results. In the case of `rtweet` and `Tweepy` this has often been worked out for the user in advance (such as data cleaning, time series analysis and sentiment analysis). As such, it is much easier to leverage existing tools and methods in the wider analytics community than to go it alone. There are very large communities of people using packages such as `rtweet` and `Tweepy` and that ecosystem means that you will generally be able to find answers to questions or inspiration for your analysis.

In this chapter we will use the very popular `rtweet` package in R developed by Michael Kierney at the University of Missouri. We will simply follow Mike Kierney's basic walkthrough at [https://rtweet.info/](https://rtweet.info/) and adapt it to our topic.

To get started we will install the packages that we will need to retrieve and analyse Twitter data. For this you will need to [install R](http://cran.rstudio.com/) and then [install the free RStudio Desktop](https://rstudio.com/products/rstudio/download/#download) for your system.

```{r pkgs, eval=FALSE, eval=FALSE}
install.packages("rtweet")
install.packages("httpuv")
install.packages("tidyverse") # for wrangling and analytics
install.packages("reactable") # optional for rendering react tables
```

When the packages have been installed simply load the two main libraries

```{r libs, eval=TRUE, message=FALSE}
library("rtweet")
library("tidyverse")
```

If you will be using Twitter data a lot then it is sensible to create a developer account and a dedicated application following the instructions on the Twitter Developer site [https://developer.twitter.com/](https://developer.twitter.com/). Most social media companies offer some kind of developer service and this should be your starting point in most cases. However, we can do quite a lot just using a personal account.

### Authentication

You will need a Twitter account to access the data. When you first start to search Twitter with `rtweet` a web page will open in your default browser asking you to authenticate with your username and password. It is very easy.

When seeking to develop research with Twitter it is a good idea to create an exploratory starter set. The aim is to identify terms or hashtags that are commonly linked with a topic. In the course of the COVID 19 pandemic a great deal of research has been generated on Twitter using a wide variety of definitions including a COVID 19 endpoint by Twitter itself <!---[INSERT REFERENCES]--->. For our purposes we will use a simple step by step approach to illustrate the process. Thus as a starting point we will use the term "covid" and gather 1000 tweets by running the following code in R with rtweet.

```{r searchtweets, echo=FALSE, eval=FALSE}
library(rtweet)
## search for 18000 tweets containing the phrase covid
covid <- search_tweets(
  "covid", n = 1000, include_rts = FALSE
)
```

```{r, savecovid, echo=FALSE, eval=FALSE}
save(covid, file = "data/social/covid.rda")
```

```{r loadcovid, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE}
load("data/social/covid.rda")
```

When you run these lines for the first time a pop up will appear to login.

```{r auth,  echo=FALSE}
knitr::include_graphics("images/twitter.png")
```

When login is complete a message will appear confirming authentication and the tweets will begin to download.

The twitter API returns the data in [Javascript Object Notation Format (JSON)](https://www.json.org/json-en.html) format. This format becomes very familiar to anyone who uses APIs. `rtweet` makes life easier for us by parsing the JSON into a table with 90 columns (at the time of writing).

Our aim is to identify hashtags that will assist in expanding the search. Tweets commonly contain one or more hashtags meaning that they form lists that need to be separated out before they can be counted. Table \@ref(tab:hashtags) shows the top 20 hashtags in our sample.

```{r hashtags, echo=FALSE}
library(tidyverse)
library(reactable)

hashtags <- covid %>% 
  unnest(hashtags) %>% 
  select(hashtags) %>% 
  count(hashtags, sort = TRUE) %>% 
  filter(hashtags != "NA")

hashtags %>% 
  reactable::reactable(., searchable = TRUE)
```

From this sample we can see that there are a variety of different terms being used in hashtags to describe the same thing with the case (upper, lower, mixed) and use of punctuation and numbers as the main issue.

To more adequately capture COVID related tweets we would probably want to work on a new query that captured the variants of these terms:

"Covid_19 OR COVID19 OR COVID OR coronavirus OR COVID-19 OR Covid OR covid OR COVID_19 OR Coronavirus or Covid19"

We would then want to add on our terms for intellectual property. First, we will want to take a look at the results of a variety of searches on intellectual property related terms to ensure capture of the most relevant terms.

```{r searchiprs, echo=FALSE, eval=FALSE}
library(rtweet)

ip <- search_tweets("patent OR intellectual property OR intellectualproperty OR IPR", n = 18000, include_rts = FALSE)
```

```{r saveip, echo=FALSE, eval=FALSE}
save(ip, file = "data/social/ip.rda")
```

```{r loadip, echo=FALSE, eval=TRUE}
load("data/social/ip.rda")
```

```{r iphashtags, echo=TRUE, eval=TRUE}
library(tidyverse)
library(reactable)

iphashtags <- ip %>% 
  unnest(hashtags) %>% 
  select(hashtags) %>% 
  count(hashtags, sort = TRUE) %>% 
  filter(hashtags != "NA")

iphashtags %>% 
  reactable::reactable(., searchable = TRUE)
```

Once we are happy that the base query was addressing the universe of things we want to capture we might then want to work on a strategy to capture data for our terms of interest.

In this example we want to find references to the term covid and terms such as patents and intellectual property or IP or IPR. To do that, as above, we need to look at the operators that are used by Twitter that are described on the [developers search queries page](https://developer.twitter.com/en/docs/labs/recent-search/guides/search-queries)<!---^[See: [https://developer.twitter.com/en/docs/labs/recent-search/guides/search-queries](https://developer.twitter.com/en/docs/labs/recent-search/guides/search-queries)]--->

We can now pull these things together into a single query:

In building a wider search strategy to capture the universe of tweets the data suggests that we would want to capture AI and ai and the lower and mixed (camel) case form of artificial intelligence. As we might expect there is a close relationship with machine learning (supervised and unsupervised) as MachineLearning and ML and ml, DeepLearning and dl (as a type of machine learning) as well as Big Data and Data Science (datascience). Elsewhere we see references to the internet of things (IoT) and general references to technology, robotics, innovation and news outlets such as `intoAInews`.

```{r eval=FALSE}
library(rtweet)
rate = rate_delay(30, max_times = 3)

#ai_search <- function(x){
#  out <- search_tweets(
#  x, n = 10, include_rts = FALSE)
#}

slow_tweet <- slowly(search_tweets, rate = rate, quiet = FALSE)

out <- map("artificial intelligence", slow_tweet)
```

```{r echo=FALSE, eval=FALSE}
search_tweets
```

One feature of assistive technology tweets is that there are not a lot of them and they often (from manual review) do not use hashtags... suggesting that those who are tweeting may be communicating mainly with their followers and that ther term `assistive technology` is not a focus of much in the way of community mobilisation. However, we can gain some clues for expanding our understanding in two ways

a)  experimenting wth some of the hashtags to expand the number of tweets
b)  examining prominent people

### Streaming Tweets

Once we have decided on the search terms that meet our needs from our sample we will often want to obtain more data. We can do this through the main API but will run into rate limiting if we want to obtain a lot of data. In these circumstances it is better to use the streaming API. Using the streaming API we can leave a query running (potentially for weeks or months or longer) and download datasets for processing. This is also very easy to do with `rtweet`. However, before you go ahead with a long running query (that may generate large datasets) it is a good idea to ensure that you review a sample of the results to ensure you are retrieving the desired data.

In this example, again following the `rtweet` tutorial we will set up a query that looks for two key words. You can add multiple key words. The first line of the code sets up a search that will run every 60 seconds (each minute), 60 times (for each hour) over 24 hours for 7 days over two weeks. This can of course be adjusted upwards or downwards but provides a good idea of the basic set up. We the have a set of search terms (which can be longer) that become the value 'q' for query in the set. We do not want to parse the data as it streams in (see below) which means that we will be saving JSON files. Next we set the time out to our calculation in two weeks. Finally, we provide the directory that we want to save the files to.

```{r, eval=FALSE}
library(rtweet)

# check the data is capturing what I want in terms of search terms
rt <- search_tweets(
  "patent OR ipr AND covid OR coronavirus", n = 1000,
  retryonratelimit = TRUE, include_rts = FALSE
)

# set up streaming for two weeks of tweets

testq <- 60L * 20L
twoweeks <- 60L * 60L * 24L * 7L * 2L

patentcovid<- "patent,covid"
stream_tweets2(
  q = biocovid,
  parse = TRUE,
  timeout = testq,
  dir = "covid_stream",
  lang = "en" #,
  #include_rts = FALSE
)


```

When running a long query it is best to do it on a dedicated machine, this could for example be a virtual machine on a cloud service, where you know that the internat connection is likely to be stable. If you do not have access to a second machine consider using a different strategy, such as a timed stream at a particular time of day where you know you will be online, rather than running continuously.

It may appear that nothing is happening, another reason to test your query first, but every 24 hours a new file should appear in the destination folder, as we can see below in Figure \@ref(fig:jsondownload) for a project that is tracking tweets on biodiversity and nature.

```{r jsondownload, echo=FALSE, fig.align='center'}
knitr::include_graphics("images/social/streaming_json_download.png")
```

As we can see in Figure \@ref(fig:jsondownload) for each 24 hour period a new JSON file is created as the stream completes its cycle. Note that the time this takes can vary. So stream one was created at 5.31 but stream 2 was created a couple of hours later. This is likely to reflect limiting or internet connection issues (the code will keep retrying).

Once we have gathered some data we can then easily parse it to a table that we can do something with.

```{r, parsestream, eval=FALSE}
library(rtweet)
# note this file is not to be publicly shared and is in the ignore folder for that reason. Look at how to include something on the data. Maybe as an image or a short set?
out <- parse_stream("ignore/stream-1.json")
```

```{r, eval=FALSE}
out_eng <- out %>% 
  filter(lang == "en")
```
